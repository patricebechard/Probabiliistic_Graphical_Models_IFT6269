{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "from numpy.random import dirichlet, multinomial\n",
    "import sys\n",
    "\n",
    "np.random.seed(6269)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generate toy documents \"\"\"\n",
    "K = 3\n",
    "D = 1000\n",
    "N = 10\n",
    "V = 12\n",
    "\n",
    "# 1) Set alpha and beta for generative model\n",
    "alpha = np.array([2, 2, 2])\n",
    "beta = np.array([[0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0 ,0, 0.25, 0.25, 0.25, 0.25]])\n",
    "\n",
    "# 2) Using alpha, sample theta_d for each of the D documents\n",
    "theta = dirichlet(alpha, D)\n",
    "\n",
    "# 3) Sample topic z_dn for each of the N words of each of the D documents\n",
    "# 4) Sample word w_dn from topic beta_k corresponding to z_dn\n",
    "docs = []\n",
    "for d in range(D):\n",
    "    # Counts of words for each topic in doc d (e.g [4, 3, 3] for N = 10)\n",
    "    z_d = multinomial(N, theta[d])\n",
    "    # One hot matrix of words for doc d\n",
    "    w_d = []\n",
    "    for k in xrange(K):\n",
    "        for i in xrange(z_d[k]):\n",
    "            w_d.append(multinomial(1, beta[k]))\n",
    "    docs.append(np.array(w_d))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_gibbs:\n",
    "\n",
    "    def __init__(self, n_topics, id2word=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_topics : int\n",
    "            number of topics for the model\n",
    "        id2word : dictionary\n",
    "            used to map id of word to real world\n",
    "            helps to monitor progress of learning\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_topics = n_topics\n",
    "        # Initialize alpha\n",
    "        self.alpha = np.ones(self.n_topics) / self.n_topics\n",
    "        self.id2word = id2word\n",
    "        \n",
    "    def fit(self, docs, threshold=0.001):\n",
    "        \"\"\"\n",
    "        Learning the parameters for the model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        docs : list of array-like elements, length = n_docs\n",
    "            each element is an array of dimensions n_words\n",
    "            by number of words in vocabulary (n_vocab). The \n",
    "            number of words varies from one text to the other.\n",
    "        threshold : float\n",
    "            used to stop fitting when the model has converged\n",
    "            to a final state.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_docs = len(docs)\n",
    "        self.n_vocab = docs[0].shape[1]\n",
    "        self.beta = np.ones(self.n_vocab) / self.n_vocab\n",
    "        \n",
    "        #initializing counters\n",
    "        self._initialize_counters(docs)\n",
    "    \n",
    "        old_loss = 10\n",
    "        new_loss = 0\n",
    "        \n",
    "        for i in xrange(100):\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                #monitoring progress by showing top words for each topic\n",
    "                print i\n",
    "                self._estimate_parameters()\n",
    "                if id2word:\n",
    "                    visualizeTopics(self, id2word)\n",
    "\n",
    "            self._gibbs_sampling_fit(docs)\n",
    "\n",
    "        #update parameters at the end of training\n",
    "        self._estimate_parameters()\n",
    "    \n",
    "    def _initialize_counters(self, docs):\n",
    "        \"\"\"\n",
    "        Initializing counters randomly\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialize arrays to 0\n",
    "        self.topic_doc_count = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.term_topic_count = np.zeros((self.n_topics, self.n_vocab))\n",
    "        self.phi = np.zeros_like(self.topic_doc_count)\n",
    "        self.theta = np.zeros_like(self.topic_doc_count)\n",
    "        \n",
    "        #keep topic assignment for each word in memory\n",
    "        self.topic_assign = []\n",
    "    \n",
    "        for i in xrange(self.n_docs):\n",
    "            \n",
    "            n_words = len(docs[i])\n",
    "\n",
    "            #sample topic randomly for each word in document\n",
    "            topic = np.random.randint(self.n_topics, size=n_words)\n",
    "            self.topic_assign.append(topic)\n",
    "            \n",
    "            #count number of words belonging to each topic\n",
    "            self.topic_doc_count[i] = np.bincount(topic, minlength=self.n_topics)\n",
    "            \n",
    "            #count word occurence for each topic\n",
    "            self.term_topic_count += np.array([np.sum(docs[i][np.where(topic==j)], axis=0)\\\n",
    "                                               for j in range(self.n_topics)])\n",
    "    \n",
    "    def _gibbs_sampling_fit(self, docs):\n",
    "        \"\"\"\n",
    "        Update counters for each word using gibbs sampling\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in xrange(self.n_docs):\n",
    "            \n",
    "            n_words = len(docs[i])\n",
    "            \n",
    "            for j in xrange(n_words):\n",
    "                \n",
    "                #decrement associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] -= 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] -= docs[i][j]\n",
    "                \n",
    "                #sample new topic\n",
    "                word_idx = np.argmax(docs[i][j])               \n",
    "                prob = (self.topic_doc_count[i] + self.alpha) * \\\n",
    "                        (self.term_topic_count[:, word_idx] + self.beta[word_idx]) /\\\n",
    "                        (np.sum(self.term_topic_count, axis=1) + np.sum(self.beta))                \n",
    "                prob /= np.sum(prob)                      #normalize\n",
    "                prob = np.cumsum(prob)                    #cdf\n",
    "                self.topic_assign[i][j] = np.argmax(prob > np.random.random())\n",
    "\n",
    "                #increment associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] += 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] += docs[i][j]      \n",
    "                \n",
    "    def _estimate_parameters(self):\n",
    "        \"\"\"\n",
    "        Estimate parameters used to make predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        self.phi = np.divide((self.term_topic_count + self.beta).T, \n",
    "                             np.sum((self.term_topic_count + self.beta), axis=1)).T\n",
    "        \n",
    "        self.theta = np.divide((self.topic_doc_count + self.alpha).T, \n",
    "                             np.sum((self.topic_doc_count + self.alpha), axis=1)).T\n",
    "        \n",
    "        self.components_ = self.phi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Get the 20 news groups data \"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "cats = ['sci.space', 'rec.sport.hockey']\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"train\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for sklearn (counts) \"\"\"\n",
    "n_features = 1000\n",
    "vectorizer = CountVectorizer(max_features=n_features, stop_words=\"english\")\n",
    "\n",
    "# Word counts per document matrix (input for sklearn)\n",
    "W_counts = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Keep track of vocabulary to visualize top words of each topic\n",
    "vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for our model (one hot vectors) \"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "\n",
    "# 1) Tokenize document\n",
    "# 2) Remove stop words\n",
    "# 3) Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer()\n",
    "# TODO : treat special list better\n",
    "# TODO See what count vectorizer does\n",
    "special = [\"''\", \"'s\", \"``\", \"n't\", \"...\", \"--\"]\n",
    "stop = set(stopwords.words('english') + \\\n",
    "           list(string.punctuation) + \n",
    "           list(string.digits) + \n",
    "           list(string.ascii_lowercase) + \\\n",
    "           special)\n",
    "\n",
    "def prepare_document(doc):\n",
    "    words = [lemmatizer.lemmatize(w) for w in word_tokenize(doc.lower()) \n",
    "             if w not in stop] \n",
    "    return words\n",
    "\n",
    "\n",
    "# List of documents (each document is a list of word tokens)\n",
    "texts = [prepare_document(text) for text in newsgroups.data]\n",
    "\n",
    "# Create a gensim dictionary for our corpus\n",
    "dic = Dictionary(texts)\n",
    "\n",
    "# Keep only k most frequent words\n",
    "n_features = 1000\n",
    "dic.filter_extremes(keep_n=n_features)\n",
    "vocab_size = len(dic.token2id) # Vocabulary size\n",
    "\n",
    "# List of documents (each document is now a list of word indexes)\n",
    "# We have removed all words not in the k most frequent words\n",
    "texts_idx = [[dic.token2id[word] for word in text \n",
    "              if word in dic.token2id] for text in texts]\n",
    "\n",
    "# Convert each index to a one hot vector\n",
    "W = np.array([np.eye(vocab_size)[text] for text in texts_idx if len(text) > 0])\n",
    "\n",
    "# Keep track of id to word mapping to visualize top words of topics\n",
    "id2word = dict([[v, k] for k, v in dic.token2id.items()])\n",
    "\n",
    "pickle.dump(W, open(\"W.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Topic 1:\n",
      "space would game team year one like time first get player think\n",
      "Topic 2:\n",
      "space team game would one year get time play first like new\n",
      "10\n",
      "Topic 1:\n",
      "would game team one year get like time think hockey player space\n",
      "Topic 2:\n",
      "space pt 25 period 10 satellite la power first data new mission\n",
      "20\n",
      "Topic 1:\n",
      "would game team one year get like time hockey player think season\n",
      "Topic 2:\n",
      "space pt 25 period satellite 10 launch nasa mission system la data\n",
      "30\n",
      "Topic 1:\n",
      "would game team one year get like time hockey player think season\n",
      "Topic 2:\n",
      "space pt 25 launch period satellite 10 nasa system power mission la\n",
      "40\n",
      "Topic 1:\n",
      "would game team one year get like time hockey player think season\n",
      "Topic 2:\n",
      "space launch pt 25 period satellite 10 nasa system mission power la\n",
      "50\n",
      "Topic 1:\n",
      "would game team one year get like time hockey player think season\n",
      "Topic 2:\n",
      "space launch pt 25 period satellite 10 nasa system mission la power\n",
      "60\n",
      "Topic 1:\n",
      "would team game one year get time like hockey player think season\n",
      "Topic 2:\n",
      "space launch pt 25 period satellite 10 system nasa mission la power\n",
      "70\n",
      "Topic 1:\n",
      "game team would one year get like time hockey player think season\n",
      "Topic 2:\n",
      "space launch pt 25 period satellite 10 nasa system power mission la\n",
      "80\n",
      "Topic 1:\n",
      "game team would one year get like time hockey player think season\n",
      "Topic 2:\n",
      "space launch pt 25 period nasa satellite 10 system mission la power\n",
      "90\n",
      "Topic 1:\n",
      "game would team one year get like time hockey player think season\n",
      "Topic 2:\n",
      "space launch pt 25 period nasa satellite 10 system mission la data\n",
      "Topic 1:\n",
      "game would team one year get like time hockey player think season\n",
      "Topic 2:\n",
      "space launch pt 25 period nasa satellite 10 system mission la data\n"
     ]
    }
   ],
   "source": [
    "n_topics = len(cats)\n",
    "lda = LDA_gibbs(n_topics, id2word=id2word)\n",
    "lda.fit(W)\n",
    "        \n",
    "visualizeTopics(lda, id2word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
