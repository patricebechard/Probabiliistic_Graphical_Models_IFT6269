%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2017 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2017,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2017} with
% \usepackage[nohyperref]{icml2017} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2017} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2017}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Mid-project Report - Latent Dirichlet Allocation}

\begin{document} 

\twocolumn[
\icmltitle{IFT6269 - Mid-project Report - Latent Dirichlet Allocation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2017
% package.

% list of affiliations. the first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% you can specify symbols, otherwise they are numbered in order
% ideally, you should not use this facility. affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Patrice B\'echard}{equal,um}
\icmlauthor{Th\'eophile Gervet}{equal,mg}

\end{icmlauthorlist}

\icmlaffiliation{um}{Universit\'e de Montr\'eal, Montr\'eal, Canada}
\icmlaffiliation{mg}{McGill University, Montr\'eal, Canada}

\icmlcorrespondingauthor{Patrice B\'echard}{patrice.bechard@umontreal.ca}
\icmlcorrespondingauthor{Th\'eophile Gervet}{theophile.gervet@umontreal.ca}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract} 
The project we have chosen is to review the article from Blei, Ng and Jordan on Latent Dirichlet Allocation \cite{blei2003latent}, implement the model and experiment with real data from the \textit{20 newsgroups} text dataset.
\end{abstract} 


\section{What has been done}

We already have started to write the final report for the project. We first provide a description of the Latent Dirichlet Allocation generative probabilistic model, explaining the setting of the problem and providing both the graphical model and the joint distribution associated with the model.

In the following section, we provide the mathematical developments on how to learn the parameters of the model to make accurate predictions using the EM algorithm and we highlight an important problem where the posterior distribution to our model is intractable, forcing us to rely on approximate inference to estimate the parameters. The first method used to solve this problem consist of using variational inference. We provide an in-depth explanation on how we define a new function which replaces the real posterior distribution, which we use to conduct the EM algorithm. We then reformulate both the E-step and the M-step applied to the situation and provide a pseudo-code for the algorithm.

An implementation of the Latent Dirichlet Allocation using variational inference has already been done. We tested the algorithm on sets of documents from two topics (\textit{atheism} and \textit{space}) taken from the \textit{20 newsgroups} text dataset and compared the obtained results with a pre-made implementation of the Latent Dirichlet Allocation provided within the Scikit-Learn library. Table \ref{tab:top_words} presents some words associated with each topic for both our implementation of the LDA using variational inference and the Scikit-Learn implementation.

We see that the results for both implementation are pretty similar, which is a good sign that our implementation is working as it should be. We also have implemented a function which presents the most likely topic assignment for each word in a document.

\begin{table}[htp]
\caption{\label{tab:top_words} Words associated to topics for our implementation of LDA using variational inference and Scikit-learn's implementation of LDA.}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{c|c|c|c}
\multicolumn{2}{c|}{Our implementation} & \multicolumn{2}{c}{Scikit-learn} \\
\multicolumn{2}{c|}{using variational inference} & \multicolumn{2}{c}{implementation} \\
\hline 
\textsc{Space} & \textsc{Atheism} & \textsc{Space} & \textsc{Atheism} \\
\hline 
space & one & space & people \\
launch & would & nasa & god \\
nasa & people & launch & don \\
satellite & god & earth & just \\
system & think & data & think \\
mission & like & orbit & like \\
year & thing & shuttle & does \\
orbit & say & satellite & know \\
program & know & lunar & say \\
data & atheist & moon & atheism \\
also & could & program & time \\
earth & make & edu & believe

\end{tabular}
\end{small}
\end{center}
\label{default}
\end{table}%

\section{What still has to be done}

A lot of things are still left for us to do. First, we will present an explanation of the EM algorithm for the model using Gibbs sampling as we have already done with variational inference and will provide a pseudocode of the algorithm as well. There is a lot of documentation about this precise subject in the literature \cite{griffiths2002gibbs}\cite{darling2011theoretical}\cite{heinrich2005parameter}. We will implement the Latent Dirichlet Allocation using Gibbs sampling to do approximate inference on the parameters of the model and experiment with similar data to look at the difference between both of our implementations as well as the one provided by Scikit-Learn. We will also expand our experimentation on more than two subjects and provide various cool visualisations.

% http://scikit-learn.org/stable/datasets/twenty_newsgroups.html
% Griffiths
% Darling
% Heinrich
% Blei, Ng, Jordan

\bibliographystyle{icml2017}
\bibliography{one_page_report}


\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
