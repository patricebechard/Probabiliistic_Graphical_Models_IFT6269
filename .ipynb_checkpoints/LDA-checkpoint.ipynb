{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "from numpy.random import dirichlet, multinomial\n",
    "import sys\n",
    "np.random.seed(0)\n",
    "np.seterr(all='warn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Generate toy documents \"\"\"\n",
    "K = 3\n",
    "D = 1000\n",
    "N = 10\n",
    "V = 12\n",
    "\n",
    "# 1) Set alpha and beta for generative model\n",
    "alpha = np.array([2, 2, 2])\n",
    "beta = np.array([[0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0 ,0, 0.25, 0.25, 0.25, 0.25]])\n",
    "\n",
    "# 2) Using alpha, sample theta_d for each of the D documents\n",
    "theta = dirichlet(alpha, D)\n",
    "\n",
    "# 3) Sample topic z_dn for each of the N words of each of the D documents\n",
    "# 4) Sample word w_dn from topic beta_k corresponding to z_dn\n",
    "W = []\n",
    "for d in range(D):\n",
    "    # Counts of words for each topic in doc d (e.g [4, 3, 3] for N = 10)\n",
    "    z_d = multinomial(N, theta[d])\n",
    "    # One hot matrix of words for doc d\n",
    "    w_d = []\n",
    "    for k in xrange(K):\n",
    "        for i in xrange(z_d[k]):\n",
    "            w_d.append(multinomial(1, beta[k]))\n",
    "    W.append(np.array(w_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma, digamma\n",
    "from scipy.stats import multinomial as multinomial_distribution\n",
    "from scipy.stats import dirichlet as dirichlet_distribution\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class LDA():\n",
    "    def __init__(self, K=3):\n",
    "        self.K = K\n",
    "        # Initialize alpha\n",
    "        self.alpha = np.ones(self.K) / self.K\n",
    "        \n",
    "    # W is a list of D numpy arrays of shape (N_d, V)\n",
    "    def fit(self, W, max_iter=51, threshold=1e-4, verbose=True):\n",
    "        self.D = len(W)\n",
    "        self.V = W[0].shape[1]\n",
    "        \n",
    "        # Randomly initialize variational parameters\n",
    "        # TODO best way to initialize gamma, phi ?\n",
    "        self.gamma = np.random.rand(self.D, self.K)\n",
    "        self.phi = [np.random.rand(w_d.shape[0], self.K) for w_d in W]\n",
    "        self.phi = [phi_d / np.sum(phi_d, axis=1, keepdims=True) \\\n",
    "            for phi_d in self.phi]\n",
    "        \n",
    "        # Initialize beta\n",
    "        self._updateBeta(W)\n",
    "        \n",
    "        # Keep track of previous value of lower bound on log likelihood\n",
    "        old_bound = self._computeBound(W)\n",
    "        \n",
    "        # Update phi, gamma, beta iteratively while the bound improves\n",
    "        for i in xrange(max_iter):\n",
    "            self._updatePhi(W)\n",
    "            self._updateGamma()\n",
    "            self._updateBeta(W)\n",
    "            current_bound = self._computeBound(W)\n",
    "            if verbose and i % 10 == 0:\n",
    "                print \"Iter {}, bound = {}\".format(i, old_bound)\n",
    "            if (current_bound - old_bound) < threshold:\n",
    "                break\n",
    "            old_bound = current_bound\n",
    "                \n",
    "        # Return final state of beta and variational parameters\n",
    "        self.components_ = self.beta # To match sklearn API\n",
    "        return self.beta, self.gamma, self.phi\n",
    "        \n",
    "    def _updateGamma(self):\n",
    "        for d in xrange(self.D):\n",
    "            self.gamma[d] = self.alpha + np.sum(self.phi[d], axis=0)\n",
    "    \n",
    "    def _updatePhi(self, W):\n",
    "        for d in xrange(self.D):\n",
    "            self.phi[d] = W[d].dot(self.beta.T) * \\\n",
    "                np.exp(digamma(self.gamma[d]))\n",
    "            self.phi[d] = normalize(self.phi[d], norm=\"l1\", axis=1)\n",
    "    \n",
    "    def _updateBeta(self, W):\n",
    "        self.beta = np.concatenate(self.phi, axis=0).T.dot(\n",
    "            np.concatenate(W, axis=0))\n",
    "        self.beta = normalize(self.beta, norm=\"l1\", axis=1)\n",
    "        \n",
    "    def _computeBound(self, W):\n",
    "        # Avoid recomputing expensive expressions\n",
    "        C1 = digamma(self.gamma)\n",
    "        C2 = digamma(np.sum(self.gamma, axis=1))\n",
    "        \n",
    "        # First three terms correspond to expectation of complete\n",
    "        # log likelihood under variational distribution\n",
    "        term1 = self.D * (np.log(gamma(np.sum(self.alpha))) - \\\n",
    "            np.sum(np.log(gamma(self.alpha)))) + \\\n",
    "            np.sum(C1 * (self.alpha - 1)) - np.sum(C2) * \\\n",
    "            np.sum(self.alpha - 1)\n",
    "        term2 = np.sum(np.array([np.sum(phi_d, axis=0) \\\n",
    "            for phi_d in self.phi]) * C1) - \\\n",
    "            np.array([np.sum(phi_d) for phi_d in self.phi]).dot(C2)\n",
    "        term3 = np.sum(np.log(self.beta) * np.concatenate(\n",
    "            self.phi, axis=0).T.dot(np.concatenate(W, axis=0)))\n",
    "        \n",
    "        # Sum of entropies of dirichlet variational posteriors\n",
    "        #term4 = -np.sum(np.log(gamma(np.sum(self.gamma, axis=1)))) + \\\n",
    "        #    np.sum(np.log(gamma(self.gamma))) - \\\n",
    "        #    np.sum((self.gamma - 1) * C1) + \\\n",
    "        #    C2.dot(np.sum(self.gamma - 1, axis=1))\n",
    "        term4 = 0\n",
    "        for d in xrange(self.D):\n",
    "            term4 += dirichlet_distribution.entropy(self.gamma[d])\n",
    "        \n",
    "        # Sum of entropies of multinomial variational posteriors\n",
    "        #term5 = -np.sum(np.concatenate(self.phi, axis=0) * \\\n",
    "        #    np.log(np.concatenate(self.phi, axis=0)))\n",
    "        term5 = np.sum(multinomial_distribution.entropy(\n",
    "            1, np.concatenate(self.phi, axis=0)))\n",
    "        \n",
    "        # Return lower bound on likelihood \n",
    "        # (averaged over total number of words)\n",
    "        L = term1 + term2 + term3 + term4 + term5\n",
    "        return L / np.concatenate(W, axis=0).shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LDA_gibbs:\n",
    "\n",
    "    def __init__(self, n_topics, alpha=None, id2word=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_topics : int\n",
    "            number of topics for the model\n",
    "        alpha : array-like, shape=(n_topics)\n",
    "            hyperparameter for the model\n",
    "            controls the likelihood of a topic being assigned\n",
    "            to a document\n",
    "        id2word : dictionary\n",
    "            used to map id of word to real world\n",
    "            helps to monitor progress of learning\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_topics = n_topics\n",
    "        #initialize alpha\n",
    "        if alpha is None:\n",
    "            self.alpha = np.ones(self.n_topics) * 50 / self.n_topics\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        self.id2word = id2word\n",
    "        \n",
    "    def fit(self, docs, threshold=0.001, eta=None):\n",
    "        \"\"\"\n",
    "        Learning the parameters for the model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        docs : list of array-like elements, length = n_docs\n",
    "            each element is an array of dimensions n_words\n",
    "            by number of words in vocabulary (n_vocab). The \n",
    "            number of words varies from one text to the other.\n",
    "        threshold : float\n",
    "            used to stop fitting when the model has converged\n",
    "            to a final state.\n",
    "        eta : array-like, shape=(n_vocab)\n",
    "            hyperparameter for the model\n",
    "            controls the likelihood of a word being assigned \n",
    "            to a topic\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_docs = len(docs)\n",
    "        self.n_vocab = docs[0].shape[1]\n",
    "        #initialize eta\n",
    "        if eta is None:\n",
    "            self.eta = np.ones(self.n_vocab) * 0.01\n",
    "        else:\n",
    "            self.eta = eta\n",
    "        \n",
    "        #initializing counters\n",
    "        self._initialize_counters(docs)\n",
    "    \n",
    "        old_loss = 10\n",
    "        new_loss = 0\n",
    "        \n",
    "        for i in xrange(50):\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                #monitoring progress by showing top words for each topic\n",
    "                print i\n",
    "                self._estimate_parameters()\n",
    "                if id2word:\n",
    "                    visualizeTopics(self, id2word)\n",
    "\n",
    "            self._gibbs_sampling_fit(docs)\n",
    "\n",
    "        #update parameters at the end of training\n",
    "        self._estimate_parameters()\n",
    "    \n",
    "    def _initialize_counters(self, docs):\n",
    "        \"\"\"Initializing counters randomly\"\"\"\n",
    "        \n",
    "        #initialize arrays to 0\n",
    "        self.topic_doc_count = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.term_topic_count = np.zeros((self.n_topics, self.n_vocab))\n",
    "        self.phi = np.zeros_like(self.topic_doc_count)\n",
    "        self.theta = np.zeros_like(self.topic_doc_count)\n",
    "        \n",
    "        #keep topic assignment for each word in memory\n",
    "        self.topic_assign = []\n",
    "    \n",
    "        for i in xrange(self.n_docs):\n",
    "            \n",
    "            n_words = len(docs[i])\n",
    "\n",
    "            #sample topic randomly for each word in document\n",
    "            topic = np.random.randint(self.n_topics, size=n_words)\n",
    "            self.topic_assign.append(topic)\n",
    "            \n",
    "            #count number of words belonging to each topic\n",
    "            self.topic_doc_count[i] = np.bincount(topic, minlength=self.n_topics)\n",
    "            \n",
    "            #count word occurence for each topic\n",
    "            self.term_topic_count += np.array([np.sum(docs[i][np.where(topic==j)], axis=0)\\\n",
    "                                               for j in range(self.n_topics)])\n",
    "    \n",
    "    def _gibbs_sampling_fit(self, docs):\n",
    "        \"\"\"Update counters for each word using gibbs sampling\"\"\"\n",
    "        \n",
    "        for i in xrange(self.n_docs):\n",
    "            \n",
    "            n_words = len(docs[i])\n",
    "            \n",
    "            for j in xrange(n_words):\n",
    "                \n",
    "                #decrement associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] -= 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] -= docs[i][j]\n",
    "                \n",
    "                #sample new topic\n",
    "                word_idx = np.argmax(docs[i][j])               \n",
    "                prob = (self.topic_doc_count[i] + self.alpha) * \\\n",
    "                        (self.term_topic_count[:, word_idx] + self.beta[word_idx]) /\\\n",
    "                        (np.sum(self.term_topic_count, axis=1) + np.sum(self.beta))                \n",
    "                prob /= np.sum(prob)                      #normalize\n",
    "                prob = np.cumsum(prob)                    #cdf\n",
    "                self.topic_assign[i][j] = np.argmax(prob > np.random.random())\n",
    "\n",
    "                #increment associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] += 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] += docs[i][j]      \n",
    "                \n",
    "    def _estimate_parameters(self):\n",
    "        \"\"\"Estimate parameters used to make predictions\"\"\"\n",
    "        \n",
    "        self.phi = np.divide((self.term_topic_count + self.beta).T, \n",
    "                             np.sum((self.term_topic_count + self.beta), axis=1)).T\n",
    "        \n",
    "        self.theta = np.divide((self.topic_doc_count + self.alpha).T, \n",
    "                             np.sum((self.topic_doc_count + self.alpha), axis=1)).T\n",
    "        \n",
    "        self.components_ = self.phi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, bound = -17.0677205003\n",
      "Iter 10, bound = -2.73484132991\n",
      "Iter 20, bound = -2.68272297592\n",
      "Iter 30, bound = -2.60498487916\n",
      "Iter 40, bound = -2.54304748958\n",
      "Iter 50, bound = -2.52028907819\n",
      "[[  1.16909951e-02   5.73124740e-05   1.52979499e-02   1.81426330e-02\n",
      "    2.92443035e-02   2.79711349e-02   3.60553959e-02   4.54574605e-02\n",
      "    1.91830209e-01   1.95422191e-01   2.13527118e-01   2.15303298e-01]\n",
      " [  1.85256175e-01   2.08647020e-01   2.10219764e-01   2.27440249e-01\n",
      "    2.61659358e-03   1.01107956e-02   1.56850425e-02   1.87054548e-02\n",
      "    2.01845677e-02   4.16485516e-02   2.37349949e-02   3.57507912e-02]\n",
      " [  5.07096290e-02   5.39302978e-02   2.80182236e-02   2.42073425e-02\n",
      "    2.10828664e-01   2.17548102e-01   1.86850044e-01   1.71769691e-01\n",
      "    2.90199990e-02   2.23790843e-02   4.72491206e-03   1.40110218e-05]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Simple test on toy documents \"\"\"\n",
    "model = LDA()\n",
    "beta, _, _ = model.fit(W)\n",
    "\n",
    "# We more or less retrieve the beta used in the generating process\n",
    "print beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Get the 20 news groups data \"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "cats = ['rec.sport.hockey', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"train\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for sklearn (counts) \"\"\"\n",
    "n_features = 1000\n",
    "vectorizer = CountVectorizer(max_features=n_features, stop_words=\"english\")\n",
    "\n",
    "# Word counts per document matrix (input for sklearn)\n",
    "W_counts = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Keep track of vocabulary to visualize top words of each topic\n",
    "vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1154\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Prepare input for our model (one hot vectors) \"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "\n",
    "# 1) Tokenize document\n",
    "# 2) Remove stop words\n",
    "# 3) Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer()\n",
    "# TODO : treat special list better\n",
    "# TODO See what count vectorizer does\n",
    "special = [\"''\", \"'s\", \"``\", \"n't\", \"...\", \"--\"]\n",
    "stop = set(stopwords.words('english') + \\\n",
    "           list(string.punctuation) + special)\n",
    "def prepare_document(doc):\n",
    "    words = [lemmatizer.lemmatize(w) for w in word_tokenize(doc.lower()) \n",
    "             if w not in stop] \n",
    "    return words\n",
    "\n",
    "\n",
    "# List of documents (each document is a list of word tokens)\n",
    "texts = [prepare_document(text) for text in newsgroups.data]\n",
    "\n",
    "# Create a gensim dictionary for our corpus\n",
    "dic = Dictionary(texts)\n",
    "\n",
    "# Keep only k most frequent words\n",
    "n_features = 1000\n",
    "dic.filter_extremes(keep_n=n_features)\n",
    "vocab_size = len(dic.token2id) # Vocabulary size\n",
    "\n",
    "# List of documents (each document is now a list of word indexes)\n",
    "# We have removed all words not in the k most frequent words\n",
    "texts_idx = [[dic.token2id[word] for word in text \n",
    "              if word in dic.token2id] for text in texts]\n",
    "\n",
    "# Convert each index to a one hot vector\n",
    "W = np.array([np.eye(vocab_size)[text] for text in texts_idx if len(text) > 0])\n",
    "\n",
    "# Keep track of id to word mapping to visualize top words of topics\n",
    "id2word = dict([[v, k] for k, v in dic.token2id.items()])\n",
    "\n",
    "pickle.dump(W, open(\"W.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "space like game nasa team just time year don think launch good\n",
      "Topic 2:\n",
      "10 25 11 55 12 14 16 18 15 13 20 period\n",
      "\n",
      "Our model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, bound = -13.3293797052\n",
      "Iter 10, bound = -6.27622866457\n",
      "Iter 20, bound = nan\n",
      "Iter 30, bound = nan\n",
      "Iter 40, bound = nan\n",
      "Iter 50, bound = nan\n",
      "Topic 1:\n",
      "1 0 2 3 4 5 7 6 game pt 25 season\n",
      "Topic 2:\n",
      "space would one year get time like team also think launch game\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 2\n",
    "\n",
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "\n",
    "# Compare with sklearn implementation\n",
    "print \"Sklearn model\"\n",
    "lda_sklearn = LatentDirichletAllocation(n_topics=n_topics, \n",
    "                                        learning_method=\"batch\")\n",
    "lda_sklearn.fit(W_counts)\n",
    "visualizeTopics(lda_sklearn, vocabulary)\n",
    "\n",
    "# Our model\n",
    "print \"\\nOur model\"\n",
    "lda = LDA(K=n_topics)\n",
    "W = pickle.load(open(\"W.p\", \"rb\"))\n",
    "lda.fit(W, verbose=True)\n",
    "visualizeTopics(lda, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Topic 1:\n",
      "1 0 2 space 3 team would game 4 one 5 year\n",
      "Topic 2:\n",
      "1 0 2 space 3 game team 4 would year one 5\n",
      "10\n",
      "Topic 1:\n",
      "space would one launch also 25 nasa satellite system year like mission\n",
      "Topic 2:\n",
      "1 0 2 3 game team 4 5 7 6 play hockey\n",
      "20\n",
      "Topic 1:\n",
      "space would one year like get time also think launch 25 could\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n",
      "30\n",
      "Topic 1:\n",
      "space would one year get time like also think launch 25 could\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n",
      "40\n",
      "Topic 1:\n",
      "space would one year time get like also think launch 25 could\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n",
      "50\n",
      "Topic 1:\n",
      "space would one year time get like also think launch 25 could\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n",
      "60\n",
      "Topic 1:\n",
      "space would one year time get like also think launch 25 could\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n",
      "70\n",
      "Topic 1:\n",
      "space would one year time get like also think launch 25 go\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n",
      "80\n",
      "Topic 1:\n",
      "space would one year time get like also think launch 25 go\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n",
      "90\n",
      "Topic 1:\n",
      "space would one year time get like also think launch 25 go\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n",
      "Topic 1:\n",
      "space would one year time get like also think launch 25 go\n",
      "Topic 2:\n",
      "1 0 2 3 team game 4 5 7 6 play hockey\n"
     ]
    }
   ],
   "source": [
    "#results for the LDA implementation using Gibbs sampling\n",
    "n_topics = len(cats)\n",
    "lda_gibbs = LDA_gibbs(n_topics, id2word=id2word)\n",
    "lda_gibbs.fit(W)\n",
    "        \n",
    "visualizeTopics(lda_lda_gibbs, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Visualize most likely topic assignment for each word \n",
    "    [if p(most likely topic) < threshold, don't assign a topic]\n",
    "    and visualize expected distribution over topics\n",
    "    Inputs:\n",
    "        model, our trained lda model\n",
    "        document_idx, index of document to visualize\n",
    "        id2word, mapping from word index to word string\n",
    "        topic2word, mapping from topic index to topic string\n",
    "        threshold, float in [0.0, 1.0]\n",
    "    \"\"\"\n",
    "def visualizeWordAssignments(model, document_idx, id2word, \n",
    "                             topic2word, threshold=0.9):\n",
    "    # We add a column with value threshold so that if the index of this \n",
    "    # column gets selected by the argmax operation, we don't assign a topic\n",
    "    col = np.ones((model.phi[document_idx].shape[0], 1)) * threshold\n",
    "    col_index = model.phi[document_idx].shape[1]\n",
    "    topic2word[col_index] = \" \"\n",
    "    word_topic_probas = np.concatenate(\n",
    "        (model.phi[document_idx], col), axis=1)\n",
    "    \n",
    "    # Expected distribution over topics\n",
    "    doc_topic_probas = dirichlet_distribution.mean(\n",
    "        model.gamma[document_idx])\n",
    "    \n",
    "    print \"DOCUMENT:\"\n",
    "    print newsgroups.data[document_idx]\n",
    "    \n",
    "    print \"\\nEXPECTED PROBA DISTRIB OVER TOPICS:\"\n",
    "    for i in xrange(len(doc_topic_probas)):\n",
    "        print \"{}: {}\".format(topic2word[i], doc_topic_probas[i])\n",
    "        \n",
    "    print \"\\nWORD TOPIC ASSIGNMENTS:\"\n",
    "    for word_idx, topic_idx in zip(texts_idx[document_idx], \n",
    "                                   np.argmax(word_topic_probas, axis=1)): \n",
    "        print \"{} -> {}\".format(id2word[word_idx], topic2word[topic_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a345113f5e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtopic2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"space\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"religion\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualizeWordAssignments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-868531a00b95>\u001b[0m in \u001b[0;36mvisualizeWordAssignments\u001b[0;34m(model, document_idx, id2word, topic2word, threshold)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# column gets selected by the argmax operation, we don't assign a topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocument_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcol_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocument_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtopic2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     word_topic_probas = np.concatenate(\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "topic2word = {0: \"space\", 1: \"religion\"}\n",
    "visualizeWordAssignments(lda, 0, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2word = {0: \"space\", 1: \"religion\"}\n",
    "visualizeWordAssignments(lda, 6, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Perform inference on new documents\n",
    "# only E-step (estimate phi and gamma) until the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
