{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import dirichlet, multinomial\n",
    "np.random.seed(0)\n",
    "np.seterr(all='warn')\n",
    "\n",
    "\"\"\" Generate toy documents \"\"\"\n",
    "K = 3\n",
    "D = 1000\n",
    "N = 10\n",
    "V = 12\n",
    "\n",
    "# 1) Set alpha and beta for generative model\n",
    "alpha = np.array([2, 2, 2])\n",
    "beta = np.array([[0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0 ,0, 0.25, 0.25, 0.25, 0.25]])\n",
    "\n",
    "# 2) Using alpha, sample theta_d for each of the D documents\n",
    "theta = dirichlet(alpha, D)\n",
    "\n",
    "# 3) Sample topic z_dn for each of the N words of each of the D documents\n",
    "# 4) Sample word w_dn from topic beta_k corresponding to z_dn\n",
    "W = []\n",
    "for d in range(D):\n",
    "    # Counts of words for each topic in doc d (e.g [4, 3, 3] for N = 10)\n",
    "    z_d = multinomial(N, theta[d])\n",
    "    # One hot matrix of words for doc d\n",
    "    w_d = []\n",
    "    for k in xrange(K):\n",
    "        for i in xrange(z_d[k]):\n",
    "            w_d.append(multinomial(1, beta[k]))\n",
    "    W.append(np.array(w_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma, digamma\n",
    "from scipy.stats import multinomial as multinomial_distribution\n",
    "from scipy.stats import dirichlet as dirichlet_distribution\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class LDA():\n",
    "    def __init__(self, K=3):\n",
    "        self.K = K\n",
    "        # Initialize alpha\n",
    "        self.alpha = np.ones(self.K) / self.K\n",
    "        \n",
    "    # W is a list of D numpy arrays of shape (N_d, V)\n",
    "    def fit(self, W, max_iter=51, threshold=1e-4, verbose=True):\n",
    "        self.D = len(W)\n",
    "        self.V = W[0].shape[1]\n",
    "        \n",
    "        # Randomly initialize variational parameters\n",
    "        # TODO best way to initialize gamma, phi ?\n",
    "        self.gamma = np.random.rand(self.D, self.K)\n",
    "        self.phi = [np.random.rand(w_d.shape[0], self.K) for w_d in W]\n",
    "        self.phi = [phi_d / np.sum(phi_d, axis=1, keepdims=True) \\\n",
    "            for phi_d in self.phi]\n",
    "        \n",
    "        # Initialize beta\n",
    "        self._updateBeta(W)\n",
    "        \n",
    "        # Keep track of previous value of lower bound on log likelihood\n",
    "        old_bound = self._computeBound(W)\n",
    "        \n",
    "        # Update phi, gamma, beta iteratively while the bound improves\n",
    "        for i in xrange(max_iter):\n",
    "            self._updatePhi(W)\n",
    "            self._updateGamma()\n",
    "            self._updateBeta(W)\n",
    "            current_bound = self._computeBound(W)\n",
    "            if verbose and i % 10 == 0:\n",
    "                print \"Iter {}, bound = {}\".format(i, old_bound)\n",
    "            if (current_bound - old_bound) < threshold:\n",
    "                break\n",
    "            old_bound = current_bound\n",
    "                \n",
    "        # Return final state of beta and variational parameters\n",
    "        self.components_ = self.beta # To match sklearn API\n",
    "        return self.beta, self.gamma, self.phi\n",
    "        \n",
    "    def _updateGamma(self):\n",
    "        for d in xrange(self.D):\n",
    "            self.gamma[d] = self.alpha + np.sum(self.phi[d], axis=0)\n",
    "    \n",
    "    def _updatePhi(self, W):\n",
    "        for d in xrange(self.D):\n",
    "            self.phi[d] = W[d].dot(self.beta.T) * \\\n",
    "                np.exp(digamma(self.gamma[d]))\n",
    "            self.phi[d] = normalize(self.phi[d], norm=\"l1\", axis=1)\n",
    "    \n",
    "    def _updateBeta(self, W):\n",
    "        self.beta = np.concatenate(self.phi, axis=0).T.dot(\n",
    "            np.concatenate(W, axis=0))\n",
    "        self.beta = normalize(self.beta, norm=\"l1\", axis=1)\n",
    "        \n",
    "    def _computeBound(self, W):\n",
    "        # Avoid recomputing expensive expressions\n",
    "        C1 = digamma(self.gamma)\n",
    "        C2 = digamma(np.sum(self.gamma, axis=1))\n",
    "        \n",
    "        # First three terms correspond to expectation of complete\n",
    "        # log likelihood under variational distribution\n",
    "        term1 = self.D * (np.log(gamma(np.sum(self.alpha))) - \\\n",
    "            np.sum(np.log(gamma(self.alpha)))) + \\\n",
    "            np.sum(C1 * (self.alpha - 1)) - np.sum(C2) * \\\n",
    "            np.sum(self.alpha - 1)\n",
    "        term2 = np.sum(np.array([np.sum(phi_d, axis=0) \\\n",
    "            for phi_d in self.phi]) * C1) - \\\n",
    "            np.array([np.sum(phi_d) for phi_d in self.phi]).dot(C2)\n",
    "        term3 = np.sum(np.log(self.beta) * np.concatenate(\n",
    "            self.phi, axis=0).T.dot(np.concatenate(W, axis=0)))\n",
    "        \n",
    "        # Sum of entropies of dirichlet variational posteriors\n",
    "        #term4 = -np.sum(np.log(gamma(np.sum(self.gamma, axis=1)))) + \\\n",
    "        #    np.sum(np.log(gamma(self.gamma))) - \\\n",
    "        #    np.sum((self.gamma - 1) * C1) + \\\n",
    "        #    C2.dot(np.sum(self.gamma - 1, axis=1))\n",
    "        term4 = 0\n",
    "        for d in xrange(self.D):\n",
    "            term4 += dirichlet_distribution.entropy(self.gamma[d])\n",
    "        \n",
    "        # Sum of entropies of multinomial variational posteriors\n",
    "        #term5 = -np.sum(np.concatenate(self.phi, axis=0) * \\\n",
    "        #    np.log(np.concatenate(self.phi, axis=0)))\n",
    "        term5 = np.sum(multinomial_distribution.entropy(\n",
    "            1, np.concatenate(self.phi, axis=0)))\n",
    "        \n",
    "        # Return lower bound on likelihood \n",
    "        # (averaged over total number of words)\n",
    "        L = term1 + term2 + term3 + term4 + term5\n",
    "        return L / np.concatenate(W, axis=0).shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, bound = -17.0677205003\n",
      "Iter 10, bound = -2.73484132991\n",
      "Iter 20, bound = -2.68272297592\n",
      "Iter 30, bound = -2.60498487916\n",
      "Iter 40, bound = -2.54304748958\n",
      "Iter 50, bound = -2.52028907819\n",
      "[[  1.16909951e-02   5.73124740e-05   1.52979499e-02   1.81426330e-02\n",
      "    2.92443035e-02   2.79711349e-02   3.60553959e-02   4.54574605e-02\n",
      "    1.91830209e-01   1.95422191e-01   2.13527118e-01   2.15303298e-01]\n",
      " [  1.85256175e-01   2.08647020e-01   2.10219764e-01   2.27440249e-01\n",
      "    2.61659358e-03   1.01107956e-02   1.56850425e-02   1.87054548e-02\n",
      "    2.01845677e-02   4.16485516e-02   2.37349949e-02   3.57507912e-02]\n",
      " [  5.07096290e-02   5.39302978e-02   2.80182236e-02   2.42073425e-02\n",
      "    2.10828664e-01   2.17548102e-01   1.86850044e-01   1.71769691e-01\n",
      "    2.90199990e-02   2.23790843e-02   4.72491206e-03   1.40110218e-05]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Simple test on toy documents \"\"\"\n",
    "model = LDA()\n",
    "beta, _, _ = model.fit(W)\n",
    "\n",
    "# We more or less retrieve the beta used in the generating process\n",
    "print beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Get the 20 news groups data \"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"train\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for sklearn (counts) \"\"\"\n",
    "n_features = 1000\n",
    "vectorizer = CountVectorizer(max_features=n_features, stop_words=\"english\")\n",
    "\n",
    "# Word counts per document matrix (input for sklearn)\n",
    "W_counts = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Keep track of vocabulary to visualize top words of each topic\n",
    "vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042\n",
      "(35, 1000)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Prepare input for our model (one hot vectors) \"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "\n",
    "# 1) Tokenize document\n",
    "# 2) Remove stop words\n",
    "# 3) Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer()\n",
    "# TODO : treat special list better\n",
    "# TODO See what count vectorizer does\n",
    "special = [\"''\", \"'s\", \"``\", \"n't\", \"...\", \"--\"]\n",
    "stop = set(stopwords.words('english') + \\\n",
    "           list(string.punctuation) + special)\n",
    "def prepare_document(doc):\n",
    "    words = [lemmatizer.lemmatize(w) for w in word_tokenize(doc.lower()) \n",
    "             if w not in stop] \n",
    "    return words\n",
    "\n",
    "\n",
    "# List of documents (each document is a list of word tokens)\n",
    "texts = [prepare_document(text) for text in newsgroups.data]\n",
    "\n",
    "# Create a gensim dictionary for our corpus\n",
    "dic = Dictionary(texts)\n",
    "\n",
    "# Keep only k most frequent words\n",
    "n_features = 1000\n",
    "dic.filter_extremes(keep_n=n_features)\n",
    "vocab_size = len(dic.token2id) # Vocabulary size\n",
    "\n",
    "# List of documents (each document is now a list of word indexes)\n",
    "# We have removed all words not in the k most frequent words\n",
    "texts_idx = [[dic.token2id[word] for word in text \n",
    "              if word in dic.token2id] for text in texts]\n",
    "\n",
    "# Convert each index to a one hot vector\n",
    "W = np.array([np.eye(vocab_size)[text] for text in texts_idx if len(text) > 0])\n",
    "\n",
    "print len(W)\n",
    "print W[0].shape[1]\n",
    "\n",
    "\n",
    "# Keep track of id to word mapping to visualize top words of topics\n",
    "id2word = dict([[v, k] for k, v in dic.token2id.items()])\n",
    "\n",
    "pickle.dump(W, open(\"W.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "people god don just think like does know say atheism time believe\n",
      "Topic 2:\n",
      "space nasa launch earth data orbit shuttle satellite lunar moon program edu\n",
      "\n",
      "Our model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, bound = -17.8195650678\n",
      "Iter 10, bound = -6.54943548086\n",
      "Iter 20, bound = -6.43186710498\n",
      "Iter 30, bound = nan\n",
      "Iter 40, bound = nan\n",
      "Iter 50, bound = nan\n",
      "Topic 1:\n",
      "space launch nasa satellite system mission year orbit program data also earth\n",
      "Topic 2:\n",
      "one would people god think like thing say know atheist could make\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 2\n",
    "\n",
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "\n",
    "# Compare with sklearn implementation\n",
    "print \"Sklearn model\"\n",
    "lda_sklearn = LatentDirichletAllocation(n_topics=n_topics, \n",
    "                                        learning_method=\"batch\")\n",
    "lda_sklearn.fit(W_counts)\n",
    "visualizeTopics(lda_sklearn, vocabulary)\n",
    "\n",
    "# Our model\n",
    "print \"\\nOur model\"\n",
    "lda = LDA(K=n_topics)\n",
    "W = pickle.load(open(\"W.p\", \"rb\"))\n",
    "lda.fit(W, verbose=True)\n",
    "visualizeTopics(lda, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Visualize most likely topic assignment for each word \n",
    "    [if p(most likely topic) < threshold, don't assign a topic]\n",
    "    and visualize expected distribution over topics\n",
    "    Inputs:\n",
    "        model, our trained lda model\n",
    "        document_idx, index of document to visualize\n",
    "        id2word, mapping from word index to word string\n",
    "        topic2word, mapping from topic index to topic string\n",
    "        threshold, float in [0.0, 1.0]\n",
    "    \"\"\"\n",
    "def visualizeWordAssignments(model, document_idx, id2word, \n",
    "                             topic2word, threshold=0.9):\n",
    "    # We add a column with value threshold so that if the index of this \n",
    "    # column gets selected by the argmax operation, we don't assign a topic\n",
    "    col = np.ones((model.phi[document_idx].shape[0], 1)) * threshold\n",
    "    col_index = model.phi[document_idx].shape[1]\n",
    "    topic2word[col_index] = \" \"\n",
    "    word_topic_probas = np.concatenate(\n",
    "        (model.phi[document_idx], col), axis=1)\n",
    "    \n",
    "    # Expected distribution over topics\n",
    "    doc_topic_probas = dirichlet_distribution.mean(\n",
    "        model.gamma[document_idx])\n",
    "    \n",
    "    print \"DOCUMENT:\"\n",
    "    print newsgroups.data[document_idx]\n",
    "    \n",
    "    print \"\\nEXPECTED PROBA DISTRIB OVER TOPICS:\"\n",
    "    for i in xrange(len(doc_topic_probas)):\n",
    "        print \"{}: {}\".format(topic2word[i], doc_topic_probas[i])\n",
    "        \n",
    "    print \"\\nWORD TOPIC ASSIGNMENTS:\"\n",
    "    for word_idx, topic_idx in zip(texts_idx[document_idx], \n",
    "                                   np.argmax(word_topic_probas, axis=1)): \n",
    "        print \"{} -> {}\".format(id2word[word_idx], topic2word[topic_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT:\n",
      "Yes, the Phobos mission did return some useful data including images of Phobos\n",
      "itself. The best I've seen had a surface resolution of about 40 meters. By\n",
      "the way, the new book entitled \"Mars\" (Kieffer et al, 1992, University of\n",
      "Arizona Press) has a great chapter on spacecraft exploration of the planet.\n",
      "The chapter is co-authored by V.I. Moroz of the Space Research Institute in\n",
      "Moscow, and includes details never before published in the West. Don't\n",
      "know of any ftp sites with images though.\n",
      "\n",
      "EXPECTED PROBA DISTRIB OVER TOPICS:\n",
      "space: 0.79000058284\n",
      "religion: 0.20999941716\n",
      "\n",
      "WORD TOPIC ASSIGNMENTS:\n",
      "yes -> religion\n",
      "mission -> space\n",
      "return -> space\n",
      "useful -> space\n",
      "data -> space\n",
      "including -> space\n",
      "image -> space\n",
      "best ->  \n",
      "'ve ->  \n",
      "seen ->  \n",
      "surface -> space\n",
      "way ->  \n",
      "new -> space\n",
      "book ->  \n",
      "mar -> space\n",
      "1992 -> space\n",
      "university -> space\n",
      "press -> space\n",
      "great ->  \n",
      "spacecraft -> space\n",
      "exploration -> space\n",
      "planet -> space\n",
      "space -> space\n",
      "research -> space\n",
      "institute -> space\n",
      "includes -> space\n",
      "detail -> space\n",
      "never ->  \n",
      "west ->  \n",
      "don't ->  \n",
      "know ->  \n",
      "ftp -> space\n",
      "site -> space\n",
      "image -> space\n",
      "though ->  \n"
     ]
    }
   ],
   "source": [
    "topic2word = {0: \"space\", 1: \"religion\"}\n",
    "visualizeWordAssignments(lda, 0, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT:\n",
      "\n",
      "I'll take a wild guess and say Freedom is objectively valuable.  I base\n",
      "this on the assumption that if everyone in the world were deprived utterly\n",
      "of their freedom (so that their every act was contrary to their volition),\n",
      "almost all would want to complain.  Therefore I take it that to assert or\n",
      "believe that \"Freedom is not very valuable\", when almost everyone can see\n",
      "that it is, is every bit as absurd as to assert \"it is not raining\" on\n",
      "a rainy day.  I take this to be a candidate for an objective value, and it\n",
      "it is a necessary condition for objective morality that objective values\n",
      "such as this exist.\n",
      "\n",
      "\n",
      "EXPECTED PROBA DISTRIB OVER TOPICS:\n",
      "space: 0.0206985585065\n",
      "religion: 0.979301441493\n",
      "\n",
      "WORD TOPIC ASSIGNMENTS:\n",
      "'ll -> religion\n",
      "take -> religion\n",
      "guess -> religion\n",
      "say -> religion\n",
      "freedom -> religion\n",
      "base ->  \n",
      "assumption -> religion\n",
      "everyone -> religion\n",
      "world -> religion\n",
      "freedom -> religion\n",
      "every -> religion\n",
      "act -> religion\n",
      "almost -> religion\n",
      "would -> religion\n",
      "want -> religion\n",
      "therefore -> religion\n",
      "take -> religion\n",
      "believe -> religion\n",
      "freedom -> religion\n",
      "almost -> religion\n",
      "everyone -> religion\n",
      "see -> religion\n",
      "every -> religion\n",
      "bit -> religion\n",
      "day -> religion\n",
      "take -> religion\n",
      "objective -> religion\n",
      "value -> religion\n",
      "necessary -> religion\n",
      "condition -> religion\n",
      "objective -> religion\n",
      "morality -> religion\n",
      "objective -> religion\n",
      "value -> religion\n",
      "exist -> religion\n"
     ]
    }
   ],
   "source": [
    "topic2word = {0: \"space\", 1: \"religion\"}\n",
    "visualizeWordAssignments(lda, 6, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Perform inference on new documents\n",
    "# only E-step (estimate phi and gamma) until the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
