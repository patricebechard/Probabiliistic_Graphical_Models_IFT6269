{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['beta', 'gamma']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'warn'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import dirichlet, multinomial\n",
    "import sys\n",
    "np.random.seed(0)\n",
    "np.seterr(all='warn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Generate toy documents \"\"\"\n",
    "K = 3\n",
    "D = 1000\n",
    "N = 10\n",
    "V = 12\n",
    "\n",
    "# 1) Set alpha and beta for generative model\n",
    "alpha = np.array([2, 2, 2])\n",
    "beta = np.array([[0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0 ,0, 0.25, 0.25, 0.25, 0.25]])\n",
    "\n",
    "# 2) Using alpha, sample theta_d for each of the D documents\n",
    "theta = dirichlet(alpha, D)\n",
    "\n",
    "# 3) Sample topic z_dn for each of the N words of each of the D documents\n",
    "# 4) Sample word w_dn from topic beta_k corresponding to z_dn\n",
    "W = []\n",
    "for d in range(D):\n",
    "    # Counts of words for each topic in doc d (e.g [4, 3, 3] for N = 10)\n",
    "    z_d = multinomial(N, theta[d])\n",
    "    # One hot matrix of words for doc d\n",
    "    w_d = []\n",
    "    for k in xrange(K):\n",
    "        for i in xrange(z_d[k]):\n",
    "            w_d.append(multinomial(1, beta[k]))\n",
    "    W.append(np.array(w_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma, digamma\n",
    "from scipy.stats import multinomial as multinomial_distribution\n",
    "from scipy.stats import dirichlet as dirichlet_distribution\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class LDA():\n",
    "    def __init__(self, K=3):\n",
    "        self.K = K\n",
    "        # Initialize alpha\n",
    "        self.alpha = np.ones(self.K) / self.K\n",
    "        \n",
    "    # W is a list of D numpy arrays of shape (N_d, V)\n",
    "    def fit(self, W, max_iter=51, threshold=1e-4, verbose=True):\n",
    "        self.D = len(W)\n",
    "        self.V = W[0].shape[1]\n",
    "        \n",
    "        # Randomly initialize variational parameters\n",
    "        # TODO best way to initialize gamma, phi ?\n",
    "        self.gamma = np.random.rand(self.D, self.K)\n",
    "        self.phi = [np.random.rand(w_d.shape[0], self.K) for w_d in W]\n",
    "        self.phi = [phi_d / np.sum(phi_d, axis=1, keepdims=True) \\\n",
    "            for phi_d in self.phi]\n",
    "        \n",
    "        # Initialize beta\n",
    "        self._updateBeta(W)\n",
    "        \n",
    "        # Keep track of previous value of lower bound on log likelihood\n",
    "        old_bound = self._computeBound(W)\n",
    "        \n",
    "        # Update phi, gamma, beta iteratively while the bound improves\n",
    "        for i in xrange(max_iter):\n",
    "            self._updatePhi(W)\n",
    "            self._updateGamma()\n",
    "            self._updateBeta(W)\n",
    "            current_bound = self._computeBound(W)\n",
    "            if verbose and i % 10 == 0:\n",
    "                print \"Iter {}, bound = {}\".format(i, old_bound)\n",
    "            if (current_bound - old_bound) < threshold:\n",
    "                break\n",
    "            old_bound = current_bound\n",
    "                \n",
    "        # Return final state of beta and variational parameters\n",
    "        self.components_ = self.beta # To match sklearn API\n",
    "        return self.beta, self.gamma, self.phi\n",
    "        \n",
    "    def _updateGamma(self):\n",
    "        for d in xrange(self.D):\n",
    "            self.gamma[d] = self.alpha + np.sum(self.phi[d], axis=0)\n",
    "    \n",
    "    def _updatePhi(self, W):\n",
    "        for d in xrange(self.D):\n",
    "            self.phi[d] = W[d].dot(self.beta.T) * \\\n",
    "                np.exp(digamma(self.gamma[d]))\n",
    "            self.phi[d] = normalize(self.phi[d], norm=\"l1\", axis=1)\n",
    "    \n",
    "    def _updateBeta(self, W):\n",
    "        self.beta = np.concatenate(self.phi, axis=0).T.dot(\n",
    "            np.concatenate(W, axis=0))\n",
    "        self.beta = normalize(self.beta, norm=\"l1\", axis=1)\n",
    "        \n",
    "    def _computeBound(self, W):\n",
    "        # Avoid recomputing expensive expressions\n",
    "        C1 = digamma(self.gamma)\n",
    "        C2 = digamma(np.sum(self.gamma, axis=1))\n",
    "        \n",
    "        # First three terms correspond to expectation of complete\n",
    "        # log likelihood under variational distribution\n",
    "        term1 = self.D * (np.log(gamma(np.sum(self.alpha))) - \\\n",
    "            np.sum(np.log(gamma(self.alpha)))) + \\\n",
    "            np.sum(C1 * (self.alpha - 1)) - np.sum(C2) * \\\n",
    "            np.sum(self.alpha - 1)\n",
    "        term2 = np.sum(np.array([np.sum(phi_d, axis=0) \\\n",
    "            for phi_d in self.phi]) * C1) - \\\n",
    "            np.array([np.sum(phi_d) for phi_d in self.phi]).dot(C2)\n",
    "        term3 = np.sum(np.log(self.beta) * np.concatenate(\n",
    "            self.phi, axis=0).T.dot(np.concatenate(W, axis=0)))\n",
    "        \n",
    "        # Sum of entropies of dirichlet variational posteriors\n",
    "        #term4 = -np.sum(np.log(gamma(np.sum(self.gamma, axis=1)))) + \\\n",
    "        #    np.sum(np.log(gamma(self.gamma))) - \\\n",
    "        #    np.sum((self.gamma - 1) * C1) + \\\n",
    "        #    C2.dot(np.sum(self.gamma - 1, axis=1))\n",
    "        term4 = 0\n",
    "        for d in xrange(self.D):\n",
    "            term4 += dirichlet_distribution.entropy(self.gamma[d])\n",
    "        \n",
    "        # Sum of entropies of multinomial variational posteriors\n",
    "        #term5 = -np.sum(np.concatenate(self.phi, axis=0) * \\\n",
    "        #    np.log(np.concatenate(self.phi, axis=0)))\n",
    "        term5 = np.sum(multinomial_distribution.entropy(\n",
    "            1, np.concatenate(self.phi, axis=0)))\n",
    "        \n",
    "        # Return lower bound on likelihood \n",
    "        # (averaged over total number of words)\n",
    "        L = term1 + term2 + term3 + term4 + term5\n",
    "        return L / np.concatenate(W, axis=0).shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_gibbs:\n",
    "\n",
    "    def __init__(self, n_topics, alpha=None, id2word=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_topics : int\n",
    "            number of topics for the model\n",
    "        alpha : array-like, shape=(n_topics)\n",
    "            hyperparameter for the model\n",
    "            controls the likelihood of a topic being assigned\n",
    "            to a document\n",
    "        id2word : dictionary\n",
    "            used to map id of word to real world\n",
    "            helps to monitor progress of learning\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_topics = n_topics\n",
    "        #initialize alpha\n",
    "        if alpha is None:\n",
    "            # default value (from Heinrich, 2005)\n",
    "            self.alpha = np.ones(self.n_topics) / self.n_topics\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        self.sumalpha = np.sum(self.alpha)\n",
    "        self.id2word = id2word\n",
    "        \n",
    "    def fit(self, docs, eta=None, niter=100):\n",
    "        \"\"\"\n",
    "        Learning the parameters for the model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        docs : list of array-like elements, length = n_docs\n",
    "            each element is an array of dimensions n_words\n",
    "            by number of words in vocabulary (n_vocab). The \n",
    "            number of words varies from one text to the other.\n",
    "        threshold : float\n",
    "            used to stop fitting when the model has converged\n",
    "            to a final state.\n",
    "        eta : array-like, shape=(n_vocab)\n",
    "            hyperparameter for the model\n",
    "            controls the likelihood of a word being assigned \n",
    "            to a topic\n",
    "        niter : int\n",
    "            number of gibbs sampling iterations for the training\n",
    "            of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_docs = len(docs)\n",
    "        self.n_vocab = docs[0].shape[1]\n",
    "        #initialize eta\n",
    "        if eta is None:\n",
    "            #default value (from Heinrich, 2005)\n",
    "            self.eta = np.ones(self.n_vocab) * 0.01\n",
    "        else:\n",
    "            self.eta = eta\n",
    "        self.sumeta = np.sum(self.eta)\n",
    "        \n",
    "        #initializing counters\n",
    "        self._initialize_counters_train(docs)\n",
    "        \n",
    "        for i in xrange(niter):\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                #monitoring progress by showing top words for each topic\n",
    "                print i\n",
    "                self._estimate_parameters_train()\n",
    "                if self.id2word:\n",
    "                    visualizeTopics(self, id2word)\n",
    "\n",
    "            self._gibbs_sampling_fit_train(docs)\n",
    "\n",
    "        #update parameters at the end of training\n",
    "        self._estimate_parameters_train()\n",
    "        \n",
    "    def query_sampling(self, queries, niter=100):\n",
    "        \n",
    "        self.n_queries = len(queries)\n",
    "        \n",
    "        #initializing counters\n",
    "        self._initialize_counters_test(queries)\n",
    "        \n",
    "        for i in xrange(niter):\n",
    "            self._gibbs_sampling_fit_test(queries)\n",
    "        \n",
    "        #update parameters at the end\n",
    "    \n",
    "    def perplexity(self, queries):\n",
    "        \n",
    "        log_likelihood = 0\n",
    "        tot_num_words = 0\n",
    "        \n",
    "        print(self.topic_doc_count_test.shape)\n",
    "        print(self.beta.shape)\n",
    "        print(self.theta.shape)\n",
    "        for i in range(self.n_queries):\n",
    "            \n",
    "            log_likelihood += np.sum(self.topic_doc_count_test * \\\n",
    "                                     np.log(np.sum((self.beta.T * self.theta[i]).T, axis=1)))\n",
    "            \n",
    "            tot_num_words += len(queries[i])\n",
    "        \n",
    "        perplexity = np.exp(-log_likelihood / tot_num_words)\n",
    "        \n",
    "        print(perplexity)\n",
    "\n",
    "    \n",
    "    def _initialize_counters_train(self, docs):\n",
    "        \"\"\"Initializing counters randomly for the training phase\"\"\"\n",
    "\n",
    "        #initialize arrays to 0\n",
    "        self.topic_doc_count = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.term_topic_count = np.zeros((self.n_topics, self.n_vocab))\n",
    "\n",
    "        #keep track of topic assignment for each word in memory\n",
    "        self.topic_assign = []\n",
    "\n",
    "        for i in xrange(self.n_docs):\n",
    "\n",
    "            n_words = len(docs[i])\n",
    "\n",
    "            #sample topic randomly for each word in document\n",
    "            topic = np.random.randint(self.n_topics, size=n_words)\n",
    "            self.topic_assign.append(topic)\n",
    "\n",
    "            #count number of words belonging to each topic\n",
    "            self.topic_doc_count[i] = np.bincount(topic, minlength=self.n_topics)\n",
    "\n",
    "            #count word occurence for each topic\n",
    "            self.term_topic_count += np.array([np.sum(docs[i][np.where(topic==j)], axis=0)\\\n",
    "                                               for j in range(self.n_topics)])\n",
    "\n",
    "            #sum of number of words in each topic\n",
    "            self.term_topic_sum = np.sum(self.term_topic_count, axis=1)\n",
    "            \n",
    "    def _initialize_counters_test(self, queries):\n",
    "        \"\"\"Initializing counters randomly for the test phase\"\"\"\n",
    "        \n",
    "        #initialize arrays to 0\n",
    "        self.topic_doc_count_test = np.zeros((self.n_queries,self.n_topics))\n",
    "        self.term_topic_count_test = np.zeros((self.n_topics, self.n_vocab))\n",
    "        \n",
    "        #keep track of topic assignment for each word in memory\n",
    "        self.topic_assign_test = []\n",
    "        \n",
    "        for i in xrange(self.n_queries):\n",
    "            \n",
    "            n_words = len(queries[i])\n",
    "            \n",
    "            #sample topic randomly for each word in query\n",
    "            topic = np.random.randint(self.n_topics, size=n_words)\n",
    "            self.topic_assign_test.append(topic)\n",
    "            \n",
    "            #count number fo words belonging to each topic\n",
    "            self.topic_doc_count_test[i] = np.bincount(topic, minlength=self.n_topics)\n",
    "            \n",
    "            #count word occurence for each topic\n",
    "            self.term_topic_count_test += np.array([np.sum(queries[i][np.where(topic==j)], axis=0)\\\n",
    "                                                    for j in range(self.n_topics)])\n",
    "            \n",
    "            #sum of number of words in each topic\n",
    "            self.term_topic_sum_test = np.sum(self.term_topic_count_test, axis=1)\n",
    "    \n",
    "    def _gibbs_sampling_fit_train(self, docs):\n",
    "        \"\"\"Update counters for each word using gibbs sampling\"\"\"\n",
    "        \n",
    "        for i in xrange(self.n_docs):\n",
    "            \n",
    "            n_words = len(docs[i])\n",
    "            \n",
    "            for j in xrange(n_words):\n",
    "                \n",
    "                #decrement associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] -= 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] -= docs[i][j]\n",
    "                self.term_topic_sum[self.topic_assign[i][j]] -= 1\n",
    "                \n",
    "                #sample new topic\n",
    "                word_idx = np.argmax(docs[i][j])               \n",
    "                prob = (self.topic_doc_count[i] + self.alpha) * \\\n",
    "                        (self.term_topic_count[:, word_idx] + self.eta[word_idx]) /\\\n",
    "                        (np.sum(self.term_topic_count, axis=1) + self.sumeta) /\\\n",
    "                        ((n_words-1) + self.sumalpha)\n",
    "                prob /= np.sum(prob)                      #normalize\n",
    "                prob = np.cumsum(prob)                    #cdf\n",
    "                self.topic_assign[i][j] = np.argmax(prob > np.random.random())\n",
    "\n",
    "                #increment associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] += 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] += docs[i][j]  \n",
    "                self.term_topic_sum[self.topic_assign[i][j]] += 1\n",
    "                \n",
    "    def _gibbs_sampling_fit_test(self, queries):\n",
    "        \n",
    "        for i in xrange(self.n_queries):\n",
    "            \n",
    "            n_words = len(queries[i])\n",
    "            \n",
    "            for j in xrange(n_words):\n",
    "                \n",
    "                #decrement associated counters\n",
    "                self.topic_doc_count_test[i][self.topic_assign_test[i][j]] -= 1\n",
    "                self.term_topic_count_test[self.topic_assign_test[i][j]] -= queries[i][j]\n",
    "                self.term_topic_sum_test[self.topic_assign_test[i][j]] -= 1\n",
    "                \n",
    "                #sample new topic\n",
    "                word_idx = np.argmax(queries[i][j])\n",
    "                \n",
    "                prob = (self.topic_doc_count_test[i] + self.alpha) * \\\n",
    "                        (self.term_topic_count_test[:,word_idx] + \\\n",
    "                             self.term_topic_count[:,word_idx] + \\\n",
    "                             self.eta[word_idx]) /\\\n",
    "                        (np.sum(self.term_topic_count_test, axis=1) + \\\n",
    "                             self.term_topic_sum + \\\n",
    "                             self.sumeta) /\\\n",
    "                        ((n_words + self.sumalpha))\n",
    "                prob /= np.sum(prob)              #normalize\n",
    "                prob = np.cumsum(prob)            #cdf\n",
    "                self.topic_assign_test[i][j] = np.argmax(prob > np.random.random())\n",
    "                \n",
    "                #increment associated counters\n",
    "                self.topic_doc_count_test[i][self.topic_assign_test[i][j]] += 1\n",
    "                self.term_topic_count_test[self.topic_assign_test[i][j]] += queries[i][j]\n",
    "                self.term_topic_sum_test[self.topic_assign_test[i][j]] += 1\n",
    "                \n",
    "    def _estimate_parameters_train(self):\n",
    "        \"\"\"Estimate parameters used to make predictions\"\"\"\n",
    "        \n",
    "        self.beta = np.divide((self.term_topic_count + self.eta).T, \n",
    "                             np.sum((self.term_topic_count + self.eta), axis=1)).T\n",
    "        \n",
    "        self.theta = np.divide((self.topic_doc_count + self.alpha).T, \n",
    "                             np.sum((self.topic_doc_count + self.alpha), axis=1)).T\n",
    "        \n",
    "        self.components_ = self.beta\n",
    "    \n",
    "    def _estimate_parameters_test(self):\n",
    "        \n",
    "                                         \n",
    "        self.theta_test = np.divide((self.topic_doc_count_test + self.alpha).T,\n",
    "                                   np.sum((self.topic_doc_count_test + self.alpha), axis=1)).T\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, bound = -17.0677205003\n",
      "Iter 10, bound = -2.73484132991\n",
      "Iter 20, bound = -2.68272297592\n",
      "Iter 30, bound = -2.60498487916\n",
      "Iter 40, bound = -2.54304748958\n",
      "Iter 50, bound = -2.52028907819\n",
      "[[  1.16909951e-02   5.73124740e-05   1.52979499e-02   1.81426330e-02\n",
      "    2.92443035e-02   2.79711349e-02   3.60553959e-02   4.54574605e-02\n",
      "    1.91830209e-01   1.95422191e-01   2.13527118e-01   2.15303298e-01]\n",
      " [  1.85256175e-01   2.08647020e-01   2.10219764e-01   2.27440249e-01\n",
      "    2.61659358e-03   1.01107956e-02   1.56850425e-02   1.87054548e-02\n",
      "    2.01845677e-02   4.16485516e-02   2.37349949e-02   3.57507912e-02]\n",
      " [  5.07096290e-02   5.39302978e-02   2.80182236e-02   2.42073425e-02\n",
      "    2.10828664e-01   2.17548102e-01   1.86850044e-01   1.71769691e-01\n",
      "    2.90199990e-02   2.23790843e-02   4.72491206e-03   1.40110218e-05]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Simple test on toy documents \"\"\"\n",
    "model = LDA()\n",
    "beta, _, _ = model.fit(W)\n",
    "\n",
    "# We more or less retrieve the beta used in the generating process\n",
    "print beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Get the 20 news groups data \"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"train\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for sklearn (counts) \"\"\"\n",
    "n_features = 1000\n",
    "vectorizer = CountVectorizer(max_features=n_features, stop_words=\"english\")\n",
    "\n",
    "# Word counts per document matrix (input for sklearn)\n",
    "W_counts = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Keep track of vocabulary to visualize top words of each topic\n",
    "vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for our model (one hot vectors) \"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "\n",
    "# 1) Tokenize document\n",
    "# 2) Remove stop words\n",
    "# 3) Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer()\n",
    "# TODO : treat special list better\n",
    "# TODO See what count vectorizer does\n",
    "special = [\"''\", \"'s\", \"``\", \"n't\", \"...\", \"--\"]\n",
    "stop = set(stopwords.words('english') + \\\n",
    "           list(string.punctuation) + special)\n",
    "\n",
    "def prepare_document(doc):\n",
    "    words = [lemmatizer.lemmatize(w) for w in word_tokenize(doc.lower()) \n",
    "             if w not in stop] \n",
    "    return words\n",
    "\n",
    "# List of documents (each document is a list of word tokens)\n",
    "texts = [prepare_document(text) for text in newsgroups.data]\n",
    "\n",
    "# Create a gensim dictionary for our corpus\n",
    "dic = Dictionary(texts)\n",
    "\n",
    "# Keep only k most frequent words\n",
    "n_features = 1000\n",
    "dic.filter_extremes(keep_n=n_features)\n",
    "vocab_size = len(dic.token2id) # Vocabulary size\n",
    "\n",
    "# List of documents (each document is now a list of word indexes)\n",
    "# We have removed all words not in the k most frequent words\n",
    "texts_idx = [[dic.token2id[word] for word in text \n",
    "              if word in dic.token2id] for text in texts]\n",
    "\n",
    "# Convert each index to a one hot vector\n",
    "W = np.array([np.eye(vocab_size)[text] for text in texts_idx if len(text) > 0])\n",
    "\n",
    "# Keep track of id to word mapping to visualize top words of topics\n",
    "id2word = dict([[v, k] for k, v in dic.token2id.items()])\n",
    "\n",
    "pickle.dump(W, open(\"W_train.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn model\n",
      "Topic 1:\n",
      "people god don just think like does know say atheism time believe\n",
      "Topic 2:\n",
      "space nasa launch earth data orbit shuttle satellite lunar moon program edu\n",
      "\n",
      "Our model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, bound = -17.8195650678\n",
      "Iter 10, bound = -6.54943548086\n",
      "Iter 20, bound = -6.43186710498\n",
      "Iter 30, bound = nan\n",
      "Iter 40, bound = nan\n",
      "Iter 50, bound = nan\n",
      "Topic 1:\n",
      "space launch nasa satellite system mission year orbit program data also earth\n",
      "Topic 2:\n",
      "one would people god think like thing say know atheist could make\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 2\n",
    "\n",
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "\n",
    "# Compare with sklearn implementation\n",
    "print \"Sklearn model\"\n",
    "lda_sklearn = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                        learning_method=\"batch\")\n",
    "lda_sklearn.fit(W_counts)\n",
    "visualizeTopics(lda_sklearn, vocabulary)\n",
    "\n",
    "# Our model\n",
    "print \"\\nOur model\"\n",
    "lda = LDA(K=n_topics)\n",
    "W = pickle.load(open(\"W_train.p\", \"rb\"))\n",
    "lda.fit(W, verbose=True)\n",
    "visualizeTopics(lda, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Topic 1:\n",
      "space launch satellite nasa system would year mission data also program orbit\n",
      "Topic 2:\n",
      "one would god people think say atheist like thing know could many\n"
     ]
    }
   ],
   "source": [
    "#results for the LDA implementation using Gibbs sampling\n",
    "lda_gibbs = LDA_gibbs(n_topics)\n",
    "lda_gibbs.fit(W, niter=10)\n",
    "        \n",
    "visualizeTopics(lda_gibbs, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading and preprocessing test data\n",
    "newsgroups_test = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"test\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"), categories=cats)\n",
    "\n",
    "# List of documents (each document is a list of word tokens)\n",
    "texts_test = [prepare_document(text) for text in newsgroups_test.data]\n",
    "\n",
    "# List of documents (each document is now a list of word indexes)\n",
    "# We have removed all words not in the k most frequent words\n",
    "texts_idx_test = [[dic.token2id[word] for word in text \n",
    "              if word in dic.token2id] for text in texts_test]\n",
    "\n",
    "# Convert each index to a one hot vector\n",
    "W_test = np.array([np.eye(vocab_size)[text] for text in texts_idx_test if len(text) > 0])\n",
    "\n",
    "# Keep track of id to word mapping to visualize top words of topics\n",
    "id2word = dict([[v, k] for k, v in dic.token2id.items()])\n",
    "\n",
    "pickle.dump(W_test, open(\"W_test.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 2)\n",
      "(2, 1000)\n",
      "(1042, 2)\n",
      "5.51365243754e+12\n"
     ]
    }
   ],
   "source": [
    "#testing our model on test data\n",
    "\n",
    "W_test = pickle.load(open(\"W_test.p\", \"rb\"))\n",
    "\n",
    "#keep only a certain number of documents\n",
    "n_kept_docs = 30\n",
    "W_test = W_test[:n_kept_docs]\n",
    "\n",
    "lda_gibbs.query_sampling(W_test)\n",
    "lda_gibbs.perplexity(W_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Visualize most likely topic assignment for each word \n",
    "    [if p(most likely topic) < threshold, don't assign a topic]\n",
    "    and visualize expected distribution over topics\n",
    "    Inputs:\n",
    "        model, our trained lda model\n",
    "        document_idx, index of document to visualize\n",
    "        id2word, mapping from word index to word string\n",
    "        topic2word, mapping from topic index to topic string\n",
    "        threshold, float in [0.0, 1.0]\n",
    "    \"\"\"\n",
    "def visualizeWordAssignments(model, document_idx, id2word, \n",
    "                             topic2word, threshold=0.9):\n",
    "    # We add a column with value threshold so that if the index of this \n",
    "    # column gets selected by the argmax operation, we don't assign a topic\n",
    "    col = np.ones((model.phi[document_idx].shape[0], 1)) * threshold\n",
    "    col_index = model.phi[document_idx].shape[1]\n",
    "    topic2word[col_index] = \" \"\n",
    "    word_topic_probas = np.concatenate(\n",
    "        (model.phi[document_idx], col), axis=1)\n",
    "    \n",
    "    # Expected distribution over topics\n",
    "    doc_topic_probas = dirichlet_distribution.mean(\n",
    "        model.gamma[document_idx])\n",
    "    \n",
    "    print \"DOCUMENT:\"\n",
    "    print newsgroups.data[document_idx]\n",
    "    \n",
    "    print \"\\nEXPECTED PROBA DISTRIB OVER TOPICS:\"\n",
    "    for i in xrange(len(doc_topic_probas)):\n",
    "        print \"{}: {}\".format(topic2word[i], doc_topic_probas[i])\n",
    "        \n",
    "    print \"\\nWORD TOPIC ASSIGNMENTS:\"\n",
    "    for word_idx, topic_idx in zip(texts_idx[document_idx], \n",
    "                                   np.argmax(word_topic_probas, axis=1)): \n",
    "        print \"{} -> {}\".format(id2word[word_idx], topic2word[topic_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT:\n",
      "Yes, the Phobos mission did return some useful data including images of Phobos\n",
      "itself. The best I've seen had a surface resolution of about 40 meters. By\n",
      "the way, the new book entitled \"Mars\" (Kieffer et al, 1992, University of\n",
      "Arizona Press) has a great chapter on spacecraft exploration of the planet.\n",
      "The chapter is co-authored by V.I. Moroz of the Space Research Institute in\n",
      "Moscow, and includes details never before published in the West. Don't\n",
      "know of any ftp sites with images though.\n",
      "\n",
      "EXPECTED PROBA DISTRIB OVER TOPICS:\n",
      "space: 0.79000058284\n",
      "religion: 0.20999941716\n",
      "\n",
      "WORD TOPIC ASSIGNMENTS:\n",
      "yes -> religion\n",
      "mission -> space\n",
      "return -> space\n",
      "useful -> space\n",
      "data -> space\n",
      "including -> space\n",
      "image -> space\n",
      "best ->  \n",
      "'ve ->  \n",
      "seen ->  \n",
      "surface -> space\n",
      "way ->  \n",
      "new -> space\n",
      "book ->  \n",
      "mar -> space\n",
      "1992 -> space\n",
      "university -> space\n",
      "press -> space\n",
      "great ->  \n",
      "spacecraft -> space\n",
      "exploration -> space\n",
      "planet -> space\n",
      "space -> space\n",
      "research -> space\n",
      "institute -> space\n",
      "includes -> space\n",
      "detail -> space\n",
      "never ->  \n",
      "west ->  \n",
      "don't ->  \n",
      "know ->  \n",
      "ftp -> space\n",
      "site -> space\n",
      "image -> space\n",
      "though ->  \n"
     ]
    }
   ],
   "source": [
    "topic2word = {0: \"space\", 1: \"religion\"}\n",
    "visualizeWordAssignments(lda, 0, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT:\n",
      "\n",
      "I'll take a wild guess and say Freedom is objectively valuable.  I base\n",
      "this on the assumption that if everyone in the world were deprived utterly\n",
      "of their freedom (so that their every act was contrary to their volition),\n",
      "almost all would want to complain.  Therefore I take it that to assert or\n",
      "believe that \"Freedom is not very valuable\", when almost everyone can see\n",
      "that it is, is every bit as absurd as to assert \"it is not raining\" on\n",
      "a rainy day.  I take this to be a candidate for an objective value, and it\n",
      "it is a necessary condition for objective morality that objective values\n",
      "such as this exist.\n",
      "\n",
      "\n",
      "EXPECTED PROBA DISTRIB OVER TOPICS:\n",
      "space: 0.0206985585065\n",
      "religion: 0.979301441493\n",
      "\n",
      "WORD TOPIC ASSIGNMENTS:\n",
      "'ll -> religion\n",
      "take -> religion\n",
      "guess -> religion\n",
      "say -> religion\n",
      "freedom -> religion\n",
      "base ->  \n",
      "assumption -> religion\n",
      "everyone -> religion\n",
      "world -> religion\n",
      "freedom -> religion\n",
      "every -> religion\n",
      "act -> religion\n",
      "almost -> religion\n",
      "would -> religion\n",
      "want -> religion\n",
      "therefore -> religion\n",
      "take -> religion\n",
      "believe -> religion\n",
      "freedom -> religion\n",
      "almost -> religion\n",
      "everyone -> religion\n",
      "see -> religion\n",
      "every -> religion\n",
      "bit -> religion\n",
      "day -> religion\n",
      "take -> religion\n",
      "objective -> religion\n",
      "value -> religion\n",
      "necessary -> religion\n",
      "condition -> religion\n",
      "objective -> religion\n",
      "morality -> religion\n",
      "objective -> religion\n",
      "value -> religion\n",
      "exist -> religion\n"
     ]
    }
   ],
   "source": [
    "topic2word = {0: \"space\", 1: \"religion\"}\n",
    "visualizeWordAssignments(lda, 6, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Perform inference on new documents\n",
    "# only E-step (estimate phi and gamma) until the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "(684, 2)\n",
      "(2, 1000)\n",
      "(1042, 2)\n",
      "2.40348784022e+296\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "(684, 5)\n",
      "(5, 1000)\n",
      "(1042, 5)\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:103: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-f9e95f8872bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#results for the LDA implementation using Gibbs sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlda_gibbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA_gibbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlda_gibbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlda_gibbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-b57382b5ec44>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, docs, eta, niter)\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0mvisualizeTopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gibbs_sampling_fit_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m#update parameters at the end of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-b57382b5ec44>\u001b[0m in \u001b[0;36m_gibbs_sampling_fit_train\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_doc_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_topic_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m                        \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm_topic_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msumeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m                        \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msumalpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mprob\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m                      \u001b[0;31m#normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m#cdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_assign\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m     \"\"\"\n\u001b[0;32m-> 2117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cumsum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W = pickle.load(open(\"W_train.p\", \"rb\"))\n",
    "W_test = pickle.load(open(\"W_test.p\", \"rb\"))\n",
    "\n",
    "perp = []\n",
    "\n",
    "for n_topics in [2,5,10,25,50]:\n",
    "    #results for the LDA implementation using Gibbs sampling\n",
    "    lda_gibbs = LDA_gibbs(n_topics)\n",
    "    lda_gibbs.fit(W, niter=50)\n",
    "\n",
    "    lda_gibbs.query_sampling(W_test)\n",
    "    perp.append(lda_gibbs.perplexity(W_test))\n",
    "\n",
    "\n",
    "plt.plot([2,5,10,25,50], perp,'x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
