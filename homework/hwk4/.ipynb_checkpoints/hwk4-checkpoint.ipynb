{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor : Patrice Bechard BECP30119404\\nIFT6269 - Probabilistic Graphical Models\\nHomework 3 - Hidden Markov Model Implementation\\nNovember 21th, 2017\\n'"
      ]
     },
     "execution_count": 1254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Author : Patrice Bechard BECP30119404\n",
    "IFT6269 - Probabilistic Graphical Models\n",
    "Homework 3 - Hidden Markov Model Implementation\n",
    "November 21th, 2017\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Importing modules\n",
    "\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as col\n",
    "import sys\n",
    "from matplotlib.patches import Ellipse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining initial prob, transition matrix,\n",
    "# means and covariance matrices of clusters (given)\n",
    "\n",
    "np.random.seed(6269)\n",
    "\n",
    "MEAN = np.array([[-2.0344,  4.1726],\n",
    "                 [ 3.9779,  3.7735],\n",
    "                 [ 3.8007, -3.7972],\n",
    "                 [-3.0620, -3.5345]])\n",
    "\n",
    "COV = np.array([[[2.9044,  0.2066],\n",
    "                 [0.2066,  2.7562]],\n",
    "                [[0.2104,  0.2904],\n",
    "                 [0.2904, 12.2392]],\n",
    "                [[0.9213,  0.0574],\n",
    "                 [0.0574,  1.8660]],\n",
    "                [[6.2414,  6.0502],\n",
    "                 [6.0502,  6.1825]]])\n",
    "\n",
    "PI = np.ones(MEAN.shape[0]) / MEAN.shape[0]\n",
    "\n",
    "TRANSIT_MATRIX = np.array([[1/2, 1/6, 1/6, 1/6],\n",
    "                           [1/6, 1/2, 1/6, 1/6],\n",
    "                           [1/6, 1/6, 1/2, 1/6],\n",
    "                           [1/6, 1/6, 1/6, 1/2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    \"\"\"K-means algorithm for clustering unlabeled data\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, train_data, init=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_classes : integer\n",
    "            number K of clusters to separate data\n",
    "        n_dims : integer\n",
    "            number of dimensions for the data\n",
    "        train_data : array-like, shape = [n_examples, n_dims]\n",
    "            data used to train the clustering algorithm\n",
    "        initializing : boolean\n",
    "            parameter is True if the algorithm is used to initialize cluster\n",
    "            means for another clustering algorithm\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_pts = train_data.shape[0]\n",
    "        self.n_dims = train_data.shape[1]\n",
    "        self.train_data = train_data\n",
    "        self.trained = False\n",
    "        self.init = init\n",
    "        \n",
    "        self._init_cluster_means()         #initializing self.means randomly\n",
    "        self.z = np.zeros(self.n_pts).astype('int')\n",
    "        \n",
    "    def train(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        Training the algorithm until convergence of objective function\n",
    "        or for a certain number of iterations\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_iter : int (or None)\n",
    "            Number of iterations to train the K-means algorithm.\n",
    "            If it is None, we train the k-means algorithm\n",
    "            until convergence.            \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        z : array-like, shape = [n_examples,n_classes]\n",
    "            array of onehot vectors indicating to which cluster each data\n",
    "            point is assigned\n",
    "        mean : array-like, shape = [n_classes,n_dims]\n",
    "            array containing the cluster mean for each class\n",
    "        \"\"\"\n",
    "        \n",
    "        if n_iter is None:\n",
    "            \n",
    "            # initial values for convergence condition\n",
    "            old_val = 1e9                       \n",
    "            new_val = 1e8       \n",
    "            \n",
    "            while np.abs(old_val - new_val) > 0.0001:    #convergence criterion\n",
    "                #E step\n",
    "                for i in range(self.n_pts):\n",
    "                    dist = np.sum(np.abs(self.train_data[i] - self.mean), axis=1)\n",
    "                    self.z[i] = np.argmin(dist)\n",
    "                    \n",
    "                #M step\n",
    "                update_mean = np.zeros_like(self.mean)\n",
    "                for i in range(self.n_pts):\n",
    "                    update_mean[self.z[i]] += self.train_data[i]\n",
    "                     \n",
    "                self.mean = np.divide(update_mean.T, np.bincount(self.z)).T\n",
    "            \n",
    "                old_val = new_val\n",
    "                new_val = self._compute_objective()\n",
    "        else:\n",
    "            for i in range(n_iter):\n",
    "                #E step\n",
    "                for i in range(self.n_pts):\n",
    "                    dist = np.sum(np.abs(self.train_data[i] - self.mean), axis=1)\n",
    "                    self.z[i] = np.argmin(dist)\n",
    "                    \n",
    "                #M step\n",
    "                update_mean = np.zeros_like(self.mean)\n",
    "                for i in range(self.n_pts):\n",
    "                    update_mean[self.z[i]] += self.train_data[i]\n",
    "                     \n",
    "                self.mean = np.divide(update_mean.T, np.bincount(self.z)).T       \n",
    "        \n",
    "        self.trained = True\n",
    "        \n",
    "        if self.init:                #algorithm used to initialize values for another algorithm\n",
    "            return self.z, self.mean\n",
    "    \n",
    "    def show_clusters(self, tag=None):\n",
    "        \n",
    "        if self.trained:\n",
    "            \"\"\"Show data, cluster means and cluster assignments\"\"\" \n",
    "            colors = ['r','g','b','y']\n",
    "            datacolor = []\n",
    "            datalabels = np.argmax(self.z,axis=1)\n",
    "            for i in range(len(datalabels)):\n",
    "                datacolor.append(colors[datalabels[i]])\n",
    "            objective = self._compute_objective()\n",
    "            \n",
    "            plt.scatter(self.train_data[:,0], self.train_data[:,1],alpha=0.3,marker='.',c=datacolor)\n",
    "            plt.scatter(self.mean[:,0], self.mean[:,1],marker='X',s=150,facecolor=colors, edgecolors='k',lw=2)\n",
    "            plt.xlabel('feature 1')\n",
    "            plt.ylabel('feature 2')\n",
    "            plt.title('Final position of cluster means with associated clusters (J=%.2f)'%objective)\n",
    "            if tag is not None:\n",
    "                plt.savefig('latex/figures/final_kmeans_%02d.png'%tag)\n",
    "            print('Value of objective function at end of convergence : %.2f'%objective)\n",
    "\n",
    "        else:\n",
    "            \"\"\"only show data points and randomly set cluster means\"\"\"\n",
    "            plt.scatter(self.train_data[:,0], self.train_data[:,1],marker='.', alpha=0.3)\n",
    "            plt.scatter(self.mean[:,0], self.mean[:,1],marker='X',s=150, edgecolors='k',lw=2)\n",
    "            plt.xlabel('feature 1')\n",
    "            plt.ylabel('feature 2')\n",
    "            plt.title('Initial cluster means position')\n",
    "            if tag is not None:\n",
    "                plt.savefig('latex/figures/init_kmeans_%02d.png'%tag) \n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "    def _init_cluster_means(self):\n",
    "        \"\"\"Randomly initializing cluster means within domain where data spreads\"\"\"\n",
    "        self.mean = np.zeros((self.n_states, self.n_dims))\n",
    "        x_span = np.max(self.train_data[:,0])-np.min(self.train_data[:,0])\n",
    "        y_span = np.max(self.train_data[:,1])-np.min(self.train_data[:,1])\n",
    "        \n",
    "        for i in range(self.n_states):\n",
    "            x_comp = x_span * np.random.random() + np.min(self.train_data[:,0])\n",
    "            y_comp = y_span * np.random.random() + np.min(self.train_data[:,1])\n",
    "\n",
    "            self.mean[i] = np.array([x_comp,y_comp])\n",
    "        \n",
    "    def _compute_objective(self):\n",
    "        \"\"\"Computes objective function which we try to minimize\"\"\"\n",
    "        objective = 0\n",
    "        for i in range(self.n_pts):\n",
    "            objective += np.sum(np.abs(self.train_data[i] - \\\n",
    "                                       self.mean[self.z[i]])**2)\n",
    "        return objective\n",
    "            \n",
    "    def log_likelihood(self, data):\n",
    "        \"\"\"Computes objective function which we try to minimize\"\"\"\n",
    "        objective = 0\n",
    "        n_pts = data.shape[0]\n",
    "        \n",
    "        #E step\n",
    "        for i in range(n_pts):\n",
    "            dist = np.sum(np.abs(data[i] - self.mean), axis=1)\n",
    "            objective += np.sum(np.abs(data[i] - self.mean[argmin(dist)])**2)\n",
    "            \n",
    "        return objective / n_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    \"\"\"Gaussian mixture model using the EM algorithm to cluster data for unsupervised learning\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states, train_data, cov_type):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_classes : integer\n",
    "            number K of clusters to separate data\n",
    "        n_dims : integer\n",
    "            number of dimensions for the data\n",
    "        train_data : array-like, shape = [n_examples, n_dims]\n",
    "            data used to train the clustering algorithm\n",
    "        cov_type : string\n",
    "            defines the type of covariance matrix used to fit the data.\n",
    "            Can be either 'isotropic' or 'full'.\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.n_pts = train_data.shape[0]\n",
    "        self.n_dims = train_data.shape[1]\n",
    "        self.train_data = train_data\n",
    "        self.cov_type = cov_type\n",
    "        self.trained = False\n",
    "        \n",
    "        #Initialization pi and cluster means using k-means\n",
    "        kmeans_init = K_Means(self.n_states, self.train_data, init=True)\n",
    "        self.z, self.mean = kmeans_init.train()\n",
    "        \n",
    "        self.pi = np.bincount(self.z) / self.n_pts\n",
    "        \n",
    "        #initialize as large spherical gaussian\n",
    "        self.cov = np.array([10 * np.eye(self.n_dims) \\\n",
    "                             for i in range(self.n_states)])\n",
    "\n",
    "    def train(self, n_iter=None):\n",
    "        \"\"\"\n",
    "        Training the algorithm until convergence of objective function\n",
    "        or for a certain number of iterations\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_iter : int (or None)\n",
    "            Number of iterations to train the K-means algorithm.\n",
    "            If it is None, we train the k-means algorithm\n",
    "            until convergence.  \n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        z : array-like, shape = [n_examples,n_classes]\n",
    "            array of onehot vectors indicating to which cluster each data\n",
    "            point is assigned\n",
    "        mean : array-like, shape = [n_classes,n_dims]\n",
    "            array containing the cluster mean for each class\n",
    "        \"\"\"\n",
    "        \n",
    "        old_val = 1e9\n",
    "        new_val = 1e8\n",
    "        \n",
    "        if n_iter is None:\n",
    "            while np.abs(old_val - new_val) > 0.0001:\n",
    "                #E step\n",
    "                #computing weights tau_i,j\n",
    "                self.weights = self._compute_weights(self.train_data)    \n",
    "\n",
    "                #M step\n",
    "                #updating pi, mean and covariance matrix\n",
    "                self._update_pi()\n",
    "                self._update_mean()\n",
    "                self._update_cov()\n",
    "\n",
    "                old_val = new_val\n",
    "                new_val = self._compute_objective()\n",
    "        else:\n",
    "            for i in range(n_iter):\n",
    "                #E step\n",
    "                #computing weights tau_i,j\n",
    "                self.weights = self._compute_weights(self.train_data)    \n",
    "\n",
    "                #M step\n",
    "                #updating pi, mean and covariance matrix\n",
    "                self._update_pi()\n",
    "                self._update_mean()\n",
    "                self._update_cov()                \n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def show_clusters(self,iter):\n",
    "        if self.trained:\n",
    "            \"\"\"\n",
    "            Show data, cluster means and cluster assignments with ellipse representing 95.45%\n",
    "            of the mass of the Gaussian distribution for each cluster after training.\n",
    "            \"\"\"\n",
    "            f, ax = plt.subplots()\n",
    "            colors = ['r','g','b','y']\n",
    "            datalabels = np.argmax(self.weights, axis=1)\n",
    "            datacolor = []\n",
    "            for i in range(len(datalabels)):\n",
    "                datacolor.append(colors[datalabels[i]])\n",
    "            ax.scatter(self.train_data[:,0], self.train_data[:,1],marker='.', alpha=0.5,c=datacolor)\n",
    "            ax.scatter(self.mean[:,0], self.mean[:,1],marker='X',s=150,facecolor=colors, edgecolors='k',lw=2)\n",
    "            for j in range(self.n_states):\n",
    "                plot_cov_ellipse(self.cov[j], self.mean[j], ax=ax, alpha=0.2, color=colors[j])\n",
    "            ax.set_xlabel('feature 1')\n",
    "            ax.set_ylabel('feature 2')\n",
    "            if self.cov_type == 'isotropic':\n",
    "                ax.axis('equal')\n",
    "            ax.set_title('Final cluster means position')\n",
    "            plt.savefig('latex/figures/final_%s_%02d.png'%(self.cov_type,iter))\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Show data, cluster means and cluster assignments for each cluster after initialization\n",
    "            using k-means algorithm.\n",
    "            \"\"\"\n",
    "            colors = ['r','g','b','y']\n",
    "            datacolor = []\n",
    "            datalabels = np.argmax(self.z,axis=1)\n",
    "            for i in range(len(datalabels)):\n",
    "                datacolor.append(colors[datalabels[i]])\n",
    "            plt.scatter(self.train_data[:,0], self.train_data[:,1],marker='.', alpha=0.5,c=datacolor)\n",
    "            plt.scatter(self.mean[:,0], self.mean[:,1],marker='X',s=150,facecolor=colors, edgecolors='k',lw=2)\n",
    "            plt.xlabel('feature 1')\n",
    "            plt.ylabel('feature 2')\n",
    "            if self.cov_type == 'isotropic':\n",
    "                plt.axis('equal')\n",
    "            plt.title('Initial cluster means position')\n",
    "            plt.savefig('latex/figures/init_%s_%02d.png'%(self.cov_type,iter))\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            \n",
    "    def _compute_weights(self, data):\n",
    "        \"\"\"Computing weights of each class for each data point\"\"\"\n",
    "        numerator = np.zeros((len(data), self.n_states))\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(self.n_states):\n",
    "                numerator[i,j] = self.pi[j] * gaussian(data[i],\n",
    "                                                       self.mean[j],\n",
    "                                                       self.cov[j])\n",
    "        denominator = np.sum(numerator,axis=1)      \n",
    "        return np.divide(numerator.T,denominator).T\n",
    "    \n",
    "    def _update_pi(self):\n",
    "        \"\"\"Updating the parameter pi\"\"\"\n",
    "        self.pi = np.sum(self.weights,axis=0) / self.n_states\n",
    "            \n",
    "    def _update_mean(self):      \n",
    "        \"\"\"Updating the cluster means for each class\"\"\"\n",
    "        for j in range(self.n_states):\n",
    "            self.mean[j] = np.matmul(self.weights[:,j],self.train_data)\n",
    "        self.mean = np.divide(self.mean.T,np.sum(self.weights,axis=0)).T\n",
    "            \n",
    "    def _update_cov(self):\n",
    "        \"\"\"Updating the covariance matrix depending on the cov_type\"\"\"\n",
    "        if self.cov_type == 'isotropic':\n",
    "            self.cov = np.zeros_like(self.cov)\n",
    "            sigma = np.zeros(self.n_states)\n",
    "            for j in range(self.n_states):\n",
    "                for i in range(self.n_pts):\n",
    "                    sigma[j] += self.weights[i,j] * np.dot(self.train_data[i]-self.mean[j],\n",
    "                                                           self.train_data[i]-self.mean[j])\n",
    "            sigma = np.divide(sigma, self.n_dims * np.sum(self.weights, axis=0))\n",
    "            for j in range(self.n_states):\n",
    "                self.cov[j] = np.eye(self.n_dims)*sigma[j]\n",
    "        elif self.cov_type == 'full':\n",
    "            self.cov = np.zeros_like(self.cov)\n",
    "            for j in range(self.n_states):\n",
    "                for i in range(self.n_pts):\n",
    "                    self.cov[j] += self.weights[i,j] * np.outer(self.train_data[i]-self.mean[j],\n",
    "                                                                self.train_data[i]-self.mean[j])\n",
    "            norm = np.sum(self.weights, axis=0)\n",
    "            for j in range(self.n_states):\n",
    "                self.cov[j] /= norm[j]\n",
    "        else:\n",
    "            raise Exception('Type of covariance matrix not supported')\n",
    "            \n",
    "    def _compute_objective(self):\n",
    "        \"\"\"Computing the objective function used in k-means (for convergence of algorithm)\"\"\"\n",
    "        objective = 0\n",
    "        for i in range(self.n_pts):\n",
    "            objective += np.sum(np.abs(self.train_data[i] - np.matmul(self.weights[i],self.mean))**2)\n",
    "        return objective\n",
    "\n",
    "    def log_likelihood(self, data):\n",
    "        \"\"\"\n",
    "        Computing the normalized log-likelihood for a \n",
    "        given dataset using the learned parameters\n",
    "        \"\"\"\n",
    "        logl = 0\n",
    "        weights = self._compute_weights(data)\n",
    "        \n",
    "        self.pi = np.sum(self.weights,axis=0) / self.n_states\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(self.n_states):\n",
    "                #gaussian log likelihood\n",
    "                logl += weights[i,j] * np.log(gaussian(data[i], \n",
    "                                                       self.mean[j], \n",
    "                                                       self.cov[j]))\n",
    "                #multinoulli log-likelihood\n",
    "                logl += weights[i,j] * np.log(self.pi[j])\n",
    "                \n",
    "        return  logl / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \"\"\"Implementation of the Hidden Markov Model\"\"\"\n",
    "    \n",
    "    def __init__(self, MEAN, COV, PI, TRANSIT_MATRIX):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        MEAN : array-like, shape = [n_states, n_dims]\n",
    "            Initial mean of for each possible state.\n",
    "        COV : array-like, shape = [n_states, n_dims, n_dims]\n",
    "            Initial covariance matrix for possible state.\n",
    "        PI : array-like, shape = [n_states]\n",
    "            Initial probability for each possible state.\n",
    "        TRANSIT_MATRIX : array-like, shape = [n_states, n_states]\n",
    "            Initial transition probability from one state to another.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_states = TRANSIT_MATRIX.shape[0]          #nb of states in the model\n",
    "        self.n_dims = MEAN.shape[1]\n",
    "        self.mean = MEAN\n",
    "        self.cov = COV\n",
    "        self.pi = PI\n",
    "        self.transit = TRANSIT_MATRIX\n",
    "        \n",
    "    def _forward(self, data):\n",
    "        \n",
    "        n_pts = data.shape[0]\n",
    "        \n",
    "        #initialize empty alpha array\n",
    "        self.alpha = np.zeros((n_pts, self.n_states))   \n",
    "\n",
    "        for t in range(n_pts):\n",
    "            for k in range(self.n_states):\n",
    "                if t == 0:\n",
    "                    # base case\n",
    "                    self.alpha[t,k] = self.pi[k] * gaussian(data[t], \n",
    "                                                           self.mean[k], \n",
    "                                                           self.cov[k])                    \n",
    "                else:\n",
    "                    self.alpha[t,k] = np.sum(self.alpha[t-1,:] * \\\n",
    "                                             self.transit[:,k] * \\\n",
    "                                             gaussian(data[t], self.mean[k], self.cov[k]))\n",
    "                    \n",
    "            #normalizing to solve the problem where the message vanishes to 0\n",
    "            self.alpha[t] /= np.sum(self.alpha[t])\n",
    "    \n",
    "    def _backward(self, data):\n",
    "        \n",
    "        n_pts = data.shape[0]\n",
    "        \n",
    "        #initialize empty beta array\n",
    "        self.beta = np.zeros((n_pts, self.n_states))\n",
    "\n",
    "        for t in reversed(range(n_pts)):\n",
    "            for k in range(self.n_states):\n",
    "                if t == (n_pts - 1):\n",
    "                    # base case\n",
    "                    self.beta[t,k] = 1\n",
    "                else:\n",
    "                    emission = np.array([gaussian(data[t+1], \n",
    "                                                  self.mean[i], \n",
    "                                                  self.cov[i]) for i in range(self.n_states)])\n",
    "                    \n",
    "                    self.beta[t,k] = np.sum(self.beta[t+1,:] * self.transit[k,:] * emission)\n",
    "                    \n",
    "            #normalizing to solve the problem where the message vanishes to 0\n",
    "            self.beta[t] /= np.sum(self.beta[t])    \n",
    "            \n",
    "    def _e_step(self, data):\n",
    "        \n",
    "        # E step\n",
    "        self._forward(data)\n",
    "        self._backward(data)\n",
    "        \n",
    "        n_pts = data.shape[0]\n",
    "       \n",
    "        #computing gamma\n",
    "        self.gamma = self.alpha * self.beta\n",
    "        self.gamma = np.divide(self.gamma.T, np.sum(self.gamma, axis=1)).T\n",
    "            \n",
    "        #computing xi\n",
    "        self.xi = np.zeros((n_pts-1, self.n_states, self.n_states))    #3rd degree tensor\n",
    "        \n",
    "        for t in range(1,n_pts):\n",
    "            for i in range(self.n_states):\n",
    "                for j in range(self.n_states):\n",
    "                    self.xi[t-1,i,j] = self.alpha[t-1,i] * \\\n",
    "                                       gaussian(data[t],self.mean[j],self.cov[j]) * \\\n",
    "                                       self.transit[i,j] * \\\n",
    "                                       self.beta[t,j]\n",
    "                #normalizing\n",
    "                self.xi[t-1,i] = np.divide(self.xi[t-1,i], np.sum(self.xi[t-1,i]))\n",
    "\n",
    "\n",
    "    def _m_step(self,data):\n",
    "        \n",
    "        # M step\n",
    "\n",
    "        n_pts = data.shape[0]\n",
    "                \n",
    "        # updating pi\n",
    "        self.pi = np.divide(self.gamma[0],np.sum(self.gamma[0]))\n",
    "        \n",
    "        # updating transition matrix A\n",
    "        self.transit = np.sum(self.xi, axis=0)\n",
    "        self.transit = np.divide(self.transit, np.sum(self.transit, axis=1))\n",
    "        \n",
    "        # updating mean of gaussians\n",
    "        self.mean = np.matmul(self.gamma.T, data)\n",
    "        self.mean = np.divide(self.mean.T, np.sum(self.gamma.T, axis=1)).T\n",
    "        \n",
    "        # updating covariance matrices\n",
    "        self.cov = np.zeros((4,2,2))\n",
    "\n",
    "        for k in range(self.n_states):\n",
    "            for t in range(n_pts):\n",
    "                self.cov[k] += self.gamma[t,k] * np.outer((data[t] - self.mean[k]),\n",
    "                                                          (data[t] - self.mean[k]))\n",
    "        norm = np.sum(self.gamma, axis=0)\n",
    "        for i in range(self.n_states):\n",
    "            self.cov[i] /= norm[i]\n",
    "            \n",
    "    def train(self, data, n_iter=None):\n",
    "        \n",
    "        if n_iter is None:\n",
    "\n",
    "            # initial values for convergence condition\n",
    "            old_val = 0\n",
    "            new_val = 10000\n",
    "\n",
    "            while np.abs(new_val - old_val) > 0.0001:\n",
    "                #training iteration using the EM algorithm\n",
    "                self._e_step(data)\n",
    "                self._m_step(data)\n",
    "\n",
    "                old_val = new_val\n",
    "                new_val = self.log_likelihood(data)\n",
    "\n",
    "        else:\n",
    "            for i in range(n_iter):\n",
    "                #training iteration using the EM algorithm\n",
    "                hmm._e_step(data)\n",
    "                hmm._m_step(data)\n",
    "    \n",
    "    def viterbi(self, data):\n",
    "        \n",
    "        n_pts = data.shape[0]\n",
    "        \n",
    "        path = np.zeros((n_pts, self.n_states)).astype('int')\n",
    "        scores = np.zeros((n_pts, self.n_states))\n",
    "        \n",
    "        for t in range(n_pts):\n",
    "            if t == 0:\n",
    "                #base case\n",
    "                for k in range(self.n_states):\n",
    "                    scores[t,k] = self.pi[k] * gaussian(data[t],\n",
    "                                               self.mean[k],\n",
    "                                               self.cov[k])\n",
    "                scores[t] /= np.sum(scores[t])\n",
    "            else:\n",
    "                #forward propagation\n",
    "                tmp = (scores[t-1] * self.transit.T).T\n",
    "                scores[t] = np.max(tmp, axis=0) * \\\n",
    "                            np.array([gaussian(data[t], \n",
    "                                               self.mean[k], \n",
    "                                               self.cov[k]) for k in range(self.n_states)])\n",
    "                path[t] = np.argmax(tmp, axis=0)\n",
    "                #normalizing\n",
    "                scores[t] /= np.sum(scores[t])\n",
    "        \n",
    "        #backtracking to find most likely sequence of states\n",
    "        seq = np.zeros(n_pts).astype('int')\n",
    "\n",
    "        for t in reversed(range(n_pts)):\n",
    "            if t == (n_pts - 1):\n",
    "                #base case\n",
    "                seq[t] = np.argmax(scores[t])\n",
    "            else:\n",
    "                seq[t] = path[t+1, seq[t+1]]\n",
    "        \n",
    "        return seq, scores\n",
    "\n",
    "    def log_likelihood(self, data):\n",
    "        \n",
    "        n_pts = data.shape[0]\n",
    "        \n",
    "        #initial configuration term\n",
    "        init = 0\n",
    "        for k in range(self.n_states):\n",
    "            init += self.gamma[0,k] * np.log(self.pi[k])\n",
    "            \n",
    "        #emission term\n",
    "        emission = 0\n",
    "        for t in range(n_pts):\n",
    "            for k in range(self.n_states):\n",
    "                emission += self.gamma[t,k] * np.log(gaussian(data[t], \n",
    "                                                              self.mean[k],\n",
    "                                                              self.cov[k]))\n",
    "        #transition term\n",
    "        transition = 0\n",
    "        sum_xi = np.sum(self.xi,axis=0)\n",
    "        for i in range(self.n_states):\n",
    "            for j in range(self.n_states):\n",
    "                transition += np.log(self.transit[i,j]) * sum_xi[i,j]      \n",
    "        \n",
    "        #normalized log likelihood\n",
    "        logl = (init + emission + transition) / n_pts\n",
    "        \n",
    "        return logl\n",
    "    \n",
    "    def show_clusters(self, data):\n",
    "        \"\"\"\n",
    "        Show data, cluster means and cluster assignments with ellipse representing 95.45%\n",
    "        of the mass of the Gaussian distribution for each cluster after training.\n",
    "        \"\"\"\n",
    "        f, ax = plt.subplots()\n",
    "        colors = ['r','g','b','y']\n",
    "        datalabels = np.argmax(self.gamma, axis=1)\n",
    "        datacolor = []\n",
    "        for i in range(len(datalabels)):\n",
    "            datacolor.append(colors[datalabels[i]])\n",
    "        ax.scatter(data[:,0], data[:,1],marker='.', alpha=0.5,c=datacolor)\n",
    "        ax.scatter(self.mean[:,0], self.mean[:,1],marker='X',s=150,facecolor=colors, edgecolors='k',lw=2)\n",
    "        for j in range(self.K):\n",
    "            plot_cov_ellipse(self.cov[j], self.mean[j], ax=ax, alpha=0.2, color=colors[j])\n",
    "        ax.set_xlabel('feature 1')\n",
    "        ax.set_ylabel('feature 2')\n",
    "        ax.set_title('Final cluster means position')\n",
    "        #plt.savefig('latex/figures/final_%s_%02d.png'%(self.cov_type,iter))\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian(data, mean, cov):\n",
    "    \"\"\"Computing gaussian probability given a data point, a mean and a covariance matrix\"\"\"\n",
    "    if type(data) == np.float64:\n",
    "        return (1/(2*np.pi*cov))**0.5 * \\\n",
    "            np.exp(-0.5 * (data-mean)*(1/cov)*(data-mean))\n",
    "    else:\n",
    "        return (1/np.linalg.det(2*np.pi*cov))**0.5 * \\\n",
    "            np.exp(-0.5 * np.dot(np.matmul((data-mean),np.linalg.inv(cov)),(data-mean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cov_ellipse(cov, pos, nstd=2, ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Plotting an ellipse representing a percentage of the mass of the Gaussian distribution.\n",
    "    Taken from stackoverflow:\n",
    "    https://stackoverflow.com/questions/12301071/multidimensional-confidence-intervals\n",
    "    \"\"\"\n",
    "\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = 2 * nstd * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)\n",
    "\n",
    "    ax.add_artist(ellip)\n",
    "    return ellip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fake_parameters_inference(data, plot=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points with which the HMM is trained.\n",
    "    plot : boolean\n",
    "        Decides whether or not a plot is created.\n",
    "    \"\"\"\n",
    "    \n",
    "    hmm = HMM(MEAN, COV, PI, TRANSIT_MATRIX)\n",
    "    hmm._e_step(data)\n",
    "\n",
    "    if plot:\n",
    "        plot_data_prob_by_state(hmm.gamma, 'fake_param_inference')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_log_likelihood(train_data, test_data, n_iter=None, plot=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points with which the HMM is trained.\n",
    "    test_data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points for which we test the HMM after\n",
    "        training.\n",
    "    niter : int (or None)\n",
    "        Number of iterations to train the HMM.\n",
    "        If it is None, we train the HMM until convergence.\n",
    "    plot : boolean\n",
    "        Decides whether or not a plot is created.\n",
    "    \"\"\"\n",
    "    \n",
    "    hmm = HMM(MEAN, COV, PI, TRANSIT_MATRIX)\n",
    "    \n",
    "    if n_iter is None:\n",
    "        \n",
    "        # initial values for convergence condition\n",
    "        old_val = 0\n",
    "        new_val = 10000\n",
    "        \n",
    "        #initializing empty array to host log likelihood values\n",
    "        logl_train = np.array([])\n",
    "        logl_test = np.array([])\n",
    "        \n",
    "        while np.abs(new_val - old_val) > 0.0001:\n",
    "            #training iteration using the EM algorithm\n",
    "            hmm._e_step(train_data)\n",
    "            hmm._m_step(train_data)\n",
    "            \n",
    "            tmp = hmm.log_likelihood(train_data)\n",
    "            old_val = new_val\n",
    "            new_val = tmp\n",
    "            logl_train = np.append(logl_train, np.array([tmp]))\n",
    "\n",
    "            hmm._e_step(test_data)\n",
    "            tmp = hmm.log_likelihood(test_data)\n",
    "            logl_test = np.append(logl_test, np.array([tmp]))\n",
    "    else:\n",
    "        #initializing empty array to host log likelihood values\n",
    "        logl_train = np.zeros(niter)\n",
    "        logl_test = np.zeros(niter)\n",
    "        for i in range(n_iter):\n",
    "            #training iteration using the EM algorithm\n",
    "            hmm._e_step(train_data)\n",
    "            hmm._m_step(train_data)\n",
    "            logl_train[i] = hmm.log_likelihood(train_data)\n",
    "\n",
    "            hmm._e_step(test_data)\n",
    "            logl_test[i] = hmm.log_likelihood(test_data)\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(logl_train, label='training data')\n",
    "        plt.plot(logl_test, label='test data')\n",
    "        plt.xlabel(\"Number of training iterations\")\n",
    "        plt.ylabel(\"Log likelihood\")\n",
    "        plt.legend(fancybox=True, shadow=True)\n",
    "        plt.savefig(\"latex/figures/log_likelihood_hmm.png\")\n",
    "        plt.show()\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_table_log_likelihood(train_data, test_data, n_iter=None, table=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points with which the model is trained.\n",
    "    test_data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points for which we test the model after\n",
    "        training.\n",
    "    niter : int (or None)\n",
    "        Number of iterations to train the HMM.\n",
    "        If it is None, we train the HMM until convergence.\n",
    "    table : boolean\n",
    "        Decides whether or not a table is created.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = TRANSIT_MATRIX.shape[0]\n",
    "    n_dims = train_data.shape[1]\n",
    "\n",
    "    logl_table = np.zeros((4,2))\n",
    "    \n",
    "    hmm = HMM(MEAN, COV, PI, TRANSIT_MATRIX)\n",
    "    hmm.train(train_data, n_iter)\n",
    "    logl_table[0,0] = hmm.log_likelihood(train_data)\n",
    "    hmm._e_step(test_data)\n",
    "    logl_table[0,1] = hmm.log_likelihood(test_data)\n",
    "    \n",
    "    gmm_full = GMM(n_states, train_data, 'full')\n",
    "    gmm_full.train(n_iter)\n",
    "    logl_table[1,0] = gmm_full.log_likelihood(train_data)\n",
    "    logl_table[1,1] = gmm_full.log_likelihood(test_data)\n",
    "    \n",
    "    gmm_iso = GMM(n_states, train_data, 'isotropic')\n",
    "    gmm_iso.train(n_iter)\n",
    "    logl_table[2,0] = gmm_iso.log_likelihood(train_data)\n",
    "    logl_table[2,1] = gmm_iso.log_likelihood(test_data)\n",
    "\n",
    "    k_means = K_Means(n_states, train_data)\n",
    "    k_means.train(n_iter)\n",
    "    logl_table[3,0] = k_means.log_likelihood(train_data)\n",
    "    logl_table[3,1] = k_means.log_likelihood(test_data)\n",
    "    \n",
    "    if table:\n",
    "        np.savetxt('latex/logl_table.csv', logl_table, delimiter=' & ', \n",
    "                   fmt='%2.2e', newline=' \\\\\\\\\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_plot_data(data, plot=False):\n",
    "    \n",
    "    hmm = HMM(MEAN, COV, PI, TRANSIT_MATRIX)\n",
    "    hmm.train(data)\n",
    "    \n",
    "    seq, scores = hmm.viterbi(data)\n",
    "    \n",
    "    if plot:\n",
    "        \"\"\"\n",
    "        Show data, cluster means and cluster assignments for each cluster after assignment\n",
    "        using the Viterbi algorithm.\n",
    "        \"\"\"\n",
    "        colors = ['r','g','b','y']\n",
    "        datacolor = []\n",
    "        datalabels = seq\n",
    "        for i in range(len(datalabels)):\n",
    "            datacolor.append(colors[datalabels[i]])\n",
    "        plt.scatter(data[:,0], data[:,1],marker='.', alpha=0.5,c=datacolor)\n",
    "        plt.scatter(hmm.mean[:,0], hmm.mean[:,1],marker='X',s=150,facecolor=colors, edgecolors='k',lw=2)\n",
    "        plt.xlabel('feature 1')\n",
    "        plt.ylabel('feature 2')\n",
    "        plt.savefig('latex/figures/viterbi_2d.png')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_parameters_inference(train_data, test_data, plot=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points with which the HMM is trained.\n",
    "    test_data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points for which we test the HMM after\n",
    "        training.\n",
    "    plot : boolean\n",
    "        Decides whether or not a plot is created.\n",
    "    \"\"\"\n",
    "    hmm = HMM(MEAN, COV, PI, TRANSIT_MATRIX)\n",
    "    #learning parameters\n",
    "    hmm.train(train_data)\n",
    "    \n",
    "    hmm._e_step(test_data)\n",
    "    \n",
    "    z = np.zeros_like(hmm.gamma)\n",
    "    for i in range(z.shape[0]):\n",
    "        for j in range(z.shape[1]):\n",
    "            if hmm.gamma[i,j] == max(hmm.gamma[i]):\n",
    "                z[i,j] = 1\n",
    "            else:\n",
    "                z[i,j] = 0\n",
    "\n",
    "    if plot:\n",
    "        plot_data_prob_by_state(hmm.gamma, 'real_param_inference')\n",
    "        plot_data_prob_by_state(z, 'real_param_choice')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_ml_sequence_plot(train_data, test_data, plot=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points with which the HMM is trained.\n",
    "    test_data : array-like, shape = [n_pts, n_dims]\n",
    "        Data points for which we test the HMM after\n",
    "        training.\n",
    "    plot : boolean\n",
    "        Decides whether or not a plot is created.\n",
    "    \"\"\"\n",
    "    hmm = HMM(MEAN, COV, PI, TRANSIT_MATRIX)\n",
    "    #learning parameters\n",
    "    hmm.train(train_data)\n",
    "    \n",
    "    seq, scores = hmm.viterbi(test_data)\n",
    "\n",
    "    z = np.zeros((test_data.shape[0],TRANSIT_MATRIX.shape[0]))\n",
    "    for t in range(z.shape[0]):\n",
    "        z[t, seq[t]] = 1\n",
    "    \n",
    "    if plot:\n",
    "        plot_data_prob_by_state(z, 'real_param_viterbi')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data_prob_by_state(data, figname):\n",
    "    # Four subplots sharing both x/y axes\n",
    "    dom = np.arange(100)\n",
    "    f, (ax1, ax2, ax3, ax4) = plt.subplots(4, sharex=True, sharey=True,\n",
    "                                           figsize=(20, 10))\n",
    "\n",
    "    #setting colors of bars (from 0 (red) to 1 (green))\n",
    "    colors0 = cm.RdYlGn(data[:,0])\n",
    "    colors1 = cm.RdYlGn(data[:,1])\n",
    "    colors2 = cm.RdYlGn(data[:,2])\n",
    "    colors3 = cm.RdYlGn(data[:,3])\n",
    "\n",
    "    #making bar graphs for each state\n",
    "    ax1.bar(dom, data[:100,0], color=colors0)\n",
    "    ax2.bar(dom, data[:100,1], color=colors1)\n",
    "    ax3.bar(dom, data[:100,2], color=colors2)\n",
    "    ax4.bar(dom, data[:100,3], color=colors3)\n",
    "    # Fine-tune figure; make subplots close to each other and hide x ticks for\n",
    "    # all but bottom plot.\n",
    "    f.subplots_adjust(hspace=0.1)\n",
    "    plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
    "\n",
    "    #trick to show axes titles\n",
    "    f.text(0.5, 0.08, 't', ha='center', fontsize=15)\n",
    "    f.text(0.08, 0.5, 'Probability for each state', va='center',\n",
    "           rotation='vertical', fontsize=20)\n",
    "\n",
    "    plt.savefig('latex/figures/%s.png'%figname)\n",
    "    plt.show()\n",
    "    plt.clf()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxczMMuGCYmZmmfj/RsTve/mHEMQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aa48ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ac8a048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading datasets\n",
    "    train_data = np.loadtxt('hwk4data/EMGaussian.train')\n",
    "    test_data = np.loadtxt('hwk4data/EMGaussian.test')\n",
    "    \n",
    "    # Number 2\n",
    "    #fake_parameters_inference(test_data, plot=True)\n",
    "    \n",
    "    # Number 5\n",
    "    show_log_likelihood(train_data, test_data, plot=True)\n",
    "    \n",
    "    # Number 6\n",
    "    make_table_log_likelihood(train_data, test_data, table=True)\n",
    "    \n",
    "    # Number 8\n",
    "    #viterbi_plot_data(train_data, plot=True)\n",
    "    \n",
    "    # Numbers 9 & 10\n",
    "    #real_parameters_inference(train_data, test_data, plot=True)\n",
    "    \n",
    "    # Number 10\n",
    "    #viterbi_ml_sequence_plot(train_data, test_data, plot=True)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
