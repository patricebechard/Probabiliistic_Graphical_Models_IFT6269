{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1969,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the implementation of the Fisher LDA learning algorithm (question 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1970,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class fisher_lda:\n",
    "    \"\"\"Implementation of the fisher LDA learning algorithm\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self,train_data):\n",
    "        \"\"\"Learning the parameters using the maximum likelihood estimator\"\"\"\n",
    "\n",
    "        train_data0 = np.array([i[:-1] for i in train_data if i[-1] == 0])\n",
    "        train_data1 = np.array([i[:-1] for i in train_data if i[-1] == 1])\n",
    "\n",
    "        N0 = len(train_data0)\n",
    "        N1 = len(train_data1)\n",
    "\n",
    "        self.pi = N1 / (N0 + N1)\n",
    "        self.mu0 = (1 / N0) * np.sum(train_data0, axis=0)\n",
    "        self.mu1 = (1 / N1) * np.sum(train_data1, axis=0)\n",
    "        \n",
    "        sigmatilde0 = (1/N0) * np.matmul((train_data0-self.mu0).T,(train_data0-self.mu0))\n",
    "        sigmatilde1 = (1/N1) * np.matmul((train_data1-self.mu1).T,(train_data1-self.mu1))\n",
    "\n",
    "        self.Sigma = (1 - self.pi) * sigmatilde0 + self.pi * sigmatilde1\n",
    "        self.Sigmainv = np.linalg.inv(self.Sigma)\n",
    " \n",
    "    def compute_predictions(self,test_data):\n",
    "        \n",
    "        posterior = np.matmul((np.matmul(self.mu1, self.Sigmainv) - np.matmul(self.mu0, self.Sigmainv)),test_data.T) + \\\n",
    "                (0.5 * np.matmul(np.matmul(self.mu0,self.Sigmainv),self.mu0.T) - \\\n",
    "                 0.5 * np.matmul(np.matmul(self.mu1,self.Sigmainv),self.mu1.T) + \\\n",
    "                 np.log(self.pi/(1-self.pi)))\n",
    "        posterior = np.array([sigmoid(posterior)]).T\n",
    "        \n",
    "        return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the implementation of the logistic regression learning algorithm (question 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1971,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class logistic_regression:\n",
    "    \"\"\"Implementation of the logistic regression learning algorithm\"\"\"\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = np.ones((np.shape(data)[0],np.shape(data)[1]+1))      #array has a column of ones for the bias\n",
    "        self.X[:,:-1] = data\n",
    "        self.y = labels\n",
    "        self.params = np.zeros((np.shape(self.X)[-1],1))\n",
    "\n",
    "    def train(self,train_data):\n",
    "        \"\"\"Learning the parameters using the IRLS algorithm (2nd degree method)\"\"\"\n",
    "        update = np.ones_like(self.params)*1000\n",
    "        i = 0\n",
    "        while update.sum() > 0.001 and i < 5:  \n",
    "            \"\"\"We update until convergence up to a maximum of 5 iterations\"\"\"\n",
    "            mu = sigmoid(np.matmul(self.params.T,self.X.T)).T\n",
    "            D = mu * (1-mu)\n",
    "            Hessian = np.linalg.inv(np.matmul((self.X*D).T,self.X))\n",
    "            gradlost = np.matmul(self.X.T,(mu-self.y))\n",
    "\n",
    "            update = np.matmul(Hessian, gradlost)\n",
    "            self.params -= update\n",
    "            i += 1\n",
    "\n",
    "    def compute_predictions(self,test_data):\n",
    "        return sigmoid(np.matmul(test_data,self.params[:-1]) + self.params[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the implementation of the linear regression learning algorithm (question 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1972,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class linear_regression:\n",
    "    \"\"\"Implementation of the linear regression learning algorithm\"\"\"\n",
    "\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = np.ones((np.shape(data)[0],np.shape(data)[1]+1))      #array has a column of ones for the bias\n",
    "        self.X[:,:-1] = data\n",
    "        self.y = labels\n",
    "        self.params = np.zeros((np.shape(self.X)[-1],1))\n",
    "\n",
    "    def train(self, train_data):\n",
    "        \"\"\"Learning the parameters using the normal equations\"\"\"\n",
    "        matinv = np.linalg.inv(np.matmul(self.X.T,self.X))\n",
    "        self.params = np.matmul(np.matmul(matinv,self.X.T),self.y)\n",
    "\n",
    "    def compute_predictions(self, test_data):\n",
    "        return np.matmul(test_data,self.params[:-1]) + self.params[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the implementation of the QDA learning algorithm (question 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1973,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class qda:\n",
    "    \"\"\"Implementation of the QDA learning algorithm\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, train_data):\n",
    "        \"\"\"Learning the parameters using the maximum likelihood estimator\"\"\"\n",
    "        train_data0 = np.array([i[:-1] for i in train_data if i[-1] == 0])\n",
    "        train_data1 = np.array([i[:-1] for i in train_data if i[-1] == 1])\n",
    "\n",
    "        N0 = len(train_data0)\n",
    "        N1 = len(train_data1)\n",
    "\n",
    "        self.pi = N1 / (N0 + N1)\n",
    "        self.mu0 = (1 / N0) * np.sum(train_data0, axis=0)\n",
    "        self.mu1 = (1 / N1) * np.sum(train_data1, axis=0)\n",
    "        \n",
    "        self.sigmatilde0 = (1/N0) * np.matmul((train_data0-self.mu0).T,(train_data0-self.mu0))\n",
    "        self.sigmatilde1 = (1/N1) * np.matmul((train_data1-self.mu1).T,(train_data1-self.mu1))\n",
    "        \n",
    "        self.detsigma0 = np.linalg.det(self.sigmatilde0)\n",
    "        self.detsigma1 = np.linalg.det(self.sigmatilde1)\n",
    "        \n",
    "        self.sigma1inv = np.linalg.inv(self.sigmatilde1)\n",
    "        self.sigma0inv = np.linalg.inv(self.sigmatilde0)\n",
    "        \n",
    "    def compute_predictions(self, test_data):\n",
    "        \n",
    "        square_term = -0.5 * np.sum(np.matmul(test_data,(self.sigma1inv-self.sigma0inv))*test_data, axis=1)\n",
    "    \n",
    "        posterior = np.matmul((np.matmul(self.mu1, self.sigma1inv) - np.matmul(self.mu0, self.sigma0inv)),test_data.T) + \\\n",
    "                (0.5 * np.matmul(np.matmul(self.mu0,self.sigma0inv),self.mu0.T) - \\\n",
    "                 0.5 * np.matmul(np.matmul(self.mu1,self.sigma1inv),self.mu1.T) + \\\n",
    "                 np.log(self.pi/(1-self.pi)) + \\\n",
    "                 0.5 * np.log(self.detsigma0/self.detsigma1))\n",
    "            \n",
    "        posterior += square_term\n",
    "        posterior = np.array([sigmoid(posterior)]).T\n",
    "        \n",
    "        return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1974,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used to plot the data and the decision boundary for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1975,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_data(fit, data, dataset, model):\n",
    "    data0 = np.array([i[:-1] for i in data if i[-1] == 0])\n",
    "    data1 = np.array([i[:-1] for i in data if i[-1] == 1])\n",
    "    \n",
    "    domainx = np.linspace(np.min(train_data[:,0]),np.max(train_data[:,0]))\n",
    "    domainy = np.linspace(np.min(train_data[:,1]),np.max(train_data[:,1]))\n",
    "    X,Y = np.meshgrid(domainx, domainy)\n",
    "    domain = np.array([[i,j] for i in domainx for j in domainy])\n",
    "    prediction = fit.compute_predictions(domain)\n",
    "\n",
    "    plt.plot(data0[:,0], data0[:,1], 'r*',label='class 0')\n",
    "    plt.plot(data1[:,0], data1[:,1], 'bo',label = 'class 1')\n",
    "    \n",
    "    plt.contour(X,Y,np.reshape(prediction,X.shape).T, levels=[0.5])\n",
    "    \n",
    "    plt.xlabel('feature 1')\n",
    "    plt.ylabel('feature 2')\n",
    "    plt.legend(fancybox=True)\n",
    "    plt.savefig('latex/figures/fig_%s_dataset%s.png'%(model, dataset))\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used to compute misclassification error for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1976,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_error(fit, train_data, test_data, dataset):\n",
    "    prediction_train = np.around(fit.compute_predictions(train_data[:,:-1]))\n",
    "    prediction_test = np.around(fit.compute_predictions(test_data[:,:-1]))\n",
    "    \n",
    "    error_train = np.sum(np.abs(prediction_train-train_data[:,-1:]))*100/len(train_data)\n",
    "    error_test = np.sum(np.abs(prediction_test-test_data[:,-1:]))*100/len(test_data)\n",
    "                                     \n",
    "    print(\"Misclassification error (training): %.2f%%\" %error_train)\n",
    "    print(\"Misclassification error (test): %.2f%%\\n\" %error_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1977,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : fisher_lda\n",
      "Dataset : A\n",
      "\n",
      "The parameters learned from the dataset A for the fisher_lda model are the following:\n",
      "pi = 0.333333\n",
      "mu_0 = [ 2.89970947 -0.893874  ]\n",
      "mu_1 = [-2.69232004  0.866042  ]\n",
      "Sigma = \n",
      "[[ 2.44190897 -1.13194024]\n",
      " [-1.13194024  0.61375465]]\n",
      "Misclassification error (training): 1.33%\n",
      "Misclassification error (test): 2.00%\n",
      "\n",
      "Dataset : B\n",
      "\n",
      "The parameters learned from the dataset B for the fisher_lda model are the following:\n",
      "pi = 0.500000\n",
      "mu_0 = [ 3.34068896 -0.83546333]\n",
      "mu_1 = [-3.21670734  1.08306733]\n",
      "Sigma = \n",
      "[[ 3.34623467 -0.13516489]\n",
      " [-0.13516489  1.73807475]]\n",
      "Misclassification error (training): 3.00%\n",
      "Misclassification error (test): 4.15%\n",
      "\n",
      "Dataset : C\n",
      "\n",
      "The parameters learned from the dataset C for the fisher_lda model are the following:\n",
      "pi = 0.625000\n",
      "mu_0 = [ 2.79304824 -0.83838667]\n",
      "mu_1 = [-2.94232885 -0.9578284 ]\n",
      "Sigma = \n",
      "[[ 2.88039225 -0.63405081]\n",
      " [-0.63405081  5.19952435]]\n",
      "Misclassification error (training): 5.50%\n",
      "Misclassification error (test): 4.23%\n",
      "\n",
      "Model : logistic_regression\n",
      "Dataset : A\n",
      "\n",
      "The parameters learned from the dataset A for the logistic_regression model are\n",
      "w = \n",
      "[[-5.54892822]\n",
      " [-9.01885324]] \n",
      "and \n",
      "b = -0.717570\n",
      "\n",
      "Misclassification error (training): 0.67%\n",
      "Misclassification error (test): 2.47%\n",
      "\n",
      "Dataset : B\n",
      "\n",
      "The parameters learned from the dataset B for the logistic_regression model are\n",
      "w = \n",
      "[[-0.70215844]\n",
      " [ 0.36482568]] \n",
      "and \n",
      "b = 0.154877\n",
      "\n",
      "Misclassification error (training): 2.67%\n",
      "Misclassification error (test): 4.00%\n",
      "\n",
      "Dataset : C\n",
      "\n",
      "The parameters learned from the dataset C for the logistic_regression model are\n",
      "w = \n",
      "[[-1.69277685]\n",
      " [ 0.37903323]] \n",
      "and \n",
      "b = 0.569075\n",
      "\n",
      "Misclassification error (training): 4.00%\n",
      "Misclassification error (test): 2.63%\n",
      "\n",
      "Model : linear_regression\n",
      "Dataset : A\n",
      "\n",
      "The parameters learned from the dataset A for the linear_regression model are\n",
      "w = \n",
      "[[-0.2640075 ]\n",
      " [-0.37259311]] \n",
      "and \n",
      "b = 0.492292\n",
      "\n",
      "Misclassification error (training): 1.33%\n",
      "Misclassification error (test): 2.13%\n",
      "\n",
      "Dataset : B\n",
      "\n",
      "The parameters learned from the dataset B for the linear_regression model are\n",
      "w = \n",
      "[[-0.10424575]\n",
      " [ 0.05179118]] \n",
      "and \n",
      "b = 0.500050\n",
      "\n",
      "Misclassification error (training): 3.33%\n",
      "Misclassification error (test): 4.20%\n",
      "\n",
      "Dataset : C\n",
      "\n",
      "The parameters learned from the dataset C for the linear_regression model are\n",
      "w = \n",
      "[[-0.12769333]\n",
      " [-0.01700142]] \n",
      "and \n",
      "b = 0.508400\n",
      "\n",
      "Misclassification error (training): 6.25%\n",
      "Misclassification error (test): 4.60%\n",
      "\n",
      "Model : qda\n",
      "Dataset : A\n",
      "\n",
      "The parameters learned from the dataset A for the qda model are the following:\n",
      "pi = 0.333333\n",
      "mu_0 = [ 2.89970947 -0.893874  ]\n",
      "mu_1 = [-2.69232004  0.866042  ]\n",
      "Sigma_0 = \n",
      "[[ 2.31065259 -1.04748461]\n",
      " [-1.04748461  0.57578403]]\n",
      "Sigma_1 = \n",
      "[[ 2.70442172 -1.3008515 ]\n",
      " [-1.3008515   0.68969588]]\n",
      "Misclassification error (training): 0.67%\n",
      "Misclassification error (test): 2.00%\n",
      "\n",
      "Dataset : B\n",
      "\n",
      "The parameters learned from the dataset B for the qda model are the following:\n",
      "pi = 0.500000\n",
      "mu_0 = [ 3.34068896 -0.83546333]\n",
      "mu_1 = [-3.21670734  1.08306733]\n",
      "Sigma_0 = \n",
      "[[ 2.53885859  1.0642112 ]\n",
      " [ 1.0642112   2.96007891]]\n",
      "Sigma_1 = \n",
      "[[ 4.15361075 -1.33454097]\n",
      " [-1.33454097  0.51607059]]\n",
      "Misclassification error (training): 1.33%\n",
      "Misclassification error (test): 2.00%\n",
      "\n",
      "Dataset : C\n",
      "\n",
      "The parameters learned from the dataset C for the qda model are the following:\n",
      "pi = 0.625000\n",
      "mu_0 = [ 2.79304824 -0.83838667]\n",
      "mu_1 = [-2.94232885 -0.9578284 ]\n",
      "Sigma_0 = \n",
      "[[ 2.89913927  1.24581553]\n",
      " [ 1.24581553  2.92475448]]\n",
      "Sigma_1 = \n",
      "[[ 2.86914403 -1.76197061]\n",
      " [-1.76197061  6.56438626]]\n",
      "Misclassification error (training): 5.25%\n",
      "Misclassification error (test): 3.83%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106ed2438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    models = ['fisher_lda','logistic_regression','linear_regression','qda']\n",
    "    datasets = ['A','B','C']\n",
    "    \n",
    "    for model in models:\n",
    "        print('Model : %s'%model)\n",
    "         \n",
    "        for dataset in datasets:\n",
    "            print('Dataset : %s\\n'%dataset)\n",
    "            \n",
    "            train_data = np.loadtxt('data/classification%s.train'%dataset)\n",
    "            test_data = np.loadtxt('data/classification%s.test'%dataset) \n",
    "        \n",
    "            if model == 'fisher_lda':\n",
    "                fit = fisher_lda()\n",
    "            elif model == 'logistic_regression':\n",
    "                fit = logistic_regression(train_data[:,:-1],train_data[:,-1:])\n",
    "            elif model == 'linear_regression':\n",
    "                fit = linear_regression(train_data[:,:-1],train_data[:,-1:])\n",
    "            elif model == 'qda':\n",
    "                fit = qda()\n",
    "            else:\n",
    "                raise Exception ('model not supported')\n",
    "            \n",
    "            fit.train(train_data)\n",
    "            \n",
    "            if model in ['logistic_regression','linear_regression']:\n",
    "                print('The parameters learned from the dataset %s for the %s model are\\nw = \\n%s \\nand \\nb = %f\\n'\n",
    "                        %(dataset,model,np.array_str(fit.params[:-1]),fit.params[-1:]))\n",
    "            elif model == 'fisher_lda':\n",
    "                print('The parameters learned from the dataset %s for the %s model are the following:'%(dataset,model))\n",
    "                print('pi = %f'%fit.pi)\n",
    "                print('mu_0 = %s'%np.array_str(fit.mu0))\n",
    "                print('mu_1 = %s'%np.array_str(fit.mu1))\n",
    "                print('Sigma = \\n%s'%np.array_str(fit.Sigma))\n",
    "            elif model == 'qda':\n",
    "                print('The parameters learned from the dataset %s for the %s model are the following:'%(dataset,model))\n",
    "                print('pi = %f'%fit.pi)\n",
    "                print('mu_0 = %s'%np.array_str(fit.mu0))\n",
    "                print('mu_1 = %s'%np.array_str(fit.mu1))\n",
    "                print('Sigma_0 = \\n%s'%np.array_str(fit.sigmatilde0))\n",
    "                print('Sigma_1 = \\n%s'%np.array_str(fit.sigmatilde1))\n",
    "                \n",
    "            \n",
    "            plot_data(fit,train_data, dataset, model)\n",
    "            compute_error(fit, train_data, test_data, dataset)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
