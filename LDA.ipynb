{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "from numpy.random import dirichlet, multinomial\n",
    "import sys\n",
    "np.random.seed(0)\n",
    "np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Generate toy documents \"\"\"\n",
    "K = 3\n",
    "D = 1000\n",
    "N = 10\n",
    "V = 12\n",
    "\n",
    "# 1) Set alpha and beta for generative model\n",
    "alpha = np.array([2, 2, 2])\n",
    "beta = np.array([[0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0 ,0, 0.25, 0.25, 0.25, 0.25]])\n",
    "\n",
    "# 2) Using alpha, sample theta_d for each of the D documents\n",
    "theta = dirichlet(alpha, D)\n",
    "\n",
    "# 3) Sample topic z_dn for each of the N words of each of the D documents\n",
    "# 4) Sample word w_dn from topic beta_k corresponding to z_dn\n",
    "W = []\n",
    "for d in range(D):\n",
    "    # Counts of words for each topic in doc d (e.g [4, 3, 3] for N = 10)\n",
    "    z_d = multinomial(N, theta[d])\n",
    "    # One hot matrix of words for doc d\n",
    "    w_d = []\n",
    "    for k in xrange(K):\n",
    "        for i in xrange(z_d[k]):\n",
    "            w_d.append(multinomial(1, beta[k]))\n",
    "    W.append(np.array(w_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma, digamma\n",
    "from scipy.stats import multinomial as multinomial_distribution\n",
    "from scipy.stats import dirichlet as dirichlet_distribution\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class LDA():\n",
    "    def __init__(self, K=3):\n",
    "        self.K = K\n",
    "        # Initialize alpha\n",
    "        self.alpha = np.ones(self.K) / self.K\n",
    "        \n",
    "    # W is a list of D numpy arrays of shape (N_d, V)\n",
    "    def fit(self, W, max_iter=51, threshold=1e-4, verbose=True):\n",
    "        self.D = len(W)\n",
    "        self.V = W[0].shape[1]\n",
    "        \n",
    "        # Randomly initialize variational parameters\n",
    "        # TODO best way to initialize gamma, phi ?\n",
    "        self.gamma = np.random.rand(self.D, self.K)\n",
    "        self.phi = [np.random.rand(w_d.shape[0], self.K) for w_d in W]\n",
    "        self.phi = [phi_d / np.sum(phi_d, axis=1, keepdims=True) \\\n",
    "            for phi_d in self.phi]\n",
    "        \n",
    "        # Initialize beta\n",
    "        self._updateBeta(W)\n",
    "        \n",
    "        # Keep track of previous value of lower bound on log likelihood\n",
    "        old_bound = self._computeBound(W)\n",
    "        \n",
    "        # Update phi, gamma, beta iteratively while the bound improves\n",
    "        for i in xrange(max_iter):\n",
    "            self._updatePhi(W)\n",
    "            self._updateGamma()\n",
    "            self._updateBeta(W)\n",
    "            current_bound = self._computeBound(W)\n",
    "            if verbose and i % 10 == 0:\n",
    "                print \"Iter {}, bound = {}\".format(i, old_bound)\n",
    "            if (current_bound - old_bound) < threshold:\n",
    "                break\n",
    "            old_bound = current_bound\n",
    "                \n",
    "        # Return final state of beta and variational parameters\n",
    "        self.components_ = self.beta # To match sklearn API\n",
    "        return self.beta, self.gamma, self.phi\n",
    "        \n",
    "    def _updateGamma(self):\n",
    "        for d in xrange(self.D):\n",
    "            self.gamma[d] = self.alpha + np.sum(self.phi[d], axis=0)\n",
    "    \n",
    "    def _updatePhi(self, W):\n",
    "        for d in xrange(self.D):\n",
    "            self.phi[d] = W[d].dot(self.beta.T) * \\\n",
    "                np.exp(digamma(self.gamma[d]))\n",
    "            self.phi[d] = normalize(self.phi[d], norm=\"l1\", axis=1)\n",
    "    \n",
    "    def _updateBeta(self, W):\n",
    "        self.beta = np.concatenate(self.phi, axis=0).T.dot(\n",
    "            np.concatenate(W, axis=0))\n",
    "        self.beta = normalize(self.beta, norm=\"l1\", axis=1)\n",
    "        \n",
    "    def _computeBound(self, W):\n",
    "        # Avoid recomputing expensive expressions\n",
    "        C1 = digamma(self.gamma)\n",
    "        C2 = digamma(np.sum(self.gamma, axis=1))\n",
    "        \n",
    "        # First three terms correspond to expectation of complete\n",
    "        # log likelihood under variational distribution\n",
    "        term1 = self.D * (np.log(gamma(np.sum(self.alpha))) - \\\n",
    "            np.sum(np.log(gamma(self.alpha)))) + \\\n",
    "            np.sum(C1 * (self.alpha - 1)) - np.sum(C2) * \\\n",
    "            np.sum(self.alpha - 1)\n",
    "        term2 = np.sum(np.array([np.sum(phi_d, axis=0) \\\n",
    "            for phi_d in self.phi]) * C1) - \\\n",
    "            np.array([np.sum(phi_d) for phi_d in self.phi]).dot(C2)\n",
    "        term3 = np.sum(np.log(self.beta) * np.concatenate(\n",
    "            self.phi, axis=0).T.dot(np.concatenate(W, axis=0)))\n",
    "        \n",
    "        # Sum of entropies of dirichlet variational posteriors\n",
    "        #term4 = -np.sum(np.log(gamma(np.sum(self.gamma, axis=1)))) + \\\n",
    "        #    np.sum(np.log(gamma(self.gamma))) - \\\n",
    "        #    np.sum((self.gamma - 1) * C1) + \\\n",
    "        #    C2.dot(np.sum(self.gamma - 1, axis=1))\n",
    "        term4 = 0\n",
    "        for d in xrange(self.D):\n",
    "            term4 += dirichlet_distribution.entropy(self.gamma[d])\n",
    "        \n",
    "        # Sum of entropies of multinomial variational posteriors\n",
    "        #term5 = -np.sum(np.concatenate(self.phi, axis=0) * \\\n",
    "        #    np.log(np.concatenate(self.phi, axis=0)))\n",
    "        term5 = np.sum(multinomial_distribution.entropy(\n",
    "            1, np.concatenate(self.phi, axis=0)))\n",
    "        \n",
    "        # Return lower bound on likelihood \n",
    "        # (averaged over total number of words)\n",
    "        L = term1 + term2 + term3 + term4 + term5\n",
    "        return L / np.concatenate(W, axis=0).shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LDA_gibbs:\n",
    "\n",
    "    def __init__(self, n_topics, alpha=None, id2word=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_topics : int\n",
    "            number of topics for the model\n",
    "        alpha : array-like, shape=(n_topics)\n",
    "            hyperparameter for the model\n",
    "            controls the likelihood of a topic being assigned\n",
    "            to a document\n",
    "        id2word : dictionary\n",
    "            used to map id of word to real world\n",
    "            helps to monitor progress of learning\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_topics = n_topics\n",
    "        #initialize alpha\n",
    "        if alpha is None:\n",
    "            # default value (from Heinrich, 2005)\n",
    "            self.alpha = np.ones(self.n_topics) / self.n_topics\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        self.sumalpha = np.sum(self.alpha)\n",
    "        self.id2word = id2word\n",
    "        \n",
    "    def fit(self, docs, eta=None, niter=100):\n",
    "        \"\"\"\n",
    "        Learning the parameters for the model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        docs : list of array-like elements, length = n_docs\n",
    "            each element is an array of dimensions n_words\n",
    "            by number of words in vocabulary (n_vocab). The \n",
    "            number of words varies from one text to the other.\n",
    "        threshold : float\n",
    "            used to stop fitting when the model has converged\n",
    "            to a final state.\n",
    "        eta : array-like, shape=(n_vocab)\n",
    "            hyperparameter for the model\n",
    "            controls the likelihood of a word being assigned \n",
    "            to a topic\n",
    "        niter : int\n",
    "            number of gibbs sampling iterations for the training\n",
    "            of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_docs = len(docs)\n",
    "        self.n_vocab = docs[0].shape[1]\n",
    "        #initialize eta\n",
    "        if eta is None:\n",
    "            #default value (from Heinrich, 2005)\n",
    "            self.eta = np.ones(self.n_vocab) * 0.01\n",
    "        else:\n",
    "            self.eta = eta\n",
    "        self.sumeta = np.sum(self.eta)\n",
    "        \n",
    "        #initializing counters\n",
    "        self._initialize_counters_train(docs)\n",
    "        \n",
    "        for i in xrange(niter):\n",
    "            \n",
    "            if i%10 == 0 and i != 0:\n",
    "                #monitoring progress by showing top words for each topic\n",
    "                print i\n",
    "                self._estimate_parameters_train()\n",
    "                if self.id2word:\n",
    "                    visualizeTopics(self, id2word)\n",
    "\n",
    "            self._gibbs_sampling_fit_train(docs)\n",
    "\n",
    "        #update parameters at the end of training\n",
    "        self._estimate_parameters_train()\n",
    "        \n",
    "    def query_sampling(self, queries, niter=100):\n",
    "        \n",
    "        self.n_queries = len(queries)\n",
    "        \n",
    "        #initializing counters\n",
    "        self._initialize_counters_test(queries)\n",
    "        \n",
    "        for i in xrange(niter):\n",
    "            self._gibbs_sampling_fit_test(queries)\n",
    "        \n",
    "        #update parameters at the end\n",
    "        self._estimate_parameters_test()\n",
    "        \n",
    "    def perplexity_train(self, docs):\n",
    "        \n",
    "        log_likelihood = 0\n",
    "        tot_num_words = 0\n",
    "        for i in range(self.n_docs):\n",
    "\n",
    "            words_occurence = np.sum(docs[i], axis=0)\n",
    "            \n",
    "            log_likelihood += np.sum(words_occurence * \\\n",
    "                                     np.log(np.sum((self.beta.T * self.theta[i]), axis=1)))\n",
    "            \n",
    "            tot_num_words += len(docs[i])\n",
    "                    \n",
    "        perplexity = np.exp(-log_likelihood / tot_num_words)\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "    \n",
    "    def perplexity_test(self, queries):\n",
    "        \n",
    "        log_likelihood = 0\n",
    "        tot_num_words = 0\n",
    "        for i in range(self.n_queries):\n",
    "\n",
    "            words_occurence = np.sum(queries[i], axis=0)\n",
    "            \n",
    "            log_likelihood += np.sum(words_occurence * \\\n",
    "                                     np.log(np.sum((self.beta.T * self.theta_test[i]), axis=1)))\n",
    "            \n",
    "            tot_num_words += len(queries[i])\n",
    "                    \n",
    "        perplexity = np.exp(-log_likelihood / tot_num_words)\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    def _initialize_counters_train(self, docs):\n",
    "        \"\"\"Initializing counters randomly for the training phase\"\"\"\n",
    "\n",
    "        #initialize arrays to 0\n",
    "        self.topic_doc_count = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.term_topic_count = np.zeros((self.n_topics, self.n_vocab))\n",
    "\n",
    "        #keep track of topic assignment for each word in memory\n",
    "        self.topic_assign = []\n",
    "\n",
    "        for i in xrange(self.n_docs):\n",
    "\n",
    "            n_words = len(docs[i])\n",
    "\n",
    "            #sample topic randomly for each word in document\n",
    "            topic = np.random.randint(self.n_topics, size=n_words)\n",
    "            self.topic_assign.append(topic)\n",
    "\n",
    "            #count number of words belonging to each topic\n",
    "            self.topic_doc_count[i] = np.bincount(topic, minlength=self.n_topics)\n",
    "\n",
    "            #count word occurence for each topic\n",
    "            self.term_topic_count += np.array([np.sum(docs[i][np.where(topic==j)], axis=0)\\\n",
    "                                               for j in range(self.n_topics)])\n",
    "\n",
    "            #sum of number of words in each topic\n",
    "            self.term_topic_sum = np.sum(self.term_topic_count, axis=1)\n",
    "            \n",
    "    def _initialize_counters_test(self, queries):\n",
    "        \"\"\"Initializing counters randomly for the test phase\"\"\"\n",
    "        \n",
    "        #initialize arrays to 0\n",
    "        self.topic_doc_count_test = np.zeros((self.n_queries,self.n_topics))\n",
    "        self.term_topic_count_test = np.zeros((self.n_topics, self.n_vocab))\n",
    "        \n",
    "        #keep track of topic assignment for each word in memory\n",
    "        self.topic_assign_test = []\n",
    "        \n",
    "        for i in xrange(self.n_queries):\n",
    "            \n",
    "            n_words = len(queries[i])\n",
    "            \n",
    "            #sample topic randomly for each word in query\n",
    "            topic = np.random.randint(self.n_topics, size=n_words)\n",
    "            self.topic_assign_test.append(topic)\n",
    "            \n",
    "            #count number fo words belonging to each topic\n",
    "            self.topic_doc_count_test[i] = np.bincount(topic, minlength=self.n_topics)\n",
    "            \n",
    "            #count word occurence for each topic\n",
    "            self.term_topic_count_test += np.array([np.sum(queries[i][np.where(topic==j)], axis=0)\\\n",
    "                                                    for j in range(self.n_topics)])\n",
    "            \n",
    "            #sum of number of words in each topic\n",
    "            self.term_topic_sum_test = np.sum(self.term_topic_count_test, axis=1)\n",
    "    \n",
    "    def _gibbs_sampling_fit_train(self, docs):\n",
    "        \"\"\"Update counters for each word using gibbs sampling\"\"\"\n",
    "        \n",
    "        for i in xrange(self.n_docs):\n",
    "            \n",
    "            n_words = len(docs[i])\n",
    "            \n",
    "            for j in xrange(n_words):\n",
    "                \n",
    "                #decrement associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] -= 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] -= docs[i][j]\n",
    "                self.term_topic_sum[self.topic_assign[i][j]] -= 1\n",
    "                \n",
    "                #sample new topic\n",
    "                word_idx = np.argmax(docs[i][j])               \n",
    "                prob = (self.topic_doc_count[i] + self.alpha) * \\\n",
    "                        (self.term_topic_count[:, word_idx] + self.eta[word_idx]) /\\\n",
    "                        (np.sum(self.term_topic_count, axis=1) + self.sumeta) /\\\n",
    "                        ((n_words-1) + self.sumalpha)\n",
    "                prob /= np.sum(prob)                      #normalize\n",
    "                prob = np.cumsum(prob)                    #cdf\n",
    "                self.topic_assign[i][j] = np.argmax(prob > np.random.random())\n",
    "\n",
    "                #increment associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] += 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] += docs[i][j]  \n",
    "                self.term_topic_sum[self.topic_assign[i][j]] += 1\n",
    "                \n",
    "    def _gibbs_sampling_fit_test(self, queries):\n",
    "        \n",
    "        for i in xrange(self.n_queries):\n",
    "            \n",
    "            n_words = len(queries[i])\n",
    "            \n",
    "            for j in xrange(n_words):\n",
    "                \n",
    "                #decrement associated counters\n",
    "                self.topic_doc_count_test[i][self.topic_assign_test[i][j]] -= 1\n",
    "                self.term_topic_count_test[self.topic_assign_test[i][j]] -= queries[i][j]\n",
    "                self.term_topic_sum_test[self.topic_assign_test[i][j]] -= 1\n",
    "                \n",
    "                #sample new topic\n",
    "                word_idx = np.argmax(queries[i][j])\n",
    "                \n",
    "                prob = (self.topic_doc_count_test[i] + self.alpha) * \\\n",
    "                        (self.term_topic_count_test[:,word_idx] + \\\n",
    "                             self.term_topic_count[:,word_idx] + \\\n",
    "                             self.eta[word_idx]) /\\\n",
    "                        (np.sum(self.term_topic_count_test, axis=1) + \\\n",
    "                             self.term_topic_sum + \\\n",
    "                             self.sumeta) /\\\n",
    "                        ((n_words + self.sumalpha))\n",
    "                prob /= np.sum(prob)              #normalize\n",
    "                prob = np.cumsum(prob)            #cdf\n",
    "                self.topic_assign_test[i][j] = np.argmax(prob > np.random.random())\n",
    "                \n",
    "                #increment associated counters\n",
    "                self.topic_doc_count_test[i][self.topic_assign_test[i][j]] += 1\n",
    "                self.term_topic_count_test[self.topic_assign_test[i][j]] += queries[i][j]\n",
    "                self.term_topic_sum_test[self.topic_assign_test[i][j]] += 1\n",
    "                \n",
    "    def _estimate_parameters_train(self):\n",
    "        \"\"\"Estimate parameters used to make predictions\"\"\"\n",
    "        \n",
    "        self.beta = np.divide((self.term_topic_count + self.eta).T, \n",
    "                             np.sum((self.term_topic_count + self.eta), axis=1)).T\n",
    "        \n",
    "        self.theta = np.divide((self.topic_doc_count + self.alpha).T, \n",
    "                             np.sum((self.topic_doc_count + self.alpha), axis=1)).T\n",
    "        \n",
    "        self.components_ = self.beta\n",
    "    \n",
    "    def _estimate_parameters_test(self):\n",
    "        \n",
    "                                         \n",
    "        self.theta_test = np.divide((self.topic_doc_count_test + self.alpha).T,\n",
    "                                   np.sum((self.topic_doc_count_test + self.alpha), axis=1)).T\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, bound = -17.0677205003\n",
      "Iter 10, bound = -2.73484132991\n",
      "Iter 20, bound = -2.68272297592\n",
      "Iter 30, bound = -2.60498487916\n",
      "Iter 40, bound = -2.54304748958\n",
      "Iter 50, bound = -2.52028907819\n",
      "[[  1.16909951e-02   5.73124740e-05   1.52979499e-02   1.81426330e-02\n",
      "    2.92443035e-02   2.79711349e-02   3.60553959e-02   4.54574605e-02\n",
      "    1.91830209e-01   1.95422191e-01   2.13527118e-01   2.15303298e-01]\n",
      " [  1.85256175e-01   2.08647020e-01   2.10219764e-01   2.27440249e-01\n",
      "    2.61659358e-03   1.01107956e-02   1.56850425e-02   1.87054548e-02\n",
      "    2.01845677e-02   4.16485516e-02   2.37349949e-02   3.57507912e-02]\n",
      " [  5.07096290e-02   5.39302978e-02   2.80182236e-02   2.42073425e-02\n",
      "    2.10828664e-01   2.17548102e-01   1.86850044e-01   1.71769691e-01\n",
      "    2.90199990e-02   2.23790843e-02   4.72491206e-03   1.40110218e-05]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Simple test on toy documents \"\"\"\n",
    "model = LDA()\n",
    "beta, _, _ = model.fit(W)\n",
    "\n",
    "# We more or less retrieve the beta used in the generating process\n",
    "print beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Get the 20 news groups data \"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "cats = ['soc.religion.christian', 'sci.space', 'talk.politics.mideast',\n",
    "        'comp.sys.ibm.pc.hardware','rec.sport.baseball']\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"train\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for sklearn (counts) \"\"\"\n",
    "n_features = 2000\n",
    "vectorizer = CountVectorizer(max_features=n_features, stop_words=\"english\")\n",
    "\n",
    "# Word counts per document matrix (input for sklearn)\n",
    "W_counts = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Keep track of vocabulary to visualize top words of each topic\n",
    "vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for our model (one hot vectors) \"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "\n",
    "# 1) Tokenize document\n",
    "# 2) Remove stop words\n",
    "# 3) Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer()\n",
    "# TODO : treat special list better\n",
    "# TODO See what count vectorizer does\n",
    "special = [\"''\", \"'s\", \"``\", \"n't\", \"...\", \"--\"]\n",
    "stop = set(stopwords.words('english') + \\\n",
    "           list(string.punctuation) + special)\n",
    "def prepare_document(doc):\n",
    "    words = [lemmatizer.lemmatize(w) for w in word_tokenize(doc.lower()) \n",
    "             if w not in stop] \n",
    "    return words\n",
    "\n",
    "\n",
    "# List of documents (each document is a list of word tokens)\n",
    "texts = [prepare_document(text) for text in newsgroups.data]\n",
    "\n",
    "# Create a gensim dictionary for our corpus\n",
    "dic = Dictionary(texts)\n",
    "\n",
    "# Keep only k most frequent words\n",
    "n_features = 2000\n",
    "dic.filter_extremes(keep_n=n_features)\n",
    "vocab_size = len(dic.token2id) # Vocabulary size\n",
    "\n",
    "# List of documents (each document is now a list of word indexes)\n",
    "# We have removed all words not in the k most frequent words\n",
    "texts_idx = [[dic.token2id[word] for word in text \n",
    "              if word in dic.token2id] for text in texts]\n",
    "\n",
    "# Convert each index to a one hot vector\n",
    "W = np.array([np.eye(vocab_size)[text] for text in texts_idx if len(text) > 0])\n",
    "\n",
    "# Keep track of id to word mapping to visualize top words of topics\n",
    "id2word = dict([[v, k] for k, v in dic.token2id.items()])\n",
    "\n",
    "\n",
    "\n",
    "#pickle.dump(W, open(\"W.p\", \"wb\"))\n",
    "#pickle.dump(id2word, open(\"mapping.p\", \"wb\"))\n",
    "\n",
    "pickle.dump(W, open(\"/media/patrice/multimedia_2/DATA/W.p\", \"wb\"))\n",
    "pickle.dump(id2word, open(\"/media/patrice/multimedia_2/DATA/mapping.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "earth spacecraft lebanese orbit moon probe mars time surface sun lunar mission\n",
      "Topic 2:\n",
      "church spirit father faith catholic holy son book authority christian jesus st\n",
      "Topic 3:\n",
      "said people didn know don went just say came like going time\n",
      "Topic 4:\n",
      "nasa space available 14 data information 12 ftp 10 11 gov faq\n",
      "Topic 5:\n",
      "space station cost rom systems theory universe larson technology feature high supports\n",
      "Topic 6:\n",
      "use just like work software does tape need know sys adaptec don\n",
      "Topic 7:\n",
      "drive drives disk hard bios controller slave master floppy jumper 16 pin\n",
      "Topic 8:\n",
      "scsi ide bus controller isa drive transfer pc sec data dma mac\n",
      "Topic 9:\n",
      "israel jews israeli armenian people war armenians arab state peace arabs genocide\n",
      "Topic 10:\n",
      "card bit thanks dos problem windows pc monitor does using know use\n",
      "Topic 11:\n",
      "edu com os adam mail comp cs ac ca os2 easter net\n",
      "Topic 12:\n",
      "turkish armenian turkey greek armenians people armenia turks said azerbaijan government greece\n",
      "Topic 13:\n",
      "year good think game better hit don players games baseball season time\n",
      "Topic 14:\n",
      "00 02 03 01 lost 04 won cubs new york 05 sox\n",
      "Topic 15:\n",
      "know don like think just people believe truth does things true say\n",
      "Topic 16:\n",
      "university space sci list professor history news posts article post email schism\n",
      "Topic 17:\n",
      "god jesus people christ think time does bible say believe life christians\n",
      "Topic 18:\n",
      "right team human rights runs jewish israeli does way just game darren\n",
      "Topic 19:\n",
      "space launch satellite lunar nasa shuttle new flight year commercial dc data\n",
      "Topic 20:\n",
      "jewish information anti said adl jews people german san group police years\n",
      "\n",
      "Our model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in exp\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:51: RuntimeWarning: underflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, bound = -103.466210523\n",
      "Iter 10, bound = nan\n",
      "Iter 20, bound = nan\n",
      "Iter 30, bound = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:1438: RuntimeWarning: underflow encountered in true_divide\n",
      "  X /= norms[:, np.newaxis]\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:69: RuntimeWarning: underflow encountered in multiply\n",
      "/home/patrice/.local/lib/python2.7/site-packages/scipy/stats/_discrete_distns.py:52: RuntimeWarning: underflow encountered in exp\n",
      "  return exp(self._logpmf(x, n, p))\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:69: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40, bound = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:67: RuntimeWarning: underflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50, bound = nan\n",
      "Topic 1:\n",
      "space launch satellite mission program year nasa first would shuttle station u\n",
      "Topic 2:\n",
      "drive would scsi israel time israeli one get ide question could arab\n",
      "Topic 3:\n",
      "say said one know go people going thing time get well like\n",
      "Topic 4:\n",
      "armenian people armenia russian azeri turkish azerbaijan genocide turk said one massacre\n",
      "Topic 5:\n",
      "space system nasa satellite orbit earth lunar center available also 1 data\n",
      "Topic 6:\n",
      "god jesus one christ church sin father would u christian say son\n",
      "Topic 7:\n",
      "game year team would run good one last player hit win first\n",
      "Topic 8:\n",
      "jew one time many greek even woman child jewish would year like\n",
      "Topic 9:\n",
      "turkish jew armenian greek turkey university government history turk jewish people nazi\n",
      "Topic 10:\n",
      "driver data drive card also disk program system file problem use window\n",
      "Topic 11:\n",
      "church would people one pope year like think know catholic much time\n",
      "Topic 12:\n",
      "1 0 2 3 4 5 lost 6 game team please n\n",
      "Topic 13:\n",
      "god would christian people one life question believe say u israel also\n",
      "Topic 14:\n",
      "would think year one good like well thing know 'm see time\n",
      "Topic 15:\n",
      "one people u said know armenian could say would went came go\n",
      "Topic 16:\n",
      "israel israeli arab would state palestinian people right one lebanese civilian attack\n",
      "Topic 17:\n",
      "would know like get tape use much one could thing 'm also\n",
      "Topic 18:\n",
      "would one like people get problem want know think time also work\n",
      "Topic 19:\n",
      "would god law say one know think people christian truth like word\n",
      "Topic 20:\n",
      "drive card system 2 disk controller 1 bios hard scsi one jumper\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 20\n",
    "\n",
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "\n",
    "# Compare with sklearn implementation\n",
    "print \"Sklearn model\"\n",
    "lda_sklearn = LatentDirichletAllocation(n_topics=n_topics, \n",
    "                                        learning_method=\"batch\")\n",
    "lda_sklearn.fit(W_counts)\n",
    "visualizeTopics(lda_sklearn, vocabulary)\n",
    "\n",
    "# Our model\n",
    "print \"\\nOur model\"\n",
    "lda = LDA(K=n_topics)\n",
    "\n",
    "#W = pickle.load(open(\"W.p\", \"rb\"))\n",
    "#id2word = pickle.load(open(\"mapping.p\", \"rb\"))\n",
    "W = pickle.load(open(\"/media/patrice/multimedia_2/DATA/W.p\", \"rb\"))\n",
    "id2word = pickle.load(open(\"/media/patrice/multimedia_2/DATA/mapping.p\", \"rb\"))\n",
    "\n",
    "lda.fit(W, verbose=True)\n",
    "visualizeTopics(lda, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "Topic 1:\n",
      "would israel know question people like want get think answer see life\n",
      "Topic 2:\n",
      "1 drive 2 0 card 3 4 5 controller system driver hard\n",
      "Topic 3:\n",
      "year game would team get good think run last one player like\n",
      "Topic 4:\n",
      "one book greek christian also church world religion question many may read\n",
      "Topic 5:\n",
      "jew right israeli israel jewish people state would arab human nazi country\n",
      "Topic 6:\n",
      "space launch nasa satellite program orbit system data shuttle lunar rocket center\n",
      "Topic 7:\n",
      "drive scsi problem get ide use know one would system like 'm\n",
      "Topic 8:\n",
      "one said u people say know could would woman went go came\n",
      "Topic 9:\n",
      "god jesus believe say think christian christ one church would people sin\n",
      "Topic 10:\n",
      "would one thing mission like way much could moon work get new\n",
      "Topic 11:\n",
      "university number history group april also first power study political army book\n",
      "Topic 12:\n",
      "armenian turkish turkey people turk armenia russian government village genocide killed new\n"
     ]
    }
   ],
   "source": [
    "#results for the LDA implementation using Gibbs sampling\n",
    "n_topics = 12\n",
    "lda_gibbs = LDA_gibbs(n_topics)\n",
    "lda_gibbs.fit(W, niter=20)\n",
    "        \n",
    "visualizeTopics(lda_gibbs, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading and preprocessing test data\n",
    "newsgroups_test = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"test\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"), categories=cats)\n",
    "\n",
    "# List of documents (each document is a list of word tokens)\n",
    "texts_test = [prepare_document(text) for text in newsgroups_test.data]\n",
    "\n",
    "# List of documents (each document is now a list of word indexes)\n",
    "# We have removed all words not in the k most frequent words\n",
    "texts_idx_test = [[dic.token2id[word] for word in text \n",
    "              if word in dic.token2id] for text in texts_test]\n",
    "\n",
    "# Convert each index to a one hot vector\n",
    "W_test = np.array([np.eye(vocab_size)[text] for text in texts_idx_test if len(text) > 0])\n",
    "\n",
    "# Keep track of id to word mapping to visualize top words of topics\n",
    "id2word = dict([[v, k] for k, v in dic.token2id.items()])\n",
    "\n",
    "\n",
    "pickle.dump(W_test, open(\"/media/patrice/multimedia_2/DATA/W_test.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Visualize most likely topic assignment for each word \n",
    "    [if p(most likely topic) < threshold, don't assign a topic]\n",
    "    and visualize expected distribution over topics\n",
    "    Inputs:\n",
    "        model, our trained lda model\n",
    "        document_idx, index of document to visualize\n",
    "        id2word, mapping from word index to word string\n",
    "        topic2word, mapping from topic index to topic string\n",
    "        threshold, float in [0.0, 1.0]\n",
    "    \"\"\"\n",
    "def visualizeWordAssignments(model, document_idx, id2word, \n",
    "                             topic2word, threshold=0.9):\n",
    "    # We add a column with value threshold so that if the index of this \n",
    "    # column gets selected by the argmax operation, we don't assign a topic\n",
    "    col = np.ones((model.phi[document_idx].shape[0], 1)) * threshold\n",
    "    col_index = model.phi[document_idx].shape[1]\n",
    "    topic2word[col_index] = \" \"\n",
    "    word_topic_probas = np.concatenate(\n",
    "        (model.phi[document_idx], col), axis=1)\n",
    "    \n",
    "    # Expected distribution over topics\n",
    "    doc_topic_probas = dirichlet_distribution.mean(\n",
    "        model.gamma[document_idx])\n",
    "    \n",
    "    print \"DOCUMENT:\"\n",
    "    print newsgroups.data[document_idx]\n",
    "    \n",
    "    print \"\\nEXPECTED PROBA DISTRIB OVER TOPICS:\"\n",
    "    for i in xrange(len(doc_topic_probas)):\n",
    "        print \"{}: {}\".format(topic2word[i], doc_topic_probas[i])\n",
    "        \n",
    "    print \"\\nWORD TOPIC ASSIGNMENTS:\"\n",
    "    for word_idx, topic_idx in zip(texts_idx[document_idx], \n",
    "                                   np.argmax(word_topic_probas, axis=1)): \n",
    "        print \"{} -> {}\".format(id2word[word_idx], topic2word[topic_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT:\n",
      "\n",
      "In the \"sex ed\" portion of the  high school \"health\" course I took\n",
      "in 1984, it was impressed that the only 100% positive way to *not*\n",
      "get pregnant was to *not* have sex.\n",
      "\n",
      "Other methods of contraception were discussed, in the framework of\n",
      "a chart which showed both the _expected_ failure rate (theoretical,\n",
      "assumes no mistakes) and the _actual_ failure rate (based on research).\n",
      "Top of the chart was something like this:\n",
      "\n",
      "\n",
      " Method                  Expected         Actual \n",
      " ------                 Failure Rate    Failure Rate\n",
      " Abstinence                 0%              0% \n",
      "\n",
      "\n",
      "And NFP (Natural Family Planning) was on the bottom. The teacher even\n",
      "said, \"I've had some students tell me that they can't use anything for\n",
      "birth control because they're Catholic. Well, if you're not married and\n",
      "you're a practicing Catholic, the *top* of the list is your slot, not \n",
      "the *bottom*.  Even if you're not religious, the top of the list is\n",
      "safest.\"\n",
      "\n",
      "Yes, this was a public school and after Dr Koop's \"failing abstinence,\n",
      "use a condom\" statement on the prevention of AIDS.\n",
      "\n",
      "-jen\n",
      "\n",
      "-- \n",
      "\n",
      "EXPECTED PROBA DISTRIB OVER TOPICS:\n",
      "topic 1: 0.000746268673377\n",
      "topic 2: 0.000746268673246\n",
      "topic 3: 0.000746268676027\n",
      "topic 4: 0.000746268672256\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c24b08b13d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtopic2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"topic 1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"topic 2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"topic 3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"topic 4\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualizeWordAssignments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-868531a00b95>\u001b[0m in \u001b[0;36mvisualizeWordAssignments\u001b[0;34m(model, document_idx, id2word, topic2word, threshold)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\nEXPECTED PROBA DISTRIB OVER TOPICS:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_topic_probas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"{}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_probas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\nWORD TOPIC ASSIGNMENTS:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "topic2word = {0: \"topic 1\", 1: \"topic 2\", 2: \"topic 3\", 3: \"topic 4\"}\n",
    "visualizeWordAssignments(lda, 0, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic2word = {0: \"space\", 1: \"religion\"}\n",
    "visualizeWordAssignments(lda, 6, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Perform inference on new documents\n",
    "# only E-step (estimate phi and gamma) until the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = pickle.load(open(\"/media/patrice/multimedia_2/DATA/W.p\", \"rb\"))\n",
    "id2word = pickle.load(open(\"/media/patrice/multimedia_2/DATA/mapping.p\", \"rb\"))\n",
    "\n",
    "#W = pickle.load(open(\"W_train.p\", \"rb\"))\n",
    "W_test = pickle.load(open(\"/media/patrice/multimedia_2/DATA/W_test.p\", \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of topics : 2\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 4\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 6\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 8\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 10\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 12\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 14\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 16\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 18\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 20\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 22\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 24\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 26\n",
      "Training model\n",
      "Testing model\n",
      "\n",
      "number of topics : 28\n",
      "Training model\n",
      "Testing model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAGz9JREFUeJzt3X+QXWWd5/H3B5no6AJBGKASJOkNIpEdQFwgViy5mRkgkCqiFoNgMohajitmwgpFEYbNpndjSsEiq0ihO06WCUMgAs4KCAOEMncoUsMPJ4QfQ0Ji7IakM/RYQhPULRLCd/84Tyc3l+5097339P1xPq+qW33uc899znO44XzPeX4qIjAzs+I5qNkFMDOz5nAAMDMrKAcAM7OCcgAwMysoBwAzs4JyADAzK6gRA4CkFZL6JT1XkXaDpI2SNkj6iaRDKz67VtKW9Pk5FemzJW2StFnSNY0/FTMzG4vRPAHcCpxblfYIcFJEnApsAa4FkPRR4CJgOnAecIsyBwE3p3xOAi6RdGJjTsHMzGoxYgCIiMeB16vSHo2Id9LbJ4Bj0/YFwOqIeDsiesmCwxnptSUiXo6I3cBqYG5jTsHMzGrRiDaALwEPpu3JwLaKz/pSWnX69pRmZmZNUlcAkHQdsDsi7mxQeczMbJwcXOsXJV0GnA/8SUVyH/ChivfHpjQBxw2RPlzenqDIzGyMIkJj2X+0TwBKr+yNNBu4GrggIt6q2O8+4GJJEyR1AccDTwFPA8dLmiJpAnBx2ndYEdGRryVLljS9DD4/n5/Pr/NetRjxCUDSHUAJOELSK8AS4K+BCcAaSQBPRMTlEfGipLuAF4HdwOWRlWyPpAVkvYcOAlZExMaaSmxmZg0xYgCIiM8PkXzrAfb/FvCtIdIfAj4yptKZmVluPBJ4nJVKpWYXIVc+v/bm8ysW1Vp3lCdJ0YrlMjNrVZKInBqBzcyswzgAmJkVlAOAmVlBOQCYmRWUA4CZWUE5AJiZFZQDgJlZQTkAmJkVlAOAmVlBOQCYmRWUA4CZWUE5AJiZFZQDgJlZQRUiAPT09jB/4XxmXTaL+Qvn09Pb0+wimZk13YgBQNIKSf2SnqtIu1DSC5L2SDqtIn2KpN9LWp9et1R8dpqk5yRtlvTdxp/K0Hp6ezh7wdmsOmQV5a4yqw5ZxdkLznYQMLPCG80TwK3AuVVpzwOfAf5piP1/GRGnpdflFek/AL4cEScAJ0iqzjMXi5cvZuspW7MFLAEmwNZTtrJ4+eLxOLyZWcsaMQBExOPA61VpL0XEFioWiq/wrjRJxwCHRMTTKek24NNjL+7Y9e3s23fxHzQBduzcMR6HNzNrWXm0AUyV9C+S1kr6ZEqbDGyv2Gd7Ssvd5EMnw66qxF0w6dBJ43F4M7OWNeKi8GO0AzguIl5PbQM/lfTRWjLq7u7eu10qlWpey3PplUt5YsET+6qBdsG0Z6ex9OalNeVnZtYKyuUy5XK5rjxGtSawpCnA/RFxclX6WuCqiFg/zPfWAleRBYa1ETE9pV8MnBURXxvmew1dE7int4fFyxezY+cOJh06iaVXLqVralfD8jcza7Za1gQe7ROAGLq+n8p0SUcCr0XEO5L+I3A88KuIGJD0hqQzgKeBS4GbxlLQenRN7eL2m24fr8OZmbWFEQOApDuAEnCEpFeAJWSNwt8HjgR+JmlDRJwHfAr4n5J2Ae8AX42IgZTV14G/A94HPBgRDzX4XMzMbAxGVQU03hpdBWRm1ulqqQIqxEhgMzN7NwcAM7OCcgAwMysoBwAzs4JyADAzKygHADOzgnIAMDMrKAcAM7OC6vgA8MADMDCwf9rAQJZuZlZkHR8AZs6E667bFwQGBrL3M2c2t1xmZs1WiKkgBi/6V18N3/kOLFsGEyc2LHszs6arZSqIQgQAgN5e6OqCnh6YOrWhWZuZNZ3nAhrGwEB259/Tk/2tbhMwMyuijg8Ag9U/y5Zld/7Llu3fJmBmVlQdXwX0wANZg29lnf/AAKxbB3PmNOQQZmZN5zYAM7OCyqUNQNIKSf2SnqtIu1DSC5L2pMXfK/e/VtIWSRslnVORPlvSJkmbJV0zlkKamVnjjaYN4Fbg3Kq054HPAP9UmShpOnARMB04D7hFmYOAm1M+JwGXSDqxzrKbmVkdRlwTOCIelzSlKu0lAEnVjxtzgdUR8TbQK2kLcAbZwvFbIuLl9L3Vad9N9Z+CmZnVotG9gCYD2yre96W06vTtKc3MzJpkxCeAZunu7t67XSqVKJVKTSuLmVmrKZfLlMvluvIYVS+gVAV0f0ScXJW+FrgqItan94uAiIjr0/uHgCVkVUDdETF7qP2GOJ57AZmZjUGeI4GVXsN9Nug+4GJJEyR1AccDTwFPA8dLmiJpAnBx2tfMzJpkxCogSXcAJeAISa+Q3dG/DnwfOBL4maQNEXFeRLwo6S7gRWA3cHm6ld8jaQHwCFnQWRERG3M5IzMzGxUPBDMz6wCeDM7MzEbNAcDMrKAcAMzMCsoBwMysoBwAzMwKygHAzKygHADMzArKAcDMrKAcAMzMCsoBwMysoBwAzMwKygHAzKygHADMzArKAcDMrKAcAMzMCsoBwMysoEYMAJJWSOqX9FxF2uGSHpH0kqSHJR2W0s+SNCBpfXr9t4rvzJa0SdJmSdfkczpmZjZao3kCuBU4typtEfBoRHwE+DlwbcVnj0XEaen1TQBJBwE3p3xOAi6RdGLdpTczs5qNGAAi4nGyNYArzQVWpu2VwKcrPhtqSbIzgC0R8XJE7AZWpzzMzKxJam0DOCoi+gEi4lXgqIrPZkh6RtIDkj6a0iYD2yr22Z7SzMysSQ5uUD6DK7ivB6ZExO8lnQf8FDihlgy7u7v3bpdKJUqlUp1FNDPrHOVymXK5XFceioiRd5KmAPdHxMnp/UagFBH9ko4B1kbE9CG+1wN8nCwIdEfE7JS+CIiIuH6Y48VoymVmZhlJRMRQVfDDGm0VkNi/bv8+4LK0/QXg3lSAoysKcwZZgHkNeBo4XtIUSROAi1MeZmbWJCNWAUm6AygBR0h6BVgCfBu4W9KXgJeBi9LuF0r6GrAb+H/A5wAiYo+kBcAjZEFnRURsbPC5mJnZGIyqCmi8uQrIzGxs8qwCMjOzDuMAYGZWUA4AZmYF5QBgZlZQDgBmZgXlAGBmVlAOAGZmBeUAYGZWUA4AZmYF5QBgZlZQDgBmZgXlAGBmVlAOAGZmBeUAUIee3h7mL5zPrMtmMX/hfHp6e5pdJDOzUfN00DXq6e3h7AVns/WUrTAB2AXTnp3GmpvX0DW1q9nFM7OCyW06aEkrJPVLeq4i7XBJj0h6SdLDkg6r+OwmSVskbZB0akX6FyRtTt+5dCwFbTWLly/ed/EHmABbT9nK4uWLm1ouM7PRGm0V0K3AuVVpi4BHI+IjwM+BawHSYvDTIuLDwFeBH6b0w4H/DpwOnAksqQwa7aZvZ9++i/+gCbBj546mlMfMbKxGFQAi4nHg9arkucDKtL0yvR9Mvy1970ngsLRW8LnAIxHxRkQMkC0PObu+4jfP5EMnw66qxF0w6dBJTSmPmdlY1dMIfFRE9ANExKvA4ILwk4FtFfttT2nV6X0prS0tvXIp056dti8IpDaApVcubWq5zMxGq5G9gIZrtR1To0S76JraxZqb1zDvzXnM6pnFvDfnuQHYzNrKwXV8t1/S0RHRL+kY4N9Teh/woYr9jk1pfUCpKn3tcJl3d3fv3S6VSpRKpeF2bZquqV3cftPtzS6GmRVQuVymXC7Xlceou4FKmgrcHxF/nN5fD7wWEddLWgRMjIhFks4Hvh4RcyTNAL4bETNSI/AvgNPInjx+AXw8tQdUH6vlu4GambWSWrqBjuoJQNIdZHfvR0h6BVgCfBu4W9KXgJeBiwAi4kFJ50v6JfA74Isp/XVJS8ku/AH8j6Eu/mZmNj48EMzMrAPkNhDMzMw6jwOAmVlBOQCYmRWUA0CL8kyjZpY3NwK3IM80amZj5UbgDuGZRs1sPDgAtCDPNGpm48EBoAV5plEzGw9uA2hBbgMws7GqpQ3AAaBF9fT2sHj5Ynbs3MGkQyex9Mqlvvib2bAcAMzMCsq9gMzMbNQcAArGA8zMbJCrgArEjctmnctVQHZAHmBmZpUcAArEA8zMrFJdAUDSFZKeT6+FKW2JpO2S1qfX7Ir9r5W0RdJGSefUW3gbGw8wM7NKNbcBSDoJuBM4HXgb+Efga8B84M2IWF61/3TgjrT/scCjwIeHqux3G0A+3AZg1rnGuw1gOvBkRLwVEXuAx4DPDpZliP3nAqsj4u2I6AW2AGfUcXwbo66pXay5eQ3z3pzHrJ5ZzHtzni/+ZgVWzxPAicBPgU8Ab5Hd0T8N/Aa4DNhJtgD8VRHxhqTvA/8cEXek7/8t8GBE/MMQefsJwMxsDMb1CSAiNgHXA2uAB4FngD3AD4BpEXEq8CpwY63HMDOz/Bxcz5cj4lbgVgBJy4BtEfHril1+BNyftvuAD1V8dmxKG1J3d/fe7VKpRKlUqqeo1qYG50Tq29nH5EMne04ks6RcLlMul+vKo66BYJL+KCJ+Lek44CFgBvD+iHg1ff4N4PSI+LykjwKrgDOByWRPDm4EtmG50dps9MZ9MjhJjwEfBHYD34iIsqTbgFOBd4Be4KsR0Z/2vxb4ctr/ioh4ZJh8HQCM+Qvns+qQVfuPXdgF896cx+033d60cpm1oloCQL1VQJ8aIu3SA+z/LeBb9RyzVTzwAMycCRMn7ksbGIB162DOnOaVq1nyqKrp29kHR1QleuCaWcN4JHCNZs6E667LLvqQ/b3uuiy9aAaralYdsopyV5lVh6zi7AVn1z3RnAeumeXLk8HVYfCif/XV8J3vwLJl+z8RFEVeVTVuAzAbvXGvAiq6iROzi39XF/T0FPPiD/lV1QwOXNtvZbSb3QvIrFEcAOowMJDd+ff0FPsJYG9VTdUTQCOqarqmdrnB1ywnrgKq0WD1z+BFv/p9kbiqxqz5vCbwOHIvoP15EXuz5nIAMDMrKK8IZmZmo+YAYGZWUA4AZmYF5QBg1mA9vT3MXzifWZfNYv7C+XWPiDbLixuBzRrIXWKtWdwIbDYGedypL16+eN/FH2ACbD1lK4uXL647b7NG80hgK6T97tSPAHbBEwueqPtO3TOYWjvxE4AVUl536p7B1NqJA4AVUt/Ovv3nLoKG3KkvvXIp056dti8IpDaApVcurStfszzUFQAkXSHp+fRamNIOl/SIpJckPSzpsIr9b5K0RdIGSafWW3izWuV1pz44g+m8N+cxq2cW896c5wZga1k19wKSdBJwJ3A68Dbwj8DXgL8EfhMRN0i6Bjg8IhZJOg9YEBFzJJ0JfC8iZgyTt3sBWa7cW8c6zXj3ApoOPBkRb0XEHuAx4LPABcDKtM9KYG7angvcBhARTwKHSTq6juOb1cx36mb19QJ6AfimpMOBt4DzgV8ARw8uAh8Rr1Zc5CcD2yq+35fS+usog1nN2m2tgTzWXbZiqzkARMQmSdcDa4DfAs8Ae4batZb8u7u7926XSiVKpVIt2bQlTzVt1fLqtpo3B638lMtlyuVyXXk0bCSwpGVkd/hXAKWI6Jd0DLA2IqZL+mHa/nHafxNw1uDTQlVehW4D8GIzVi2vdZfz5HaW8TXuI4El/VH6exzwGeAO4D7gsrTLZcC9afs+4NK0/wxgYKiLv2UX+WXLsot+b68v/pZft9U8eVR066t3JPBPJH0Q2A1cHhE7U7XQXZK+BLwMXAQQEQ9KOl/SL4HfAV+s89gdLY8F51211L7yXHc5Lx4V3frqegKIiE9FxH+KiI9FRDmlvRYRfxYRH4mIcyJioGL/BRFxfEScEhHr6yx7R6tecH5gYOTvjGTmzOxpYjCvwaqlmTPrz9vy1Y4DzDwquvV5NtAWlGcbwGBeV1+dBRZXLbWPPNddzqOx1m0A48trAneIvKtqenv3VS1NnVp/ftbe8rxQ5xm0bH8OADYiPwFYtXbsYWTv5vUA7IAqq5KmTt3X06gR7QvWvtqxh1Ge8lrRrRVXivN6AAWybt3+d/yD3U3dC6jY2rGHUV7yGnDXqgP5XAVkVnDt2Fib1wjjvKrDxqOarZYqID8BmBXc4MR4+zXW3ty6jbV53k3nNXahVcdEOACYWVtNjHegEcb1nkNe1WGtWs3mRmAzayt5NlrnNeCuVQfyOQCYWVvJc4RxXutEtOr6E24ENrO20o6N1uPBA8HMrBA8wvjdHACsKTzLqFnzeSSwNYVnGTVrTw4AVre8FrB54IF3T1MxMJClm1n9XAVkDdPoWUa9NKbZ6DVjSchvSHpB0nOSVkl6r6RbJf1K0jOS1ks6uWL/myRtkbRB0qn1HNtaSx4L2HhpTLN81fwEIGkS8DhwYkTskvRj4EHgLOBnEfEPVfufByyIiDmSzgS+FxEzhsnbTwBtJO87da9fYDayZjQCvwf4gKSDgfcDfYDSq9pc4DaAiHgSOEzS0XUe31rAgWYZrVceTxZmlqk5AETEDuBG4BWyC/9ARDyaPv5mqua5UdIfpLTJwLaKLPpSmrW5OXPefac/cWL9XUC9foFZvmqeDE7SRLK7+inAG8A9kj4PLIqI/nTh/xFwDfDNsebf3d29d7tUKlEqlWotqrUpr19gNrxyuUy5XK4rj3raAC4Ezo2Ir6T3fwGcGRELKvY5C7gqIi6Q9ENgbUT8OH22CTgrIvqHyNttAGZmYzDebQCvADMkvU+SgD8FNko6JhVGwKeBF9L+9wGXps9mkFUZvevib2Zm46PmKqCIeErSPcAzwG5gPfA3wEOSjiRrCN4A/Je0/4OSzpf0S+B3wBfrLbyZmdXOA8HMzDqA5wIyG6W8ppnw9BXWThwArJDymsDOE+NZO3EVkBXW4MX56quzQWaNGrmcV75mB+L1AMzGKK9pJjx9hY03twGYjUFe00x4+gprFw4AVkh5TTPh6SusnbgKyAopr2UsvTymNYvbAMzMCsptAGZmNmoOAGZmBeUAYGZWUA4AZmYF5QBg1gY8x5DlwQHArA14jiHLg7uBmrUJzzFkB+JxAGYdznMM2XDGfRyApG9IekHSc5JWSZogaaqkJyRtlnSnpIPTvhMkrZa0RdI/SzqunmObFY3nGLJGqzkASJoE/BVwWkScTLa85CXA9cCNEXECMAB8OX3ly8BrEfFh4LvADfUU3KxIPMeQ5aHeRuD3AB9Id/l/COwAZgE/SZ+vJFsYHmBueg9wD9ki8mY2CuvW7V/nP3Fi9n7duuaW60Dcc6n11RwAImIHcCPwCtAHvEG2MPxARLyTdtsOTE7bk4Ft6bt7gAFJH6z1+GZFMmfOuxt8J05szARzeV2o3XOp9R1c6xclTSS7q59CdvG/G5g9liwO9GF3d/fe7VKpRKlUGnMZzWxkgxfqwSeMyuqmegw+pbjnUj7K5TLlcrmuPGruBSTpQuDciPhKev8XwCeAC4FjIuIdSTOAJRFxnqSH0vaTkt4D/FtEHDVM3u4FZDaO8uxi6p5L42O8ewG9AsyQ9D5JIqvT/1dgLfDnaZ8vAPem7fvSe9LnP6/j2GbWQBMnZhf/rq7sb6Mu/u651NrqaQN4iqwx9xngWbIqnb8BFgFXStoMfBBYkb6yAjhS0hbgv6b9zKwF5HGhds+l1ueBYGYFV3mhrm4DqOdJwKujjS+PBDazMfOFujM4AJiZFZSXhDQzq0PRBq85AJiZJUUbvOYqIDOzCu067bbbAMzMGqAdB6+5DcDMOl7e9fR5jIlo1bYFBwAzayt51tPnNXitVdsWXAVkZm0nr3r6PMdE5N224DYAMyuMdqynz7PMbgMws0Jox0nmWrHMDgBm1lbacZK5Vi2zq4DMrK2049xF41FmtwGYmRWU2wDMzGzUag4Akk6Q9Iyk9envG5IWSloiaXtKXy9pdsV3rpW0RdJGSec05hTMzKwW9awItjkiPhYRpwEfB34H/N/08fKIOC29HgKQNB24CJgOnAfckpaSLJR6F3FudT6/9ubzK5ZGVQH9GbA1Iral90Nd2OcCqyPi7YjoBbYAZzTo+G2j0/8B+vzam8+vWBoVAD4H3Fnx/uuSNkj6W0mHpbTJwLaKffpSmpmZNUHdAUDSHwAXAHenpFuAaRFxKvAqcGO9xzAzs8aruxuopAuAyyNi9hCfTQHuj4iTJS0CIiKuT589BCyJiCeH+J77gJqZjdFYu4Ee3IBjXkJF9Y+kYyLi1fT2s8ALafs+YJWk/0VW9XM88NRQGY71JMzMbOzqCgCS3k/WAPyXFck3SDoVeAfoBb4KEBEvSroLeBHYTfbU4Dt9M7MmacmRwGZmlr+WGgksabakTZI2S7qm2eVpNEm9kp5NA+eGrP5qJ5JWSOqX9FxF2uGSHpH0kqSHK3qBtZ1hzm/YgY7tRNKxkn4u6V8lPS9pYUrviN9viPP7q5TeKb/feyU9ma4lz0taktKnSnoiXUPvlHTAWp6WeQKQdBCwGfhTYAfwNHBxRGxqasEaSNKvgI9HxOvNLksjSPok8Fvgtog4OaVdD/wmIm5IQfzwiFjUzHLWapjzWwK8GRHLm1q4Okk6BjgmIjZI+g/Av5CN1fkiHfD7HeD8PkcH/H6QVcFHxO8lvQdYB1wBXAncExF3S/oBsCEi/vdwebTSE8AZwJaIeDkidgOryX6wTiJa6795XSLicaA6mM0FVqbtlcCnx7VQDTTM+cHQAx3bSkS8GhEb0vZvgY3AsXTI7zfM+Q2OO2r73w8gIn6fNt9L1p4bwCzgJyl9JfCZA+XRShej6oFi2+m8gWIBPCzpaUlfaXZhcnJURPRD9j8hcFSTy5OHoQY6ti1JU4FTgSeAozvt96s4v8Eu5x3x+0k6SNIzZOOt1gBbgYGIeCftsh2YdKA8WikAFMHMiPjPwPlk/wg/2ewCjYPWqGNsnOqBjm1dlZCqR+4Brkh3ytW/V1v/fkOcX8f8fhHxTkR8jOzJ7QzgxLHm0UoBoA84ruL9sSmtY0TEv6W/vyabOK8T50Lql3Q07K2H/fcml6ehIuLXFd2XfwSc3szy1CM1EN4D/H1E3JuSO+b3G+r8Oun3GxQRO4Ey8AlgYmpPhVFcQ1spADwNHC9piqQJwMVkg8c6gqT3p7sRJH0AOId9g+Tamdi/TvU+4LK0/QXg3uovtJn9zi9dFAdVDnRsR/8HeDEivleR1km/37vOr1N+P0lHDlZfSfpD4GyyMVZrgT9Pu434+7VMLyDIuoEC3yMLTCsi4ttNLlLDSOoiu+sPsgabVe1+fpLuAErAEUA/sAT4Kdm8UB8CXgYuiogWXq11eMOc3yyy+uS9Ax0H68zbiaSZwGPA82T/JgP4a7LR+XfR5r/fAc7v83TG7/fHZI28B6XXjyNiWbrOrAYOB54B5qdONUPn00oBwMzMxk8rVQGZmdk4cgAwMysoBwAzs4JyADAzKygHADOzgnIAMDMrKAcAM7OCcgAwMyuo/w/aSMcO6hqLxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd95830790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perp_train = []\n",
    "perp_test = []\n",
    "\n",
    "topic_numbers = range(2,30,2)\n",
    "\n",
    "\n",
    "for n_topics in topic_numbers:\n",
    "    #results for the LDA implementation using Gibbs sampling\n",
    "    print \"\\nnumber of topics : %d\"%n_topics\n",
    "    print \"Training model\"\n",
    "    lda_gibbs = LDA_gibbs(n_topics)\n",
    "    lda_gibbs.fit(W, niter=10)\n",
    "    perp_train.append(lda_gibbs.perplexity_train(W))\n",
    "\n",
    "    print \"Testing model\"\n",
    "    lda_gibbs.query_sampling(W_test, niter=10)\n",
    "    perp_test.append(lda_gibbs.perplexity_test(W_test))\n",
    "\n",
    "\n",
    "perp_train = np.array(perp_train)\n",
    "perp_test = np.array(perp_test)\n",
    "\n",
    "np.savetxt('perp_train_gibbs.txt', perp_train)\n",
    "np.savetxt('perp_test_gibbs.txt', perp_test)\n",
    "\n",
    "plt.plot(topic_numbers, perp_train,'x')\n",
    "plt.plot(topic_numbers, perp_test, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
