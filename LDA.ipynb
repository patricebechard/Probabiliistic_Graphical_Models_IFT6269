{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "from numpy.random import dirichlet, multinomial\n",
    "import sys\n",
    "np.random.seed(0)\n",
    "np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Generate toy documents \"\"\"\n",
    "K = 3\n",
    "D = 1000\n",
    "N = 10\n",
    "V = 12\n",
    "\n",
    "# 1) Set alpha and beta for generative model\n",
    "alpha = np.array([2, 2, 2])\n",
    "beta = np.array([[0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, 0 ,0, 0.25, 0.25, 0.25, 0.25]])\n",
    "\n",
    "# 2) Using alpha, sample theta_d for each of the D documents\n",
    "theta = dirichlet(alpha, D)\n",
    "\n",
    "# 3) Sample topic z_dn for each of the N words of each of the D documents\n",
    "# 4) Sample word w_dn from topic beta_k corresponding to z_dn\n",
    "W = []\n",
    "for d in range(D):\n",
    "    # Counts of words for each topic in doc d (e.g [4, 3, 3] for N = 10)\n",
    "    z_d = multinomial(N, theta[d])\n",
    "    # One hot matrix of words for doc d\n",
    "    w_d = []\n",
    "    for k in xrange(K):\n",
    "        for i in xrange(z_d[k]):\n",
    "            w_d.append(multinomial(1, beta[k]))\n",
    "    W.append(np.array(w_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma, digamma\n",
    "from scipy.stats import multinomial as multinomial_distribution\n",
    "from scipy.stats import dirichlet as dirichlet_distribution\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "class LDA():\n",
    "    def __init__(self, K=3):\n",
    "        self.K = K\n",
    "        # Initialize alpha\n",
    "        self.alpha = np.ones(self.K) / self.K\n",
    "        \n",
    "    # W is a list of D numpy arrays of shape (N_d, V)\n",
    "    def fit(self, W, max_iter=51, threshold=1e-4, verbose=True):\n",
    "        self.D = len(W)\n",
    "        self.V = W[0].shape[1]\n",
    "        \n",
    "        # Randomly initialize variational parameters\n",
    "        # TODO best way to initialize gamma, phi ?\n",
    "        self.gamma = np.random.rand(self.D, self.K)\n",
    "        self.phi = [np.random.rand(w_d.shape[0], self.K) for w_d in W]\n",
    "        self.phi = [phi_d / np.sum(phi_d, axis=1, keepdims=True) \\\n",
    "            for phi_d in self.phi]\n",
    "        \n",
    "        # Initialize beta\n",
    "        self._updateBeta(W)\n",
    "        \n",
    "        # Keep track of previous value of lower bound on log likelihood\n",
    "        old_bound = self._computeBound(W)\n",
    "        \n",
    "        # Update phi, gamma, beta iteratively while the bound improves\n",
    "        for i in xrange(max_iter):\n",
    "            self._updatePhi(W)\n",
    "            self._updateGamma()\n",
    "            self._updateBeta(W)\n",
    "            current_bound = self._computeBound(W)\n",
    "            if verbose and i % 10 == 0:\n",
    "                print \"Iter {}, bound = {}\".format(i, old_bound)\n",
    "            if (current_bound - old_bound) < threshold:\n",
    "                break\n",
    "            old_bound = current_bound\n",
    "                \n",
    "        # Return final state of beta and variational parameters\n",
    "        self.components_ = self.beta # To match sklearn API\n",
    "        return self.beta, self.gamma, self.phi\n",
    "        \n",
    "    def _updateGamma(self):\n",
    "        for d in xrange(self.D):\n",
    "            self.gamma[d] = self.alpha + np.sum(self.phi[d], axis=0)\n",
    "    \n",
    "    def _updatePhi(self, W):\n",
    "        for d in xrange(self.D):\n",
    "            self.phi[d] = W[d].dot(self.beta.T) * \\\n",
    "                np.exp(digamma(self.gamma[d]))\n",
    "            self.phi[d] = normalize(self.phi[d], norm=\"l1\", axis=1)\n",
    "    \n",
    "    def _updateBeta(self, W):\n",
    "        self.beta = np.concatenate(self.phi, axis=0).T.dot(\n",
    "            np.concatenate(W, axis=0))\n",
    "        self.beta = normalize(self.beta, norm=\"l1\", axis=1)\n",
    "        \n",
    "    def _computeBound(self, W):\n",
    "        # Avoid recomputing expensive expressions\n",
    "        C1 = digamma(self.gamma)\n",
    "        C2 = digamma(np.sum(self.gamma, axis=1))\n",
    "        \n",
    "        # First three terms correspond to expectation of complete\n",
    "        # log likelihood under variational distribution\n",
    "        term1 = self.D * (np.log(gamma(np.sum(self.alpha))) - \\\n",
    "            np.sum(np.log(gamma(self.alpha)))) + \\\n",
    "            np.sum(C1 * (self.alpha - 1)) - np.sum(C2) * \\\n",
    "            np.sum(self.alpha - 1)\n",
    "        term2 = np.sum(np.array([np.sum(phi_d, axis=0) \\\n",
    "            for phi_d in self.phi]) * C1) - \\\n",
    "            np.array([np.sum(phi_d) for phi_d in self.phi]).dot(C2)\n",
    "        term3 = np.sum(np.log(self.beta) * np.concatenate(\n",
    "            self.phi, axis=0).T.dot(np.concatenate(W, axis=0)))\n",
    "        \n",
    "        # Sum of entropies of dirichlet variational posteriors\n",
    "        #term4 = -np.sum(np.log(gamma(np.sum(self.gamma, axis=1)))) + \\\n",
    "        #    np.sum(np.log(gamma(self.gamma))) - \\\n",
    "        #    np.sum((self.gamma - 1) * C1) + \\\n",
    "        #    C2.dot(np.sum(self.gamma - 1, axis=1))\n",
    "        term4 = 0\n",
    "        for d in xrange(self.D):\n",
    "            term4 += dirichlet_distribution.entropy(self.gamma[d])\n",
    "        \n",
    "        # Sum of entropies of multinomial variational posteriors\n",
    "        #term5 = -np.sum(np.concatenate(self.phi, axis=0) * \\\n",
    "        #    np.log(np.concatenate(self.phi, axis=0)))\n",
    "        term5 = np.sum(multinomial_distribution.entropy(\n",
    "            1, np.concatenate(self.phi, axis=0)))\n",
    "        \n",
    "        # Return lower bound on likelihood \n",
    "        # (averaged over total number of words)\n",
    "        L = term1 + term2 + term3 + term4 + term5\n",
    "        return L / np.concatenate(W, axis=0).shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LDA_gibbs:\n",
    "\n",
    "    def __init__(self, n_topics, alpha=None, id2word=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_topics : int\n",
    "            number of topics for the model\n",
    "        alpha : array-like, shape=(n_topics)\n",
    "            hyperparameter for the model\n",
    "            controls the likelihood of a topic being assigned\n",
    "            to a document\n",
    "        id2word : dictionary\n",
    "            used to map id of word to real world\n",
    "            helps to monitor progress of learning\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_topics = n_topics\n",
    "        #initialize alpha\n",
    "        if alpha is None:\n",
    "            # default value (from Heinrich, 2005)\n",
    "            self.alpha = np.ones(self.n_topics) / self.n_topics\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        self.sumalpha = np.sum(self.alpha)\n",
    "        self.id2word = id2word\n",
    "        \n",
    "    def fit(self, docs, eta=None, niter=100):\n",
    "        \"\"\"\n",
    "        Learning the parameters for the model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        docs : list of array-like elements, length = n_docs\n",
    "            each element is an array of dimensions n_words\n",
    "            by number of words in vocabulary (n_vocab). The \n",
    "            number of words varies from one text to the other.\n",
    "        threshold : float\n",
    "            used to stop fitting when the model has converged\n",
    "            to a final state.\n",
    "        eta : array-like, shape=(n_vocab)\n",
    "            hyperparameter for the model\n",
    "            controls the likelihood of a word being assigned \n",
    "            to a topic\n",
    "        niter : int\n",
    "            number of gibbs sampling iterations for the training\n",
    "            of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_docs = len(docs)\n",
    "        self.n_vocab = docs[0].shape[1]\n",
    "        #initialize eta\n",
    "        if eta is None:\n",
    "            #default value (from Heinrich, 2005)\n",
    "            self.eta = np.ones(self.n_vocab) * 0.01\n",
    "        else:\n",
    "            self.eta = eta\n",
    "        self.sumeta = np.sum(self.eta)\n",
    "        \n",
    "        #initializing counters\n",
    "        self._initialize_counters_train(docs)\n",
    "        \n",
    "        for i in xrange(niter):\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                #monitoring progress by showing top words for each topic\n",
    "                print i\n",
    "                self._estimate_parameters_train()\n",
    "                if self.id2word:\n",
    "                    visualizeTopics(self, id2word)\n",
    "\n",
    "            self._gibbs_sampling_fit_train(docs)\n",
    "\n",
    "        #update parameters at the end of training\n",
    "        self._estimate_parameters_train()\n",
    "        \n",
    "    def query_sampling(self, queries, niter=100):\n",
    "        \n",
    "        self.n_queries = len(queries)\n",
    "        \n",
    "        #initializing counters\n",
    "        self._initialize_counters_train(queries)\n",
    "        \n",
    "        for i in xrange(niter):\n",
    "            self._gibbs_sampling_fit_test(queries)\n",
    "        \n",
    "        #update parameters at the end\n",
    "    \n",
    "    def perplexity(self, queries):\n",
    "        \n",
    "        log_likelihood = 0\n",
    "        tot_num_words = 0\n",
    "        for i in range(self.n_queries):\n",
    "            log_likelihood += np.sum(self.topic_doc_count_test + \\\n",
    "                                     np.log(np.sum(self.beta * self.theta[i])))\n",
    "            \n",
    "            tot_num_words += len(queries[i])\n",
    "        \n",
    "        perplexity = np.exp(-log_likelihood / tot_num_words)\n",
    "\n",
    "    \n",
    "    def _initialize_counters_train(self, docs):\n",
    "        \"\"\"Initializing counters randomly for the training phase\"\"\"\n",
    "\n",
    "        #initialize arrays to 0\n",
    "        self.topic_doc_count = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.term_topic_count = np.zeros((self.n_topics, self.n_vocab))\n",
    "\n",
    "        #keep track of topic assignment for each word in memory\n",
    "        self.topic_assign = []\n",
    "\n",
    "        for i in xrange(self.n_docs):\n",
    "\n",
    "            n_words = len(docs[i])\n",
    "\n",
    "            #sample topic randomly for each word in document\n",
    "            topic = np.random.randint(self.n_topics, size=n_words)\n",
    "            self.topic_assign.append(topic)\n",
    "\n",
    "            #count number of words belonging to each topic\n",
    "            self.topic_doc_count[i] = np.bincount(topic, minlength=self.n_topics)\n",
    "\n",
    "            #count word occurence for each topic\n",
    "            self.term_topic_count += np.array([np.sum(docs[i][np.where(topic==j)], axis=0)\\\n",
    "                                               for j in range(self.n_topics)])\n",
    "\n",
    "            #sum of number of words in each topic\n",
    "            self.term_topic_sum = np.sum(self.term_topic_count, axis=1)\n",
    "            \n",
    "    def _initialize_counters_test(self, queries):\n",
    "        \"\"\"Initializing counters randomly for the test phase\"\"\"\n",
    "        \n",
    "        #initialize arrays to 0\n",
    "        self.topic_doc_count_test = np.zeros((self.n_queries,self.n_topics))\n",
    "        self.term_topic_count_test = np.zeros(self.n_topics, self.n_vocab)\n",
    "        \n",
    "        #keep track of topic assignment for each word in memory\n",
    "        self.topic_assign_test = []\n",
    "        \n",
    "        for i in xrange(self.n_queries):\n",
    "            \n",
    "            n_words = len(queries[i])\n",
    "            \n",
    "            #sample topic randomly for each word in query\n",
    "            topic = np.random.randint(self.n_topics, size=n_words)\n",
    "            self.topic_assign_test.append(topic)\n",
    "            \n",
    "            #count number fo words belonging to each topic\n",
    "            self.topic_doc_count_test[i] = np.bincount(topic, minlength=self.n_topics)\n",
    "            \n",
    "            #count word occurence for each topic\n",
    "            self.term_topic_count_test += np.array([np.sum(queries[i][np.where(topic==j)], axis=0)\\\n",
    "                                                    for j in range(self.n_topics)])\n",
    "            \n",
    "            #sum of number of words in each topic\n",
    "            self.term_topic_sum_test = np.sum(self.term_topic_count_test, axis=1)\n",
    "    \n",
    "    def _gibbs_sampling_fit_train(self, docs):\n",
    "        \"\"\"Update counters for each word using gibbs sampling\"\"\"\n",
    "        \n",
    "        for i in xrange(self.n_docs):\n",
    "            \n",
    "            n_words = len(docs[i])\n",
    "            \n",
    "            for j in xrange(n_words):\n",
    "                \n",
    "                #decrement associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] -= 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] -= docs[i][j]\n",
    "                self.term_topic_sum[self.topic_assign[i][j]] -= 1\n",
    "                \n",
    "                #sample new topic\n",
    "                word_idx = np.argmax(docs[i][j])               \n",
    "                prob = (self.topic_doc_count[i] + self.alpha) * \\\n",
    "                        (self.term_topic_count[:, word_idx] + self.eta[word_idx]) /\\\n",
    "                        (np.sum(self.term_topic_count, axis=1) + self.sumeta) /\\\n",
    "                        ((n_words-1) + self.sumalpha)\n",
    "                prob /= np.sum(prob)                      #normalize\n",
    "                prob = np.cumsum(prob)                    #cdf\n",
    "                self.topic_assign[i][j] = np.argmax(prob > np.random.random())\n",
    "\n",
    "                #increment associated counters\n",
    "                self.topic_doc_count[i][self.topic_assign[i][j]] += 1\n",
    "                self.term_topic_count[self.topic_assign[i][j]] += docs[i][j]  \n",
    "                self.term_topic_sum[self.topic_assign[i][j]] += 1\n",
    "                \n",
    "    def _gibbs_sampling_fit_test(self, queries):\n",
    "        \n",
    "        for i in xrange(self.n_queries):\n",
    "            \n",
    "            n_words = len(queries[i])\n",
    "            \n",
    "            for j in xrange(n_words):\n",
    "                \n",
    "                #decrement associated counters\n",
    "                self.topic_doc_count_test[i][self.topic_assign[i][j]] -= 1\n",
    "                self.term_topic_count_test[self.topic_assign_test] -= queries[i][j]\n",
    "                self.term_topic_sum_test[self.topic_assign_test[i][j]] -= 1\n",
    "                \n",
    "                #sample new topic\n",
    "                word_idx = np.argmax(queries[i][j])\n",
    "                \n",
    "                prob = (self.topic_doc_count_test[i] + self.alpha) * \\\n",
    "                        (self.term_topic_count_test[:,word_idx] + \\\n",
    "                             self.term_topic_count[:,word_idx] + \\\n",
    "                             self.eta[word_idx]) /\\\n",
    "                        (np.sum(self.term_topic_count_test, axis=1) + \\\n",
    "                             self.term_topic_sum + \\\n",
    "                             self.sumeta) /\\\n",
    "                        ((n_words + self.sumalpha))\n",
    "                prob /= np.sum(prob)              #normalize\n",
    "                prob = np.cumsum(prob)            #cdf\n",
    "                self.topic_assign_test[i][j] = np.argmax(prob > np.random.random())\n",
    "                \n",
    "                #increment associated counters\n",
    "                self.topic_doc_count_test[i][self.topic_assign_test[i][j]] += 1\n",
    "                self.term_topic_count_test[self.topic_assign_test[i][j]] += queries[i][j]\n",
    "                self.term_topic_sum_test[self.topic_assign[i][j]] += 1\n",
    "                \n",
    "    def _estimate_parameters_train(self):\n",
    "        \"\"\"Estimate parameters used to make predictions\"\"\"\n",
    "        \n",
    "        self.beta = np.divide((self.term_topic_count + self.eta).T, \n",
    "                             np.sum((self.term_topic_count + self.eta), axis=1)).T\n",
    "        \n",
    "        self.theta = np.divide((self.topic_doc_count + self.alpha).T, \n",
    "                             np.sum((self.topic_doc_count + self.alpha), axis=1)).T\n",
    "        \n",
    "        self.components_ = self.beta\n",
    "    \n",
    "    def _estimate_parameters_test(self):\n",
    "        \n",
    "                                         \n",
    "        self.theta_test = np.divide((self.topic_doc_count_test + self.alpha).T,\n",
    "                                   np.sum((self.topic_doc_count_test + self.alpha), axis=1)).T\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = LDA()\\nbeta, _, _ = model.fit(W)\\n\\n# We more or less retrieve the beta used in the generating process\\nprint beta\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Simple test on toy documents \"\"\"\n",
    "model = LDA()\n",
    "beta, _, _ = model.fit(W)\n",
    "\n",
    "# We more or less retrieve the beta used in the generating process\n",
    "print beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Get the 20 news groups data \"\"\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"train\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"))#, categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Prepare input for sklearn (counts) \"\"\"\n",
    "n_features = 2000\n",
    "vectorizer = CountVectorizer(max_features=n_features, stop_words=\"english\")\n",
    "\n",
    "# Word counts per document matrix (input for sklearn)\n",
    "W_counts = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Keep track of vocabulary to visualize top words of each topic\n",
    "vocabulary = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import WordNetLemmatizer\\nfrom nltk.stem.porter import PorterStemmer\\nfrom gensim.corpora import Dictionary\\nimport string\\n\\n# 1) Tokenize document\\n# 2) Remove stop words\\n# 3) Lemmatize\\nlemmatizer = WordNetLemmatizer()\\n#stemmer = PorterStemmer()\\n# TODO : treat special list better\\n# TODO See what count vectorizer does\\nspecial = [\"\\'\\'\", \"\\'s\", \"``\", \"n\\'t\", \"...\", \"--\"]\\nstop = set(stopwords.words(\\'english\\') +            list(string.punctuation) + special)\\ndef prepare_document(doc):\\n    words = [lemmatizer.lemmatize(w) for w in word_tokenize(doc.lower()) \\n             if w not in stop] \\n    return words\\n\\n\\n# List of documents (each document is a list of word tokens)\\ntexts = [prepare_document(text) for text in newsgroups.data]\\n\\n# Create a gensim dictionary for our corpus\\ndic = Dictionary(texts)\\n\\n# Keep only k most frequent words\\nn_features = 2000\\ndic.filter_extremes(keep_n=n_features)\\nvocab_size = len(dic.token2id) # Vocabulary size\\n\\n# List of documents (each document is now a list of word indexes)\\n# We have removed all words not in the k most frequent words\\ntexts_idx = [[dic.token2id[word] for word in text \\n              if word in dic.token2id] for text in texts]\\n\\n# Convert each index to a one hot vector\\nW = np.array([np.eye(vocab_size)[text] for text in texts_idx if len(text) > 0])\\n\\n# Keep track of id to word mapping to visualize top words of topics\\nid2word = dict([[v, k] for k, v in dic.token2id.items()])\\n\\npickle.dump(W, open(\"W.p\", \"wb\"))\\npickle.dump(id2word, open(\"mapping.p\", \"wb\"))\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Prepare input for our model (one hot vectors) \"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.corpora import Dictionary\n",
    "import string\n",
    "\n",
    "# 1) Tokenize document\n",
    "# 2) Remove stop words\n",
    "# 3) Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer()\n",
    "# TODO : treat special list better\n",
    "# TODO See what count vectorizer does\n",
    "special = [\"''\", \"'s\", \"``\", \"n't\", \"...\", \"--\"]\n",
    "stop = set(stopwords.words('english') + \\\n",
    "           list(string.punctuation) + special)\n",
    "def prepare_document(doc):\n",
    "    words = [lemmatizer.lemmatize(w) for w in word_tokenize(doc.lower()) \n",
    "             if w not in stop] \n",
    "    return words\n",
    "\n",
    "\n",
    "# List of documents (each document is a list of word tokens)\n",
    "texts = [prepare_document(text) for text in newsgroups.data]\n",
    "\n",
    "# Create a gensim dictionary for our corpus\n",
    "dic = Dictionary(texts)\n",
    "\n",
    "# Keep only k most frequent words\n",
    "n_features = 2000\n",
    "dic.filter_extremes(keep_n=n_features)\n",
    "vocab_size = len(dic.token2id) # Vocabulary size\n",
    "\n",
    "# List of documents (each document is now a list of word indexes)\n",
    "# We have removed all words not in the k most frequent words\n",
    "texts_idx = [[dic.token2id[word] for word in text \n",
    "              if word in dic.token2id] for text in texts]\n",
    "\n",
    "# Convert each index to a one hot vector\n",
    "W = np.array([np.eye(vocab_size)[text] for text in texts_idx if len(text) > 0])\n",
    "\n",
    "# Keep track of id to word mapping to visualize top words of topics\n",
    "id2word = dict([[v, k] for k, v in dic.token2id.items()])\n",
    "\n",
    "pickle.dump(W, open(\"W.p\", \"wb\"))\n",
    "pickle.dump(id2word, open(\"mapping.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "years time people like work ago new think problem just know long\n",
      "Topic 2:\n",
      "edu com cs uk ca apr ac david article university pain soon\n",
      "Topic 3:\n",
      "drive scsi card disk hard drives memory bus controller ram use board\n",
      "Topic 4:\n",
      "software image bit color use data used sale video graphics jpeg monitor\n",
      "Topic 5:\n",
      "car good like just use power used new bike don ve buy\n",
      "Topic 6:\n",
      "cx ah w7 chz lk hz c_ mv uw t7 34u ck\n",
      "Topic 7:\n",
      "windows window problem thanks using use does help know file dos set\n",
      "Topic 8:\n",
      "ax max g9v b8f a86 pl 145 1d9 0t 1t giz bhj\n",
      "Topic 9:\n",
      "available edu ftp version pub server motif graphics widget mit com sun\n",
      "Topic 10:\n",
      "db armenian turkish israel armenians people war jews israeli turkey government greek\n",
      "Topic 11:\n",
      "god jesus people does believe bible christian say true church think question\n",
      "Topic 12:\n",
      "don just like think know people good really say want make does\n",
      "Topic 13:\n",
      "said didn people know did went just time don came saw told\n",
      "Topic 14:\n",
      "government key gun law mr people president encryption chip use right clipper\n",
      "Topic 15:\n",
      "mail information list send email internet post posting address group news faq\n",
      "Topic 16:\n",
      "10 25 11 12 15 16 14 20 13 17 18 55\n",
      "Topic 17:\n",
      "university 1993 new national april research american information center 1992 health san\n",
      "Topic 18:\n",
      "space 00 nasa launch earth satellite data orbit shuttle moon lunar station\n",
      "Topic 19:\n",
      "file output entry program section gm int char build rules null stream\n",
      "Topic 20:\n",
      "game team year games season play hockey players win league good player\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint \"\\nOur model\"\\nlda = LDA(K=n_topics)\\nW = pickle.load(open(\"W.p\", \"rb\"))\\nid2word = pickle.load(open(\"mapping.p\", \"rb\"))\\nlda.fit(W, verbose=True)\\nvisualizeTopics(lda, id2word)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 20\n",
    "\n",
    "def visualizeTopics(model, id2word, n_top_words=12):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print \"Topic {}:\".format(i+1)\n",
    "        print \" \".join([id2word[j] \n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "\n",
    "# Compare with sklearn implementation\n",
    "print \"Sklearn model\"\n",
    "lda_sklearn = LatentDirichletAllocation(n_topics=n_topics, \n",
    "                                        learning_method=\"batch\")\n",
    "lda_sklearn.fit(W_counts)\n",
    "visualizeTopics(lda_sklearn, vocabulary)\n",
    "\n",
    "# Our model\n",
    "print \"\\nOur model\"\n",
    "lda = LDA(K=n_topics)\n",
    "W = pickle.load(open(\"W.p\", \"rb\"))\n",
    "id2word = pickle.load(open(\"mapping.p\", \"rb\"))\n",
    "lda.fit(W, verbose=True)\n",
    "visualizeTopics(lda, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results for the LDA implementation using Gibbs sampling\n",
    "n_topics = 2\n",
    "lda_gibbs = LDA_gibbs(n_topics)\n",
    "lda_gibbs.fit(W)\n",
    "        \n",
    "visualizeTopics(lda_gibbs, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test models with test set\n",
    "newsgroups_test = fetch_20newsgroups(shuffle=True, random_state=1, subset=\"test\",\n",
    "                                remove=(\"headers\", \"footers\", \"quotes\"), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Visualize most likely topic assignment for each word \n",
    "    [if p(most likely topic) < threshold, don't assign a topic]\n",
    "    and visualize expected distribution over topics\n",
    "    Inputs:\n",
    "        model, our trained lda model\n",
    "        document_idx, index of document to visualize\n",
    "        id2word, mapping from word index to word string\n",
    "        topic2word, mapping from topic index to topic string\n",
    "        threshold, float in [0.0, 1.0]\n",
    "    \"\"\"\n",
    "def visualizeWordAssignments(model, document_idx, id2word, \n",
    "                             topic2word, threshold=0.9):\n",
    "    # We add a column with value threshold so that if the index of this \n",
    "    # column gets selected by the argmax operation, we don't assign a topic\n",
    "    col = np.ones((model.phi[document_idx].shape[0], 1)) * threshold\n",
    "    col_index = model.phi[document_idx].shape[1]\n",
    "    topic2word[col_index] = \" \"\n",
    "    word_topic_probas = np.concatenate(\n",
    "        (model.phi[document_idx], col), axis=1)\n",
    "    \n",
    "    # Expected distribution over topics\n",
    "    doc_topic_probas = dirichlet_distribution.mean(\n",
    "        model.gamma[document_idx])\n",
    "    \n",
    "    print \"DOCUMENT:\"\n",
    "    print newsgroups.data[document_idx]\n",
    "    \n",
    "    print \"\\nEXPECTED PROBA DISTRIB OVER TOPICS:\"\n",
    "    for i in xrange(len(doc_topic_probas)):\n",
    "        print \"{}: {}\".format(topic2word[i], doc_topic_probas[i])\n",
    "        \n",
    "    print \"\\nWORD TOPIC ASSIGNMENTS:\"\n",
    "    for word_idx, topic_idx in zip(texts_idx[document_idx], \n",
    "                                   np.argmax(word_topic_probas, axis=1)): \n",
    "        print \"{} -> {}\".format(id2word[word_idx], topic2word[topic_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2word = {0: \"topic 1\", 1: \"topic 2\", 2: \"topic 3\", 3: \"topic 4\"}\n",
    "visualizeWordAssignments(lda, 0, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic2word = {0: \"space\", 1: \"religion\"}\n",
    "visualizeWordAssignments(lda, 6, id2word, topic2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO Perform inference on new documents\n",
    "# only E-step (estimate phi and gamma) until the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
